# revenue_etl_report.py
import pandas as pd
import numpy as np
import glob
import os
from pathlib import Path
from datetime import datetime
import platform
from revenue_reconciliation import RevenueReconciliation, ReconciliationError


# ============================================================================
# CONFIGURATION - ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Config ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà
# ============================================================================
class Config:
    """
    Configuration ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Revenue ETL Pipeline
    ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ñ‡πà‡∏≤ config ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà
    """
    
    # ============ ‡∏õ‡∏µ ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• ============
    YEAR = "2025"

    # ============ Reconciliation Settings (‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏´‡∏°‡πà) ============
    RECONCILE_FI_MONTH = "10"       # ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå FI
    RECONCILE_TOLERANCE = 0.00      # ‡∏¢‡∏≠‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ
    ENABLE_RECONCILIATION = True    # True = ‡πÄ‡∏õ‡∏¥‡∏î, False = ‡∏õ‡∏¥‡∏î
    
    # ============ ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå Master ============
    MASTER_PRODUCT_FILE = f"MASTER_PRODUCT_NT_{YEAR}.csv"
    MASTER_GL_FILE = "source/MASTER_REVENUE_GL_CODE_NT1_NT_20250723.csv"
    MAPPING_CC_FILE = "MAPPING_CC.csv"
    MAPPING_PRODUCT_FILE = "clean/MAP_PRODUCT_NT_NEW_2024.csv"
    
    # ============ Pattern ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏ï‡πâ‡∏ô‡∏ó‡∏≤‡∏á ============
    # 1. ‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏•‡∏±‡∏Å NT1
    INPUT_FILE_PATTERNS = [
        "TRN_REVENUE_NT1_*.csv",
        "TRN_REVENUE_ADJ_GL_NT1_*.csv"
    ]
    
    # 2. ‡πÑ‡∏ü‡∏•‡πå ADJ (‡∏ú‡∏•‡∏ï‡∏≠‡∏ö‡πÅ‡∏ó‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô, ‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏≠‡∏∑‡πà‡∏ô)
    ADJ_MONTHLY_PATTERN = "TRN_REVENUE_ADJ_*.csv"
    ADJ_YTD_PATTERN = "TRN_REVENUE_ADJ_YTD_*.csv"

    # ============ ‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô ============
    EXCLUDE_BUSINESS_GROUP = "‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏≠‡∏∑‡πà‡∏ô" # ‡∏à‡∏≤‡∏Å Master Product ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏Å‡∏£‡∏≠‡∏á‡∏≠‡∏≠‡∏Å
    NON_TELECOM_SERVICE_GROUP = "‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏≠‡∏∑‡πà‡∏ô‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÇ‡∏ó‡∏£‡∏Ñ‡∏°‡∏ô‡∏≤‡∏Ñ‡∏°"
    
    NEW_ADJ_BUSINESS_GROUP = "‡∏ú‡∏•‡∏ï‡∏≠‡∏ö‡πÅ‡∏ó‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏≠‡∏∑‡πà‡∏ô"
    FINANCIAL_INCOME_NAME = "‡∏ú‡∏•‡∏ï‡∏≠‡∏ö‡πÅ‡∏ó‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô"
    OTHER_REVENUE_ADJ_NAME = "‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏≠‡∏∑‡πà‡∏ô" # ‡∏à‡∏≤‡∏Å ADJ File
    
    
    # ============ ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå Output ============
    OUTPUT_CONCAT_FILE = f"trn_revenue_nt_{YEAR}.csv"
    OUTPUT_MAPPED_CC_FILE = f"revenue_new_cc_{YEAR}.csv"
    OUTPUT_MAPPED_PRODUCT_FILE = f"revenue_mapped_product_{YEAR}_.csv"
    OUTPUT_FINAL_REPORT_FILE = f"REVENUE_NT_REPORT_{YEAR}.csv"
    
    # ============ ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå Error Log ============
    ERROR_GL_FILE = f"error_gl_REVENUE_NT_REPORT_{YEAR}.csv"
    ERROR_PRODUCT_FILE = f"error_product_REVENUE_NT_REPORT_{YEAR}.csv"
    
    # ============ Path Configuration ============
    @staticmethod
    def get_paths():
        """
        ‡∏Å‡∏≥‡∏´‡∏ô‡∏î path ‡∏ï‡∏≤‡∏°‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£
        Returns: dict ‡∏Ç‡∏≠‡∏á paths ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        """
        os_platform = platform.system()
        
        if os_platform == "Darwin":  # macOS
            base_path = "/Users/seal/Library/CloudStorage/OneDrive-Personal/share/Datasource"
            master_path = "/Users/seal/Library/CloudStorage/OneDrive-Personal/share/master"
        elif os_platform == "Linux":
            base_path = "/home/seal/nt/data/2025"
            master_path = "/home/seal/nt/master"
        elif os_platform == "Windows":
            base_path = r"C:\Users\00320845\OneDrive\share\Datasource"
            master_path = r"C:\Users\00320845\OneDrive\share\master"
        else:
            raise ValueError(f"‡πÑ‡∏°‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£: {os_platform}")
        
        return {
            "base": base_path,
            "master": master_path,
            "input": os.path.join(base_path, Config.YEAR, "revenue"),
            "output": os.path.join(base_path, Config.YEAR, "revenue", "output"),
            "final_output": os.path.join(base_path, "all", "revenue", Config.YEAR)
        }
    
    # ============ Column Names ============
    REQUIRED_COLUMNS = [
        "YEAR", "MONTH", "CUSTOMER_GROUP_KEY", "PRODUCT_KEY",
        "SUB_PRODUCT_KEY", "GL_CODE", "COST_CENTER", "REVENUE_VALUE"
    ]
    
    # ============ Special Mapping Rules ============
    SPECIAL_MAPPINGS = [
        {
            "name": "GSaaS to Other Revenue",
            "condition": {
                "PRODUCT_KEY": "102010407",
                "GL_CODE": "46400101"
            },
            "mapping": {
                "PRODUCT_KEY": "292020407",
                "SUB_PRODUCT_KEY": "1"
            }
        }
    ]
    
    # ============ Validation Thresholds ============
    GRAND_TOTAL_DIFF_THRESHOLD = 0.01  # ‡∏ö‡∏≤‡∏ó - ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö rounding error

    # ============ Anomaly Detection Settings ============
    ANOMALY_IQR_MULTIPLIER = 1.5  # ‡∏Ñ‡πà‡∏≤ k ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Tukey's Fences
    ANOMALY_MIN_HISTORY = 3  # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö

    # [‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ] ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Rolling Average
    ANOMALY_ROLLING_WINDOW = 6

    # True = ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏£‡∏∞‡∏ö‡∏≤‡∏¢‡∏™‡∏µ‡∏ó‡∏∏‡∏Å‡πÄ‡∏î‡∏∑‡∏≠‡∏ô, False = ‡∏ó‡∏≥‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏° (‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Report ‡πÅ‡∏¢‡∏Å)
    ENABLE_HISTORICAL_HIGHLIGHT = True
    
    # ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö
    ANOMALY_LEVELS = {
        "product": {"group_by": ["BUSINESS_GROUP", "SERVICE_GROUP", "PRODUCT_KEY", "PRODUCT_NAME"]},
        "service": {"group_by": ["BUSINESS_GROUP", "SERVICE_GROUP"]},
        "business": {"group_by": ["BUSINESS_GROUP"]},
        "grand_total": {"group_by": []}
    }


# ============================================================================
# ETL Pipeline Class
# ============================================================================
class RevenueETL:
    """
    ETL Pipeline ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ
    ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô:
    1. ‡∏£‡∏ß‡∏°‡πÑ‡∏ü‡∏•‡πå CSV ‡∏ï‡πâ‡∏ô‡∏ó‡∏≤‡∏á (TRN_REVENUE_NT1_*.csv ‡πÅ‡∏•‡∏∞ TRN_REVENUE_ADJ_GL_NT1_*.csv)
    2. Mapping Cost Center
    3. Mapping Product & Sub Product
    4. Merge ‡∏Å‡∏±‡∏ö Master Files, ‡∏Å‡∏£‡∏≠‡∏á, ‡∏£‡∏ß‡∏° ADJ Data, ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
    """
    
    def __init__(self, config=None):
        """
        Args:
            config: Config class ‡∏´‡∏£‡∏∑‡∏≠ None (‡πÉ‡∏ä‡πâ Config default)
        """
        self.config = config or Config
        self.paths = self.config.get_paths()
        self.setup_directories()
        
        # ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö YTD data ‡πÑ‡∏ß‡πâ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô report
        self.df_adj_ytd = pd.DataFrame() 
        
    def setup_directories(self):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á directory ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ"""
        Path(self.paths["output"]).mkdir(parents=True, exist_ok=True)
        Path(self.paths["final_output"]).mkdir(parents=True, exist_ok=True)
        
        print("=" * 80)
        print("PATH CONFIGURATION")
        print("=" * 80)
        for key, path in self.paths.items():
            print(f"{key:15s}: {path}")
        print("=" * 80)
        
    def log(self, message, grand_total=None):
        """‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° log ‡∏û‡∏£‡πâ‡∏≠‡∏° timestamp"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # [FIX] ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡∏≠‡∏á grand_total
        # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô string, ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô level (‡πÄ‡∏ä‡πà‡∏ô "ERROR", "SUCCESS")
        # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç (int/float), ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏¢‡∏≠‡∏î‡∏£‡∏ß‡∏°
        
        if isinstance(grand_total, str):
            print(f"[{timestamp}] [{grand_total}] {message}")
        elif isinstance(grand_total, (int, float)):
            print(f"[{timestamp}] {message}")
            print(f"[{timestamp}] Grand Total = {grand_total:,.2f}")
        else:
            # ‡∏Å‡∏£‡∏ì‡∏µ grand_total=None ‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡∏∑‡πà‡∏ô‡πÜ
            print(f"[{timestamp}] {message}")

    def step0_reconcile_revenue(self):
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Reconciliation)"""
        self.log("=" * 80)
        self.log("STEP 0: RECONCILIATION - ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")
        self.log("=" * 80)
        
        if not self.config.ENABLE_RECONCILIATION:
            self.log("‚ö†Ô∏è  Reconciliation ‡∏ñ‡∏π‡∏Å‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")
            return None
        
        reconciler = RevenueReconciliation(self.config, self.paths)
        
        fi_file = os.path.join(
            self.paths['base'], 
            self.config.YEAR, 
            'fi', 
            'output', 
            f'pl_revenue_nt_output_{self.config.YEAR}{self.config.RECONCILE_FI_MONTH}.csv'
        )
        
        trn_file = os.path.join(
            self.paths['output'], 
            self.config.OUTPUT_CONCAT_FILE
        )
        
        self.log(f"‡πÑ‡∏ü‡∏•‡πå FI: {os.path.basename(fi_file)}")
        self.log(f"‡πÑ‡∏ü‡∏•‡πå TRN: {os.path.basename(trn_file)}")
        
        try:
            result = reconciler.reconcile_revenue(
                fi_file_path=fi_file,
                trn_file_path=trn_file,
                tolerance=self.config.RECONCILE_TOLERANCE
            )
            
            self.log("=" * 80)
            self.log("‚úì Reconciliation ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!", "SUCCESS")
            self.log("=" * 80)
            return result
            
        except ReconciliationError as e:
            self.log("=" * 80)
            self.log("‚ùå Reconciliation ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß - ‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô", "ERROR")
            self.log("=" * 80)
            raise
        
        except FileNotFoundError as e:
            self.log(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå: {e}", "ERROR")
            self.log("üí° ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö:")
            self.log(f"   1. ‡πÑ‡∏ü‡∏•‡πå FI ‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà: {fi_file}")
            self.log(f"   2. RECONCILE_FI_MONTH = '{self.config.RECONCILE_FI_MONTH}' ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà")
    
    def step1_concat_revenue_files(self):
        """
        ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1: ‡∏£‡∏ß‡∏°‡πÑ‡∏ü‡∏•‡πå CSV ‡∏ï‡πâ‡∏ô‡∏ó‡∏≤‡∏á (NT1)
        - TRN_REVENUE_NT1_*.csv ‡πÅ‡∏•‡∏∞ TRN_REVENUE_ADJ_GL_NT1_*.csv
        - ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ GL_CODE ‡πÅ‡∏•‡∏∞ GL_CODE_NT1 ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô GL_CODE
        """
        self.log("=" * 80)
        self.log("STEP 1: ‡∏£‡∏ß‡∏°‡πÑ‡∏ü‡∏•‡πå CSV ‡∏ï‡πâ‡∏ô‡∏ó‡∏≤‡∏á (NT1)")
        self.log("=" * 80)
        
        # ‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
        all_files = []
        for p in self.config.INPUT_FILE_PATTERNS:
            pattern = os.path.join(self.paths["input"], p)
            self.log(f"‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ pattern: {pattern}")
            all_files.extend(glob.glob(pattern))
        
        # ‡πÉ‡∏ä‡πâ set ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ) ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö
        files = sorted(list(set(all_files)))
        
        if not files:
            patterns = [os.path.join(self.paths["input"], p) for p in self.config.INPUT_FILE_PATTERNS]
            raise FileNotFoundError(f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö patterns: {patterns}")
        
        self.log(f"‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(files)} ‡πÑ‡∏ü‡∏•‡πå")
        
        df_combined = pd.DataFrame()
        
        for file in files:
            self.log(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå: {os.path.basename(file)}")
            
            # ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå
            try:
                df = pd.read_csv(
                    file,
                    converters={
                        "YEAR": str,
                        "MONTH": int,
                        "COST_CENTER": str,
                        "CUSTOMER_GROUP_KEY": str,
                        "PRODUCT_KEY": str,
                        "SUB_PRODUCT_KEY": int
                    }
                )
            except Exception as e:
                self.log(f"  ‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå {file}: {e} - ‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ")
                continue
            
            # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
            df.columns = df.columns.str.strip()
            
            # ‡πÅ‡∏™‡∏î‡∏á GL columns ‡∏ó‡∏µ‡πà‡∏û‡∏ö
            gl_columns = df.filter(like='GL').columns.tolist()
            self.log(f"  ‡∏û‡∏ö GL columns: {gl_columns}")
            
            # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ REVENUE_VALUE
            df = df.dropna(subset=["REVENUE_VALUE"])
            df["REVENUE_VALUE"] = df["REVENUE_VALUE"].astype(str)
            df["REVENUE_VALUE"] = df["REVENUE_VALUE"].str.replace(",", "", regex=False)
            df["REVENUE_VALUE"] = df["REVENUE_VALUE"].str.replace(r"\(", "-", regex=True)
            df["REVENUE_VALUE"] = df["REVENUE_VALUE"].str.replace(r"\)", "", regex=True)
            df["REVENUE_VALUE"] = df["REVENUE_VALUE"].str.replace(" ", "", regex=False)
            df["REVENUE_VALUE"] = pd.to_numeric(df["REVENUE_VALUE"], errors='coerce')
            
            # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ GL_CODE ‡πÅ‡∏•‡∏∞ GL_CODE_NT1
            if "GL_CODE_NT1" in df.columns:
                self.log(f"  ‡πÉ‡∏ä‡πâ GL_CODE_NT1 ‡πÄ‡∏õ‡πá‡∏ô GL_CODE")
                df["GL_CODE"] = df["GL_CODE_NT1"].astype(str)
            elif "GL_CODE" in df.columns:
                self.log(f"  ‡πÉ‡∏ä‡πâ GL_CODE")
                df["GL_CODE"] = df["GL_CODE"].astype(str)
            else:
                if len(gl_columns) > 0:
                    self.log(f"  ‡πÉ‡∏ä‡πâ {gl_columns[0]} ‡πÄ‡∏õ‡πá‡∏ô GL_CODE")
                    df["GL_CODE"] = df[gl_columns[0]].astype(str)
                else:
                    self.log(f"  ‚ö†Ô∏è  ‡πÑ‡∏°‡πà‡∏û‡∏ö column GL_CODE ‡∏´‡∏£‡∏∑‡∏≠ GL_CODE_NT1 ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå {file} - ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÄ‡∏õ‡∏•‡πà‡∏≤")
                    df["GL_CODE"] = np.nan # ‡∏´‡∏£‡∏∑‡∏≠ "NA"
            
            # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
            try:
                df = df[self.config.REQUIRED_COLUMNS]
            except KeyError as e:
                self.log(f"  ‚ùå ‡∏Ç‡∏≤‡∏î‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡πÉ‡∏ô {file}: {e} - ‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ")
                continue

            # ‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà PRODUCT_KEY ‡πÄ‡∏õ‡πá‡∏ô null
            df = df.dropna(subset=["PRODUCT_KEY"])
            
            file_total = df["REVENUE_VALUE"].sum()
            self.log(f"  ‡∏¢‡∏≠‡∏î‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ: {file_total:,.2f}")
            self.log(f"  ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß: {len(df):,}")
            
            df_combined = pd.concat([df_combined, df], ignore_index=True)
        
        # ‡πÅ‡∏õ‡∏•‡∏á MONTH ‡πÄ‡∏õ‡πá‡∏ô string ‡πÅ‡∏ö‡∏ö 2 ‡∏´‡∏•‡∏±‡∏Å
        df_combined["MONTH"] = df_combined["MONTH"].astype(int).astype(str).str.zfill(2)
        df_combined["SUB_PRODUCT_KEY"] = df_combined["SUB_PRODUCT_KEY"].astype(int).astype(str)
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö GL_CODE ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ
        unique_gl = df_combined["GL_CODE"].nunique()
        self.log(f"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô GL_CODE ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô: {unique_gl}")
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå
        output_file = os.path.join(self.paths["output"], self.config.OUTPUT_CONCAT_FILE)
        df_combined.to_csv(output_file, index=False, float_format="%.2f")
        
        self.log(f"‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå: {output_file}")
        self.log(f"‡∏£‡∏ß‡∏°‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô - ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(df_combined):,}")
        self.log(f"‡∏£‡∏ß‡∏°‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô", df_combined["REVENUE_VALUE"].sum())
        
        return df_combined
    
    def step2_mapping_cost_center(self, df):
        """‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2: Mapping Cost Center"""
        self.log("=" * 80)
        self.log("STEP 2: Mapping Cost Center")
        self.log("=" * 80)
        
        # ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå mapping
        mapping_file = os.path.join(self.paths["master"], self.config.MAPPING_CC_FILE)
        
        if not os.path.exists(mapping_file):
            self.log(f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå {mapping_file} - ‡∏Ç‡πâ‡∏≤‡∏°‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ")
            return df
        
        df_mapping = pd.read_csv(mapping_file)
        map_dict = dict(df_mapping.values)
        
        self.log(f"‡∏û‡∏ö‡∏Å‡∏≤‡∏£ mapping {len(map_dict)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
        
        # Mapping cost center
        original_cc = df["COST_CENTER"].copy()
        df["COST_CENTER"] = df["COST_CENTER"].map(map_dict).fillna(df["COST_CENTER"])
        
        # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å map
        mapped_count = (original_cc != df["COST_CENTER"]).sum()
        self.log(f"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Cost Center ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å mapping: {mapped_count:,}")
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå
        output_file = os.path.join(self.paths["output"], self.config.OUTPUT_MAPPED_CC_FILE)
        df.to_csv(output_file, index=False, float_format="%.2f")
        
        self.log(f"‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå: {output_file}")
        self.log(f"Mapping Cost Center ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô", df["REVENUE_VALUE"].sum())
        
        return df
    
    def step3_mapping_product(self, df):
        """‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 3: Mapping Product & Sub Product"""
        self.log("=" * 80)
        self.log("STEP 3: Mapping Product & Sub Product")
        self.log("=" * 80)
        
        # ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå mapping product
        mapping_file = os.path.join(self.paths["master"], self.config.MAPPING_PRODUCT_FILE)
        
        if not os.path.exists(mapping_file):
            self.log(f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå {mapping_file} - ‡∏Ç‡πâ‡∏≤‡∏°‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ")
            return df
        
        df_mapping = pd.read_csv(
            mapping_file,
            converters={
                "PRODUCT_KEY_OLD": str,
                "SUB_PRODUCT_KEY_OLD": str,
                "PRODUCT_KEY": str,
                "SUB_PRODUCT_KEY": str,
                "GL_CODE": str
            }
        )
        
        # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
        df_mapping = df_mapping[["PRODUCT_KEY_OLD", "SUB_PRODUCT_KEY_OLD", 
                                  "PRODUCT_KEY", "SUB_PRODUCT_KEY"]]
        
        # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô int ‡πÅ‡∏•‡πâ‡∏ß‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô str (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏±‡∏î 0 ‡∏Ç‡πâ‡∏≤‡∏á‡∏´‡∏ô‡πâ‡∏≤)
        df_mapping["SUB_PRODUCT_KEY_OLD"] = df_mapping["SUB_PRODUCT_KEY_OLD"].astype(int).astype(str)
        df_mapping["SUB_PRODUCT_KEY"] = df_mapping["SUB_PRODUCT_KEY"].astype(int).astype(str)
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á composite key
        df["product_key_sub_product"] = df["PRODUCT_KEY"] + df["SUB_PRODUCT_KEY"]
        df_mapping["product_key_sub_product"] = (df_mapping["PRODUCT_KEY_OLD"] + 
                                                  df_mapping["SUB_PRODUCT_KEY_OLD"])
        
        self.log(f"‡∏û‡∏ö‡∏Å‡∏≤‡∏£ mapping product {len(df_mapping)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
        
        # Mapping PRODUCT_KEY
        map_product = dict(zip(df_mapping["product_key_sub_product"], 
                              df_mapping["PRODUCT_KEY"]))
        original_product = df["PRODUCT_KEY"].copy()
        df["PRODUCT_KEY"] = df["product_key_sub_product"].map(map_product).fillna(df["PRODUCT_KEY"])
        
        # Mapping SUB_PRODUCT_KEY
        map_sub_product = dict(zip(df_mapping["product_key_sub_product"], 
                                   df_mapping["SUB_PRODUCT_KEY"]))
        df["SUB_PRODUCT_KEY"] = df["product_key_sub_product"].map(map_sub_product).fillna(df["SUB_PRODUCT_KEY"])
        
        # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å map
        mapped_count = (original_product != df["PRODUCT_KEY"]).sum()
        self.log(f"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Product ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å mapping: {mapped_count:,}")
        
        # ‡∏•‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß
        df = df.drop(columns=["product_key_sub_product"])
        
        # Apply special mapping rules
        self.log("‡∏Å‡∏≥‡∏•‡∏±‡∏á apply special mapping rules...")
        for rule in self.config.SPECIAL_MAPPINGS:
            self.log(f"  - {rule['name']}")
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á condition
            condition = pd.Series([True] * len(df))
            for col, val in rule['condition'].items():
                condition &= (df[col] == val)
            
            special_count = condition.sum()
            if special_count > 0:
                # Apply mapping
                for col, val in rule['mapping'].items():
                    df.loc[condition, col] = val
                self.log(f"    Applied to {special_count:,} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå
        output_file = os.path.join(self.paths["output"], self.config.OUTPUT_MAPPED_PRODUCT_FILE)
        df.to_csv(output_file, index=False, float_format="%.2f")
        
        self.log(f"‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå: {output_file}")
        self.log(f"Mapping Product ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô", df["REVENUE_VALUE"].sum())
        
        return df

    def _load_adj_data(self):
        """
        [‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà] ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå ADJ ‡∏£‡∏≤‡∏¢‡πÄ‡∏î‡∏∑‡∏≠‡∏ô ‡πÅ‡∏•‡∏∞ ADJ YTD
        """
        self.log("--- Loading ADJ Data (Financial Income / Other Revenue) ---")
        
        # --- Load Monthly ADJ Files ---
        monthly_pattern = os.path.join(self.paths["input"], self.config.ADJ_MONTHLY_PATTERN)
        
        # [ START FIX ] ---
        # ‡∏Å‡∏£‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå YTD ‡πÅ‡∏•‡∏∞ ADJ_GL_NT1 ‡∏≠‡∏≠‡∏Å
        self.log(f"‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ pattern: {monthly_pattern}")
        monthly_files_all = glob.glob(monthly_pattern)
        monthly_files = []
        for f in monthly_files_all:
            filename_upper = os.path.basename(f).upper()
            if "YTD" not in filename_upper and "ADJ_GL_NT1" not in filename_upper:
                monthly_files.append(f)
            else:
                self.log(f"  ‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏ü‡∏•‡πå: {os.path.basename(f)} (‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç ADJ ‡∏£‡∏≤‡∏¢‡πÄ‡∏î‡∏∑‡∏≠‡∏ô)")
        # [ END FIX ] ---
        
        df_adj_monthly = pd.DataFrame()
        if not monthly_files:
            self.log("‚ö†Ô∏è  ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå ADJ ‡∏£‡∏≤‡∏¢‡πÄ‡∏î‡∏∑‡∏≠‡∏ô (TRN_REVENUE_ADJ_*.csv)")
        else:
            self.log(f"‡∏û‡∏ö {len(monthly_files)} ‡πÑ‡∏ü‡∏•‡πå ADJ ‡∏£‡∏≤‡∏¢‡πÄ‡∏î‡∏∑‡∏≠‡∏ô")
            for file in monthly_files:
                self.log(f"  ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡πà‡∏≤‡∏ô ADJ ‡∏£‡∏≤‡∏¢‡πÄ‡∏î‡∏∑‡∏≠‡∏ô: {os.path.basename(file)}")
                try:
                    df = pd.read_csv(file, encoding='tis-620')
                except (UnicodeDecodeError, LookupError):
                    self.log(f"    tis-620 ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß, ‡∏•‡∏≠‡∏á cp874...")
                    try:
                        df = pd.read_csv(file, encoding='cp874')
                    except Exception as e:
                        self.log(f"    ‚ùå ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå {file} ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}")
                        continue
                except Exception as e:
                    self.log(f"    ‚ùå ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå {file} ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}")
                    continue
                
                df_adj_monthly = pd.concat([df_adj_monthly, df], ignore_index=True)
            
            if not df_adj_monthly.empty:
                # Clean monthly data
                df_adj_monthly = df_adj_monthly.dropna(subset=['REVENUE_VALUE'])
                df_adj_monthly["REVENUE_VALUE"] = pd.to_numeric(
                    df_adj_monthly["REVENUE_VALUE"], errors='coerce'
                ).fillna(0)
                self.log(f"‡∏¢‡∏≠‡∏î‡∏£‡∏ß‡∏° ADJ ‡∏£‡∏≤‡∏¢‡πÄ‡∏î‡∏∑‡∏≠‡∏ô (‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏î‡πâ): {df_adj_monthly['REVENUE_VALUE'].sum():,.2f}")

        # --- Load YTD ADJ File ---
        # ‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏•‡πå YTD ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
        ytd_pattern = os.path.join(self.paths["input"], self.config.ADJ_YTD_PATTERN)
        ytd_files = glob.glob(ytd_pattern)
        
        df_adj_ytd = pd.DataFrame()
        if not ytd_files:
            self.log("‚ö†Ô∏è  ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå ADJ YTD (TRN_REVENUE_ADJ_YTD_*.csv)")
        else:
            latest_ytd_file = sorted(ytd_files)[-1] # ‡πÄ‡∏≠‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏ï‡∏≤‡∏°‡∏ä‡∏∑‡πà‡∏≠
            self.log(f"‡∏û‡∏ö {len(ytd_files)} ‡πÑ‡∏ü‡∏•‡πå YTD. ‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏•‡πå‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î: {os.path.basename(latest_ytd_file)}")
            try:
                df_adj_ytd = pd.read_csv(latest_ytd_file, encoding='tis-620')
            except (UnicodeDecodeError, LookupError):
                self.log(f"    tis-620 ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß, ‡∏•‡∏≠‡∏á cp874...")
                df_adj_ytd = pd.read_csv(latest_ytd_file, encoding='cp874')
            except Exception as e:
                 self.log(f"    ‚ùå ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå YTD {latest_ytd_file} ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}")
            
            if not df_adj_ytd.empty:
                # Clean YTD data
                df_adj_ytd = df_adj_ytd.dropna(subset=['REVENUE_VALUE'])
                df_adj_ytd["REVENUE_VALUE"] = pd.to_numeric(
                    df_adj_ytd["REVENUE_VALUE"], errors='coerce'
                ).fillna(0)
                self.log(f"‡∏¢‡∏≠‡∏î‡∏£‡∏ß‡∏° ADJ YTD (‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏î‡πâ): {df_adj_ytd['REVENUE_VALUE'].sum():,.2f}")
            
        return df_adj_monthly, df_adj_ytd

    def _process_adj_data(self, df_adj):
        """
        [‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà] ‡πÅ‡∏õ‡∏•‡∏á ADJ data ‡πÉ‡∏´‡πâ‡∏°‡∏µ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô df_output
        """
        self.log("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• ADJ data...")
        if df_adj.empty:
            self.log("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ADJ ‡πÉ‡∏´‡πâ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•")
            return pd.DataFrame()
            
        df_adj['YEAR'] = df_adj['YEAR'].astype(str)
        df_adj['MONTH'] = df_adj['MONTH'].astype(int).astype(str).str.zfill(2)
        
        # [ START FIX ]
        # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'TYPE' ‡πÅ‡∏•‡∏∞ 'REVENUE_TYPE' ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô‡∏Å‡∏±‡∏ô
        if "REVENUE_TYPE" in df_adj.columns and "TYPE" in df_adj.columns:
            self.log("  ‡∏û‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'TYPE' ‡πÅ‡∏•‡∏∞ 'REVENUE_TYPE' ... ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏£‡∏ß‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå")
            # ‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡πà‡∏≤ 'TYPE' ‡∏ó‡∏µ‡πà‡∏ß‡πà‡∏≤‡∏á (NaN) ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å 'REVENUE_TYPE'
            df_adj['TYPE'] = df_adj['TYPE'].fillna(df_adj['REVENUE_TYPE'])
            # ‡∏•‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'REVENUE_TYPE' ‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô‡∏≠‡∏≠‡∏Å
            df_adj = df_adj.drop(columns=['REVENUE_TYPE'])
            self.log("  ‡∏£‡∏ß‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå TYPE/REVENUE_TYPE ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")
        elif "REVENUE_TYPE" in df_adj.columns:
            # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡πÅ‡∏Ñ‡πà REVENUE_TYPE, ‡∏Å‡πá‡πÅ‡∏Ñ‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠
            self.log("  ‡∏û‡∏ö 'REVENUE_TYPE', ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡πá‡∏ô 'TYPE'")
            df_adj = df_adj.rename(columns={"REVENUE_TYPE": "TYPE"})
        elif "TYPE" not in df_adj.columns:
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ó‡∏±‡πâ‡∏á‡∏Ñ‡∏π‡πà
            self.log("‚ùå ‡πÑ‡∏ü‡∏•‡πå ADJ ‡∏Ç‡∏≤‡∏î‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'TYPE' ‡∏´‡∏£‡∏∑‡∏≠ 'REVENUE_TYPE'")
            return pd.DataFrame()
        # [ END FIX ]

        # ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ (‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡πÄ‡∏Å‡∏¥‡∏î Error)
        df_adj_filtered = df_adj[df_adj['TYPE'].isin([
            self.config.FINANCIAL_INCOME_NAME, 
            self.config.OTHER_REVENUE_ADJ_NAME
        ])].copy()
        
        if df_adj_filtered.empty:
            self.log("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• '‡∏ú‡∏•‡∏ï‡∏≠‡∏ö‡πÅ‡∏ó‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô' ‡∏´‡∏£‡∏∑‡∏≠ '‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏≠‡∏∑‡πà‡∏ô' ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå ADJ")
            return pd.DataFrame()

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô df_output
        df_processed = pd.DataFrame()
        df_processed['YEAR'] = df_adj_filtered['YEAR']
        df_processed['MONTH'] = df_adj_filtered['MONTH']
        df_processed['REVENUE_VALUE'] = df_adj_filtered['REVENUE_VALUE']
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå Hierarchy ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô
        df_processed['BUSINESS_GROUP'] = self.config.NEW_ADJ_BUSINESS_GROUP
        df_processed['SERVICE_GROUP'] = df_adj_filtered['TYPE'] # e.g., "‡∏ú‡∏•‡∏ï‡∏≠‡∏ö‡πÅ‡∏ó‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô"
        df_processed['PRODUCT_NAME'] = df_adj_filtered['TYPE']
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå Dummy/Placeholder ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ concat ‡∏Å‡∏±‡∏ö df_output ‡πÑ‡∏î‡πâ
        df_processed['ITEM'] = 'ADJ'
        df_processed['SUB_ITEM'] = 'ADJ'
        df_processed['PRODUCT_KEY'] = 'ADJ_' + df_adj_filtered['TYPE'].str.replace(' ', '_') # Unique key
        df_processed['SUB_PRODUCT_KEY'] = '1'
        df_processed['COST_CENTER'] = df_adj_filtered.get('COST_CENTER', 'NA')
        df_processed['CUSTOMER_GROUP_KEY'] = df_adj_filtered.get('CUSTOMER_GROUP_KEY', 'NA')
        df_processed['GL_CODE'] = 'ADJ' # Dummy GL
        df_processed['NT'] = 'NT'
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå Dummy ‡∏à‡∏≤‡∏Å Master GL
        df_processed['REPORT_CODE'] = 'ADJ'
        df_processed['GL_NAME'] = 'ADJ'
        df_processed['GL_GROUP'] = 'ADJ'
        df_processed['‡∏´‡∏°‡∏ß‡∏î‡∏ö‡∏±‡∏ç‡∏ä‡∏µ'] = 'ADJ' # ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ Error ‡πÉ‡∏ô Step 4
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå Dummy ‡∏à‡∏≤‡∏Å Master Product
        df_processed['SUB_PRODUCT_NAME'] = df_adj_filtered['TYPE']
        df_processed['REVENUE_GROUP_TYPE'] = 'ADJ'
        df_processed['BUSINESS'] = df_processed['ITEM'] + " " + df_processed['BUSINESS_GROUP']
        df_processed['SERVICE'] = df_processed['SUB_ITEM'] + " " + df_processed['SERVICE_GROUP']
        df_processed['PRODUCT'] = df_processed['PRODUCT_KEY'] + " " + df_processed['PRODUCT_NAME']
        df_processed['SUB_PRODUCT'] = df_processed['SUB_PRODUCT_KEY'] + " " + df_processed['SUB_PRODUCT_NAME']
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå Date ‡πÅ‡∏•‡∏∞ Amount
        df_processed["dt"] = "01" + df_processed["MONTH"] + df_processed["YEAR"]
        df_processed["DATE"] = pd.to_datetime(df_processed["dt"], errors="coerce", format="%d%m%Y")
        
        df_processed['TYPE'] = '‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ'
        df_processed['AMOUNT'] = df_processed['REVENUE_VALUE']
        
        self.log(f"‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• ADJ data {len(df_processed)} ‡πÅ‡∏ñ‡∏ß‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")
        return df_processed

    def step4_create_final_report(self, df):
        """‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 4: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢"""
        self.log("=" * 80)
        self.log("STEP 4: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢")
        self.log("=" * 80)
        
        grand_total_before = df["REVENUE_VALUE"].sum()
        
        # ‡∏≠‡πà‡∏≤‡∏ô Master Product
        master_product_file = os.path.join(self.paths["master"], self.config.MASTER_PRODUCT_FILE)
        if not os.path.exists(master_product_file):
            raise FileNotFoundError(f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå {master_product_file}")
        
        df_master_product = pd.read_csv(master_product_file, dtype=str)
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏£‡∏ß‡∏°
        df_master_product["BUSINESS"] = df_master_product["ITEM"] + " " + df_master_product["BUSINESS_GROUP"]
        df_master_product["SERVICE"] = df_master_product["SUB_ITEM"] + " " + df_master_product["SERVICE_GROUP"]
        df_master_product["PRODUCT"] = df_master_product["PRODUCT_KEY"] + " " + df_master_product["PRODUCT_NAME"]
        df_master_product["SUB_PRODUCT"] = df_master_product["SUB_PRODUCT_KEY"] + " " + df_master_product["SUB_PRODUCT_NAME"]
        
        df_master_product = df_master_product[[
            "PRODUCT_KEY", "PRODUCT_NAME", "SUB_PRODUCT_KEY", "SUB_PRODUCT_NAME",
            "ITEM", "BUSINESS_GROUP", "SUB_ITEM", "SERVICE_GROUP", 
            "REVENUE_GROUP_TYPE", "BUSINESS", "SERVICE", "PRODUCT", "SUB_PRODUCT"
        ]]
        df_master_product = df_master_product.drop_duplicates()
        
        self.log(f"Master Product: {len(df_master_product):,} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
        
        # ‡∏≠‡πà‡∏≤‡∏ô Master GL
        master_gl_file = os.path.join(self.paths["master"], self.config.MASTER_GL_FILE)
        if not os.path.exists(master_gl_file):
            raise FileNotFoundError(f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå {master_gl_file}")
        
        df_master_gl = pd.read_csv(master_gl_file, converters={"GL_CODE_NT1": str})
        
        # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞ columns ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏≤‡∏Å Master GL
        df_master_gl = df_master_gl[[
            "GL_CODE_NT1", "GL_NAME_NT1", "REPORT_CODE", "GL_GROUP", "‡∏´‡∏°‡∏ß‡∏î‡∏ö‡∏±‡∏ç‡∏ä‡∏µ"
        ]].copy()
        
        df_master_gl.rename(columns={"GL_NAME_NT1": "GL_NAME"}, inplace=True)
        
        self.log(f"Master GL: {len(df_master_gl):,} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÄ‡∏™‡∏£‡∏¥‡∏°
        df["NT"] = "NT"
        df["dt"] = "01" + df["MONTH"] + df["YEAR"]
        df["DATE"] = pd.to_datetime(df["dt"], errors="coerce", format="%d%m%Y")
        
        # Group by
        self.log("Group by ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (NT1)...")
        df_output = df.groupby([
            "YEAR", "MONTH", "DATE", "COST_CENTER", "CUSTOMER_GROUP_KEY",
            "PRODUCT_KEY", "SUB_PRODUCT_KEY", "GL_CODE", "NT"
        ], dropna=False)["REVENUE_VALUE"].sum().reset_index()
        df_output = df_output.round(2)
        
        self.log(f"‡∏´‡∏•‡∏±‡∏á Group by (NT1): {len(df_output):,} ‡πÅ‡∏ñ‡∏ß")
        
        # Merge ‡∏Å‡∏±‡∏ö Master GL
        self.log("Merging (NT1) ‡∏Å‡∏±‡∏ö Master GL...")
        
        gl_in_data = set(df_output["GL_CODE"].dropna().unique())
        gl_in_master = set(df_master_gl["GL_CODE_NT1"].dropna().unique())
        
        self.log(f"GL_CODE ‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (NT1): {len(gl_in_data)} ‡∏£‡∏´‡∏±‡∏™")
        self.log(f"GL_CODE_NT1 ‡πÉ‡∏ô Master: {len(gl_in_master)} ‡∏£‡∏´‡∏±‡∏™")
        
        # ‡∏´‡∏≤ GL ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô Master
        gl_not_in_master = gl_in_data - gl_in_master
        if len(gl_not_in_master) > 0:
            self.log(f"‚ö†Ô∏è  ‡∏û‡∏ö GL_CODE {len(gl_not_in_master)} ‡∏£‡∏´‡∏±‡∏™‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô Master:")
            for gl in sorted(list(gl_not_in_master))[:10]:
                count = (df_output["GL_CODE"] == gl).sum()
                amount = df_output[df_output["GL_CODE"] == gl]["REVENUE_VALUE"].sum()
                self.log(f"    - {gl}: {count:,} ‡πÅ‡∏ñ‡∏ß, ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô {amount:,.2f}")
        
        df_output = pd.merge(
            df_output,
            df_master_gl,
            left_on="GL_CODE",
            right_on="GL_CODE_NT1",
            how="left"
        )
        
        df_output = df_output.drop(columns=["GL_CODE_NT1"])
        
        self.log(f"‡∏´‡∏•‡∏±‡∏á Merge GL (NT1): {len(df_output):,} ‡πÅ‡∏ñ‡∏ß")
        
        # Group by again
        df_output = df_output.groupby([
            "YEAR", "MONTH", "DATE", "COST_CENTER", "CUSTOMER_GROUP_KEY",
            "PRODUCT_KEY", "SUB_PRODUCT_KEY", "REPORT_CODE", "GL_CODE",
            "GL_NAME", "GL_GROUP", "‡∏´‡∏°‡∏ß‡∏î‡∏ö‡∏±‡∏ç‡∏ä‡∏µ", "NT"
        ], dropna=False)["REVENUE_VALUE"].sum().reset_index()
        df_output = df_output.round(2)
        
        df_output["TYPE"] = "‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ"
        df_output["AMOUNT"] = df_output["REVENUE_VALUE"]
        
        # Merge ‡∏Å‡∏±‡∏ö Master Product
        self.log("Merging (NT1) ‡∏Å‡∏±‡∏ö Master Product...")
        df_output = pd.merge(
            df_output,
            df_master_product,
            on=["PRODUCT_KEY", "SUB_PRODUCT_KEY"],
            how="left"
        )
        
        self.log(f"‡∏´‡∏•‡∏±‡∏á Merge Product (NT1): {len(df_output):,} ‡πÅ‡∏ñ‡∏ß")

        # --- [ START MODIFICATION ] ---
        # 1. ‡∏Å‡∏£‡∏≠‡∏á "‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à ‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏≠‡∏∑‡πà‡∏ô" ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• NT1
        self.log(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Å‡∏£‡∏≠‡∏á BUSINESS_GROUP: '{self.config.EXCLUDE_BUSINESS_GROUP}' ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• NT1")
        initial_count = len(df_output)
        
        if "BUSINESS_GROUP" not in df_output.columns:
            self.log("‚ö†Ô∏è  ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå BUSINESS_GROUP, ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Å‡∏£‡∏≠‡∏á‡πÑ‡∏î‡πâ")
        else:
            df_output = df_output[df_output["BUSINESS_GROUP"] != self.config.EXCLUDE_BUSINESS_GROUP].copy()
            filtered_count = initial_count - len(df_output)
            self.log(f"  ‡∏Å‡∏£‡∏≠‡∏á‡∏≠‡∏≠‡∏Å {filtered_count:,} ‡πÅ‡∏ñ‡∏ß")
            
        grand_total_after_filter = df_output["REVENUE_VALUE"].sum()
        self.log(f"‡∏¢‡∏≠‡∏î‡∏£‡∏ß‡∏° (NT1 data) ‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏£‡∏≠‡∏á: {grand_total_after_filter:,.2f}")

        # 2. ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• ADJ data
        df_adj_monthly, df_adj_ytd = self._load_adj_data()
        
        # ‡πÄ‡∏Å‡πá‡∏ö YTD data ‡πÑ‡∏ß‡πâ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô __main__
        self.df_adj_ytd = df_adj_ytd 
        
        # 3. ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• ADJ data
        df_adj_processed = self._process_adj_data(df_adj_monthly)
        
        # 4. ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• NT1 ‡∏ó‡∏µ‡πà‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß ‡∏Å‡∏±‡∏ö ADJ data
        if df_adj_processed is not None and not df_adj_processed.empty:
            self.log("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• NT1 ‡πÅ‡∏•‡∏∞ ADJ...")
            df_output = pd.concat([df_output, df_adj_processed], ignore_index=True)
            self.log(f"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏´‡∏•‡∏±‡∏á‡∏£‡∏ß‡∏°: {len(df_output):,}")
        else:
            self.log("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ADJ ‡∏£‡∏≤‡∏¢‡πÄ‡∏î‡∏∑‡∏≠‡∏ô, ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• NT1 ‡∏ï‡πà‡∏≠‡πÑ‡∏õ")
        
        # --- [ END MODIFICATION ] ---

        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á (‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏° ADJ ‡πÅ‡∏•‡πâ‡∏ß)
        self.log("‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (NT1 + ADJ)...")
        
        # '‡∏´‡∏°‡∏ß‡∏î‡∏ö‡∏±‡∏ç‡∏ä‡∏µ' ‡∏Ç‡∏≠‡∏á ADJ ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô 'ADJ' ‡∏ã‡∏∂‡πà‡∏á‡πÑ‡∏°‡πà .isnull()
        missing_gl = df_output["‡∏´‡∏°‡∏ß‡∏î‡∏ö‡∏±‡∏ç‡∏ä‡∏µ"].isnull() 
        # 'BUSINESS_GROUP' ‡∏Ç‡∏≠‡∏á ADJ ‡∏à‡∏∞‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ (NEW_ADJ_BUSINESS_GROUP) ‡∏ã‡∏∂‡πà‡∏á‡πÑ‡∏°‡πà .isnull()
        missing_bu = df_output["BUSINESS_GROUP"].isnull()
        
        has_error = False
        
        if missing_gl.any():
            missing_count = missing_gl.sum()
            missing_amount = df_output[missing_gl]["REVENUE_VALUE"].sum()
            self.log(f"‚ö†Ô∏è  ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• GL (NT1) ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô Master: {missing_count:,} ‡πÅ‡∏ñ‡∏ß (‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô: {missing_amount:,.2f})")
            
            problem_gl = df_output[missing_gl][["GL_CODE", "REVENUE_VALUE"]].groupby("GL_CODE").agg({
                "REVENUE_VALUE": "sum"
            }).reset_index().sort_values("REVENUE_VALUE", ascending=False)
            
            self.log(f"GL_CODE (NT1) ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ (Top 10):")
            for idx, row in problem_gl.head(10).iterrows():
                self.log(f"  - {row['GL_CODE']}: {row['REVENUE_VALUE']:,.2f}")
            
            error_file = os.path.join(self.paths["final_output"], self.config.ERROR_GL_FILE)
            df_output[missing_gl].to_csv(error_file, index=False)
            self.log(f"‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å error log: {error_file}")
            has_error = True
        
        if missing_bu.any():
            missing_count = missing_bu.sum()
            missing_amount = df_output[missing_bu]["REVENUE_VALUE"].sum()
            self.log(f"‚ö†Ô∏è  ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Product (NT1) ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô Master: {missing_count:,} ‡πÅ‡∏ñ‡∏ß (‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô: {missing_amount:,.2f})")
            
            error_data = df_output.loc[missing_bu, [
                "MONTH", "PRODUCT_KEY", "SUB_PRODUCT_KEY", "REVENUE_VALUE"
            ]].drop_duplicates()
            
            self.log(f"Product (NT1) ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ (Top 10):")
            for idx, row in error_data.head(10).iterrows():
                self.log(f"  - {row['PRODUCT_KEY']}-{row['SUB_PRODUCT_KEY']}: {row['REVENUE_VALUE']:,.2f}")
            
            error_file = os.path.join(self.paths["final_output"], self.config.ERROR_PRODUCT_FILE)
            error_data.to_csv(error_file, index=False)
            self.log(f"‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å error log: {error_file}")
            has_error = True
        
        if not has_error:
            self.log("‚úì ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• GL ‡πÅ‡∏•‡∏∞ Product (NT1) ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô (ADJ ‡∏ñ‡∏π‡∏Å‡∏Ç‡πâ‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ô‡∏µ‡πâ)")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏¢‡∏≠‡∏î‡∏£‡∏ß‡∏°
        grand_total_after = df_output["REVENUE_VALUE"].sum()
        # grand_total_before ‡∏Ñ‡∏∑‡∏≠‡∏¢‡∏≠‡∏î NT1 (‡∏Å‡πà‡∏≠‡∏ô‡∏Å‡∏£‡∏≠‡∏á)
        # grand_total_after ‡∏Ñ‡∏∑‡∏≠‡∏¢‡∏≠‡∏î NT1 (‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏£‡∏≠‡∏á) + ADJ
        # ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ô‡∏µ‡πâ‡∏≠‡∏≤‡∏à‡πÑ‡∏°‡πà‡∏™‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡πÅ‡∏•‡πâ‡∏ß ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡πÑ‡∏ß‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏¢‡∏≠‡∏î‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
        
        self.log(f"Grand Total Before (NT1 Original): {grand_total_before:,.2f}")
        self.log(f"Grand Total After (NT1 Filtered + ADJ):  {grand_total_after:,.2f}")
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏° ADJ ‡πÅ‡∏•‡πâ‡∏ß)
        output_file = os.path.join(self.paths["final_output"], self.config.OUTPUT_FINAL_REPORT_FILE)
        df_output.to_csv(output_file, index=False)
        
        self.log(f"‚úì ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö): {output_file}")
        self.log(f"‚úì ETL Pipeline ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!", grand_total_after)
        
        return df_output
    
    def detect_historical_anomalies(self, df_final):
        """
        ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏ó‡∏∏‡∏Å‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡πÅ‡∏ö‡∏ö Vectorization (‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤ Loop 100 ‡πÄ‡∏ó‡πà‡∏≤)
        Returns: Dictionary {(type, identifier, date_string): status}
        """
        if not self.config.ENABLE_HISTORICAL_HIGHLIGHT:
            return {}

        self.log("=" * 80)
        self.log("HISTORICAL ANOMALY (VECTORIZED): ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö‡∏£‡∏ß‡∏î‡πÄ‡∏£‡πá‡∏ß...")
        
        heatmap_map = {} 
        
        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡πÄ‡∏õ‡πá‡∏ô Int ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
        df_final = df_final.copy()
        df_final['MONTH_INT'] = df_final['MONTH'].astype(int)
        
        # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏ï‡∏≤‡∏°‡∏£‡∏∞‡∏î‡∏±‡∏ö (Product, Service, Business, Grand Total)
        for level_name, level_config in self.config.ANOMALY_LEVELS.items():
            # [OPTION] ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏Ñ‡πà Product Level ‡πÉ‡∏´‡πâ uncomment ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏•‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ
            # if level_name != 'product': continue 
            
            group_by = level_config["group_by"]
            
            # 1. Prepare Wide Format DataFrame
            if group_by:
                df_grouped = df_final.groupby(group_by + ['MONTH_INT'], dropna=False)['REVENUE_VALUE'].sum().reset_index()
                df_pivot = df_grouped.pivot_table(index=group_by, columns='MONTH_INT', values='REVENUE_VALUE', fill_value=0)
            else:
                df_grouped = df_final.groupby(['MONTH_INT'], dropna=False)['REVENUE_VALUE'].sum().reset_index()
                df_pivot = df_grouped.pivot_table(columns='MONTH_INT', values='REVENUE_VALUE', fill_value=0)
                df_pivot.index = ["GRAND_TOTAL"]

            # 2. Vectorized Calculation
            df_calc = df_pivot.replace(0, np.nan)
            min_periods = self.config.ANOMALY_MIN_HISTORY
            
            # q1_matrix = df_calc.expanding(min_periods=min_periods, axis=1).quantile(0.25).shift(1, axis=1)
            # q3_matrix = df_calc.expanding(min_periods=min_periods, axis=1).quantile(0.75).shift(1, axis=1)
            # ‡∏™‡∏•‡∏±‡∏ö‡πÅ‡∏Å‡∏ô (T) -> ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì (‡πÑ‡∏°‡πà‡∏°‡∏µ axis=1) -> ‡∏™‡∏•‡∏±‡∏ö‡∏Å‡∏•‡∏±‡∏ö (T)
            q1_matrix = df_calc.T.expanding(min_periods=min_periods).quantile(0.25).shift(1).T
            q3_matrix = df_calc.T.expanding(min_periods=min_periods).quantile(0.75).shift(1).T
            iqr_matrix = q3_matrix - q1_matrix
            
            k = self.config.ANOMALY_IQR_MULTIPLIER
            upper_fence = q3_matrix + (k * iqr_matrix)
            lower_fence = q1_matrix - (k * iqr_matrix)
            lower_fence[lower_fence < 0] = 0 
            
            # 3. Generate Status Masks
            is_neg = df_pivot < 0
            is_high = (df_pivot > upper_fence) & (upper_fence.notna())
            is_low = (df_pivot < lower_fence) & (lower_fence.notna())
            
            # 4. Convert to Dictionary for Excel
            def add_to_map(mask_df, status_code):
                anomalies = mask_df.stack()
                anomalies = anomalies[anomalies] # Filter True only
                
                for idx, _ in anomalies.items():
                    # idx structure depend on group_by length. 
                    # For Product: (Biz, Svc, Key, Name, Month)
                    month_int = idx[-1]
                    row_keys = idx[:-1]
                    
                    # [FIX] ‡∏™‡∏£‡πâ‡∏≤‡∏á Key ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö Excel Loop (‡∏ï‡∏±‡∏î Product Name ‡∏≠‡∏≠‡∏Å)
                    if level_name == 'product':
                        # row_keys = (Biz, Svc, ProdKey, ProdName) -> ‡πÄ‡∏≠‡∏≤‡πÅ‡∏Ñ‡πà 3 ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å
                        lookup_key = (row_keys[0], row_keys[1], row_keys[2]) 
                    elif level_name == 'service':
                        # row_keys = (Biz, Svc)
                        lookup_key = (row_keys[0], row_keys[1])
                    elif level_name == 'business':
                        lookup_key = row_keys[0]
                    else:
                        lookup_key = "GRAND_TOTAL"
                        
                    date_str = f"01/{month_int:02d}/{self.config.YEAR}"
                    heatmap_map[(level_name, lookup_key, date_str)] = status_code

            add_to_map(is_low, 'Low_Spike')
            add_to_map(is_high, 'High_Spike')
            add_to_map(is_neg, 'Negative_Value')
            
        self.log(f"  ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô ‡∏û‡∏ö‡∏à‡∏∏‡∏î‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥ {len(heatmap_map)} ‡∏à‡∏∏‡∏î")
        return heatmap_map
    
    def detect_anomalies(self, df_final):
        """
        ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥‡πÉ‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
        ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÉ‡∏ô 4 ‡∏£‡∏∞‡∏î‡∏±‡∏ö: Product, Service, Business, Grand Total
        (‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏° ADJ ‡πÅ‡∏•‡πâ‡∏ß)
        """
        self.log("=" * 80)
        self.log("ANOMALY DETECTION: ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥‡πÉ‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î")
        self.log("=" * 80)
        
        # ‡∏´‡∏≤‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
        df_final['MONTH_INT'] = df_final['MONTH'].astype(int)
        latest_month = df_final['MONTH_INT'].max()
        
        self.log(f"‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö: {latest_month}")
        
        anomaly_results = {}
        
        for level_name, level_config in self.config.ANOMALY_LEVELS.items():
            self.log(f"\n‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏î‡∏±‡∏ö: {level_name.upper()}")
            
            group_by = level_config["group_by"]
            
            # Aggregate ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏°‡∏£‡∏∞‡∏î‡∏±‡∏ö
            if group_by:
                df_grouped = df_final.groupby(
                    group_by + ['MONTH_INT'], 
                    dropna=False
                )['REVENUE_VALUE'].sum().reset_index()
            else:
                # Grand total - ‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
                df_grouped = df_final.groupby(
                    ['MONTH_INT'], 
                    dropna=False
                )['REVENUE_VALUE'].sum().reset_index()
                df_grouped['LEVEL'] = 'GRAND_TOTAL'
                group_by = ['LEVEL']
            
            # Pivot ‡πÄ‡∏õ‡πá‡∏ô wide format (‡πÅ‡∏ñ‡∏ß = entity, ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå = ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô)
            if group_by:
                df_pivot = df_grouped.pivot_table(
                    index=group_by,
                    columns='MONTH_INT',
                    values='REVENUE_VALUE',
                    fill_value=0
                ).reset_index()
            else:
                # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Grand Total
                df_pivot = df_grouped.pivot_table(
                    index=group_by,
                    columns='MONTH_INT',
                    values='REVENUE_VALUE',
                    fill_value=0
                ).reset_index()
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÄ‡∏î‡∏∑‡∏≠‡∏ô
            month_cols = [col for col in df_pivot.columns if isinstance(col, int)]
            month_cols.sort()
            
            if latest_month not in month_cols:
                self.log(f"‚ö†Ô∏è  ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏î‡∏∑‡∏≠‡∏ô {latest_month} ‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• - ‡∏Ç‡πâ‡∏≤‡∏°")
                continue
            
            # ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï
            latest_col = latest_month
            historical_cols = [m for m in month_cols if m < latest_month]
            
            if len(historical_cols) < self.config.ANOMALY_MIN_HISTORY:
                self.log(f"‚ö†Ô∏è  ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠ ({len(historical_cols)} ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô) - ‡∏Ç‡πâ‡∏≤‡∏°")
                continue
            
            self.log(f"‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö {len(df_pivot)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
            self.log(f"‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï: {historical_cols}")
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥
            df_pivot['ANOMALY_STATUS'] = df_pivot.apply(
                lambda row: self._check_anomaly_row(
                    row, 
                    latest_col, 
                    historical_cols
                ), 
                axis=1
            )
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏£‡∏¥‡∏°
            df_pivot['LATEST_MONTH'] = latest_month
            df_pivot['LATEST_VALUE'] = df_pivot[latest_col]
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï
            df_pivot['AVG_HISTORICAL'] = df_pivot[historical_cols].mean(axis=1)

            # [‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏´‡∏°‡πà] 1. ‡∏´‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Rolling Window (‡πÄ‡∏ä‡πà‡∏ô 3 ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î)
            window = self.config.ANOMALY_ROLLING_WINDOW
            # ‡∏ñ‡πâ‡∏≤‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏¢‡∏≤‡∏ß‡∏Å‡∏ß‡πà‡∏≤ window ‡πÉ‡∏´‡πâ‡∏ï‡∏±‡∏î‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡πâ‡∏≤‡∏¢‡πÜ, ‡∏ñ‡πâ‡∏≤‡∏™‡∏±‡πâ‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏Å‡πá‡πÄ‡∏≠‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
            rolling_cols = historical_cols[-window:] if len(historical_cols) >= window else historical_cols
            
            # [‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏´‡∏°‡πà] 2. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Rolling Average
            df_pivot['ROLLING_AVG'] = df_pivot[rolling_cols].mean(axis=1)
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì % change
            df_pivot['PCT_CHANGE'] = (
                (df_pivot['LATEST_VALUE'] - df_pivot['AVG_HISTORICAL']) / 
                df_pivot['AVG_HISTORICAL'].replace(0, np.nan) * 100
            ).fillna(0)

            # [‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏´‡∏°‡πà] 3. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì % change (‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö Rolling Average)
            df_pivot['PCT_ROLLING'] = (
                (df_pivot['LATEST_VALUE'] - df_pivot['ROLLING_AVG']) / 
                df_pivot['ROLLING_AVG'].replace(0, np.nan) * 100
            ).fillna(0)
            
            # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•
            status_counts = df_pivot['ANOMALY_STATUS'].value_counts()
            self.log(f"‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö:")
            for status, count in status_counts.items():
                self.log(f"  - {status}: {count} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
            
            # ‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
            anomaly_results[level_name] = df_pivot
        
        return anomaly_results
    
    def _check_anomaly_row(self, row, latest_col, historical_cols):
        """
        ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥‡∏Ç‡∏≠‡∏á‡πÅ‡∏ñ‡∏ß‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
        ‡πÉ‡∏ä‡πâ IQR method (Tukey's Fences)
        """
        # ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
        latest_val = row[latest_col]
        
        # 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡πà‡∏≤‡∏ï‡∏¥‡∏î‡∏•‡∏ö
        if latest_val < 0:
            return "Negative_Value"
        
        # ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï
        history = row[historical_cols]
        
        # ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡πà‡∏≤‡∏õ‡∏Å‡∏ï‡∏¥ (> 0)
        history_clean = history[history > 0]
        
        # 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏≠‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if len(history_clean) < self.config.ANOMALY_MIN_HISTORY:
            # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥ (‡∏´‡∏£‡∏∑‡∏≠‡∏°‡∏µ‡∏ô‡πâ‡∏≠‡∏¢‡πÑ‡∏õ)
            if latest_val > 0:
                return "New_Item" # ‡πÄ‡∏£‡∏≤‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô "Not_Enough_Data"
            return "Not_Enough_Data"
        
        # 3. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Robust Statistics (IQR)
        Q1 = history_clean.quantile(0.25)
        Q3 = history_clean.quantile(0.75)
        IQR = Q3 - Q1
        
        # 4. ‡∏Å‡∏£‡∏ì‡∏µ‡∏û‡∏¥‡πÄ‡∏®‡∏©: ‡∏¢‡∏≠‡∏î‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï‡∏Ñ‡∏á‡∏ó‡∏µ‡πà (IQR = 0)
        if IQR == 0:
            # ‡∏ñ‡πâ‡∏≤‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï‡πÄ‡∏õ‡πá‡∏ô 0 ‡∏´‡∏°‡∏î ‡πÅ‡∏•‡∏∞‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏Ñ‡πà‡∏≤
            if Q1 == 0 and latest_val > 0:
                return "High_Spike" # ‡∏ñ‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô Spike
            return "Normal" if latest_val == Q1 else "Spike_vs_Constant"
        
        # 5. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏±‡πâ‡∏ß (Tukey's Fences)
        k = self.config.ANOMALY_IQR_MULTIPLIER
        lower_fence = Q1 - (k * IQR)
        upper_fence = Q3 + (k * IQR)
        
        # 6. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á‡∏ú‡∏•
        if latest_val > upper_fence:
            return "High_Spike"
        
        # ‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏Ñ‡πà‡∏≤‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤ 0
        lower_fence_adjusted = max(0, lower_fence) 

        if latest_val < lower_fence_adjusted:
            return "Low_Spike"
        
        return "Normal"
    
    def create_anomaly_report_sheets(self, anomaly_results, output_file):
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á sheet ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÉ‡∏ô Excel ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Anomaly Detection Report
        """
        self.log("=" * 80)
        self.log("‡∏™‡∏£‡πâ‡∏≤‡∏á Anomaly Detection Report Sheets")
        self.log("=" * 80)
        
        try:
            from openpyxl import load_workbook
            from openpyxl.styles import Font, PatternFill, Alignment
            from openpyxl.utils import get_column_letter
        except ImportError:
            self.log("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö openpyxl, ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á Anomaly Sheets ‡πÑ‡∏î‡πâ")
            self.log("  ‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏î‡πâ‡∏ß‡∏¢: pip install openpyxl")
            return

        
        # ‡πÄ‡∏õ‡∏¥‡∏î‡πÑ‡∏ü‡∏•‡πå Excel ‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ß‡πâ‡πÅ‡∏•‡πâ‡∏ß
        try:
            wb = load_workbook(output_file)
        except Exception as e:
            self.log(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏õ‡∏¥‡∏î‡πÑ‡∏ü‡∏•‡πå Excel ‡πÑ‡∏î‡πâ: {e}")
            return
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Summary Sheet
        self._create_anomaly_summary_sheet(wb, anomaly_results)
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Detail Sheets ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏£‡∏∞‡∏î‡∏±‡∏ö
        for level_name, df_result in anomaly_results.items():
            self._create_anomaly_detail_sheet(wb, level_name, df_result)
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå
        try:
            wb.save(output_file)
            self.log(f"‚úì ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Anomaly Report ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢: {output_file}")
        except Exception as e:
            self.log(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå Excel ‡πÑ‡∏î‡πâ (‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÄ‡∏õ‡∏¥‡∏î‡∏Ñ‡πâ‡∏≤‡∏á‡∏≠‡∏¢‡∏π‡πà): {e}")
    
    def _create_anomaly_summary_sheet(self, wb, anomaly_results):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á Summary Sheet ‡πÅ‡∏™‡∏î‡∏á‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö"""
        
        from openpyxl.styles import Font, PatternFill, Alignment

        if 'Anomaly Summary' in wb.sheetnames:
            try:
                del wb['Anomaly Summary']
            except Exception as e:
                self.log(f"  Warning: ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏•‡∏ö sheet 'Anomaly Summary' ‡πÄ‡∏Å‡πà‡∏≤‡πÑ‡∏î‡πâ: {e}")

        ws = wb.create_sheet('Anomaly Summary', 0) # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ß‡πâ‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏£‡∏Å
        
        # Header
        ws['A1'] = 'ANOMALY DETECTION SUMMARY'
        ws['A1'].font = Font(bold=True, size=14, color='FFFFFF')
        ws['A1'].fill = PatternFill(start_color='1F4E78', end_color='1F4E78', fill_type='solid')
        ws['A1'].alignment = Alignment(horizontal='center', vertical='center')
        ws.merge_cells('A1:F1')
        
        # Column headers
        headers = ['Level', 'Total Items', 'Normal', 'High Spike', 'Low Spike', 'Other Issues']
        for col_idx, header in enumerate(headers, 1):
            cell = ws.cell(row=3, column=col_idx, value=header)
            cell.font = Font(bold=True)
            cell.fill = PatternFill(start_color='D9E1F2', end_color='D9E1F2', fill_type='solid')
            cell.alignment = Alignment(horizontal='center')
        
        # Data
        row = 4
        if not anomaly_results:
            ws.cell(row=row, column=1, value="‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö")
            return

        for level_name, df_result in anomaly_results.items():
            status_counts = df_result['ANOMALY_STATUS'].value_counts()
            
            ws.cell(row=row, column=1, value=level_name.upper())
            ws.cell(row=row, column=2, value=len(df_result))
            ws.cell(row=row, column=3, value=status_counts.get('Normal', 0))
            ws.cell(row=row, column=4, value=status_counts.get('High_Spike', 0))
            ws.cell(row=row, column=5, value=status_counts.get('Low_Spike', 0))
            
            other = (
                status_counts.get('Negative_Value', 0) +
                status_counts.get('Not_Enough_Data', 0) +
                status_counts.get('Spike_vs_Constant', 0) +
                status_counts.get('New_Item', 0)
            )
            ws.cell(row=row, column=6, value=other)
            
            # Highlight ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤
            if (status_counts.get('High_Spike', 0) > 0 or 
                status_counts.get('Low_Spike', 0) > 0 or
                status_counts.get('Negative_Value', 0) > 0):
                for col in range(1, 7):
                    ws.cell(row=row, column=col).fill = PatternFill(
                        start_color='FFF2CC', end_color='FFF2CC', fill_type='solid'
                    )
            
            row += 1
        
        # ‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
        ws.column_dimensions['A'].width = 20
        for col in ['B', 'C', 'D', 'E', 'F']:
            ws.column_dimensions[col].width = 15
    
    def _create_anomaly_detail_sheet(self, wb, level_name, df_result):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á Detail Sheet ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏£‡∏∞‡∏î‡∏±‡∏ö"""
        
        from openpyxl.styles import Font, PatternFill, Alignment
        from openpyxl.utils import get_column_letter

        sheet_name = f'Anomaly_{level_name.title()}'
        
        if sheet_name in wb.sheetnames:
            try:
                del wb[sheet_name]
            except Exception as e:
                self.log(f"  Warning: ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏•‡∏ö sheet '{sheet_name}' ‡πÄ‡∏Å‡πà‡∏≤‡πÑ‡∏î‡πâ: {e}")
        
        ws = wb.create_sheet(sheet_name)
        
        # ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥
        df_anomaly = df_result[~df_result['ANOMALY_STATUS'].isin(['Normal', 'Not_Enough_Data'])].copy()
        
        if len(df_anomaly) == 0:
            ws['A1'] = f'‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö {level_name.upper()}'
            ws['A1'].font = Font(bold=True, size=12, color='008000')
            return
        
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏° absolute % change
        df_anomaly['ABS_PCT_CHANGE'] = df_anomaly['PCT_CHANGE'].abs()
        df_anomaly = df_anomaly.sort_values('ABS_PCT_CHANGE', ascending=False)
        
        # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÅ‡∏™‡∏î‡∏á
        display_cols = []
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏° group columns
        group_cols = self.config.ANOMALY_LEVELS[level_name]["group_by"]
        if group_cols:
            display_cols.extend(group_cols)
        else:
            display_cols.append('LEVEL')
        
        display_cols.extend([
            'ANOMALY_STATUS', 'LATEST_VALUE', 'AVG_HISTORICAL', 'PCT_CHANGE', 'ROLLING_AVG', 'PCT_ROLLING'
        ])
        
        df_display = df_anomaly[display_cols].copy()
        
        # ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á Excel
        # Headers
        for col_idx, col_name in enumerate(df_display.columns, 1):
            cell = ws.cell(row=1, column=col_idx, value=col_name)
            cell.font = Font(bold=True, color='FFFFFF')
            cell.fill = PatternFill(start_color='1F4E78', end_color='1F4E78', fill_type='solid')
            cell.alignment = Alignment(horizontal='center', vertical='center')
        
        # Data
        for row_idx, (_, row) in enumerate(df_display.iterrows(), 2):
            for col_idx, value in enumerate(row, 1):
                cell = ws.cell(row=row_idx, column=col_idx, value=value)
                
                # ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
                if col_idx > len(group_cols):
                    # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ PCT ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏™‡πà %
                    if 'PCT' in df_display.columns[col_idx-1]:
                        cell.number_format = '0.00"%"'
                    else:
                        cell.number_format = '#,##0.00'
                    cell.alignment = Alignment(horizontal='right')
                
                # Highlight ‡∏ï‡∏≤‡∏° status
                status = row['ANOMALY_STATUS']
                if status == 'High_Spike':
                    cell.fill = PatternFill(start_color='FFC7CE', end_color='FFC7CE', fill_type='solid') # Red
                elif status == 'Low_Spike':
                    cell.fill = PatternFill(start_color='FFEB9C', end_color='FFEB9C', fill_type='solid') # Yellow
                elif status == 'Negative_Value':
                    cell.fill = PatternFill(start_color='FF0000', end_color='FF0000', fill_type='solid') # Bright Red
                    cell.font = Font(color='FFFFFF')
                elif status == 'Spike_vs_Constant':
                    cell.fill = PatternFill(start_color='FFC7CE', end_color='FFC7CE', fill_type='solid') # Red
                elif status == 'New_Item':
                    cell.fill = PatternFill(start_color='C6E0B4', end_color='C6E0B4', fill_type='solid') # Green
        
        # ‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
        for col_idx, col_name in enumerate(df_display.columns, 1):
            width = 20
            if col_name in ('PRODUCT_NAME', 'SERVICE_GROUP', 'BUSINESS_GROUP'):
                width = 35
            ws.column_dimensions[get_column_letter(col_idx)].width = width
    
    def run(self):
        """‡∏£‡∏±‡∏ô ETL Pipeline ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ‡∏û‡∏£‡πâ‡∏≠‡∏° Anomaly Detection"""
        try:
            start_time = datetime.now()
            self.log(f"‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô ETL Pipeline ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏µ {self.config.YEAR}")
            self.log(f"‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô: {start_time}")
            
            # Step 1-4: ETL Process (Step 4 ‡∏ñ‡∏π‡∏Å‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÉ‡∏´‡πâ‡∏£‡∏ß‡∏° ADJ data)
            df = self.step1_concat_revenue_files()

            # ============================================================
            # Step 0: Reconciliation (‡∏´‡∏•‡∏±‡∏á step1, ‡∏Å‡πà‡∏≠‡∏ô step2) <<< ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ
            # ============================================================
            try:
                self.step0_reconcile_revenue()
            except ReconciliationError as e:
                self.log("\n" + str(e))
                self.log("\n‚ùå ETL Pipeline ‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô: Reconciliation Failed")
                self.log("üí° ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡πÅ‡∏•‡πâ‡∏ß‡∏£‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà")
                raise

            df = self.step2_mapping_cost_center(df)
            df = self.step3_mapping_product(df)
            df_final = self.step4_create_final_report(df)
            
            # Step 5: Anomaly Detection (‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏£‡∏±‡∏ô‡∏ö‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• NT1 + ADJ)
            self.log("\n" + "=" * 80)
            self.log("STEP 5: Anomaly Detection")
            self.log("=" * 80)
            
            anomaly_results = self.detect_anomalies(df_final)
            
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å anomaly results ‡πÄ‡∏õ‡πá‡∏ô CSV
            for level_name, df_result in anomaly_results.items():
                if df_result.empty:
                    self.log(f"‡∏Ç‡πâ‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å anomaly level '{level_name}' (‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•)")
                    continue
                output_file = os.path.join(
                    self.paths["final_output"], 
                    f"anomaly_{level_name}_{self.config.YEAR}.csv"
                )
                df_result.to_csv(output_file, index=False)
                self.log(f"‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏• anomaly detection: {output_file}")
            
            end_time = datetime.now()
            duration = end_time - start_time
            
            self.log("=" * 80)
            self.log(f"‚úì ETL Pipeline ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!")
            self.log(f"‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ: {duration}")
            self.log("=" * 80)
            
            return df_final, anomaly_results
            
        except Exception as e:
            self.log(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏£‡πâ‡∏≤‡∏¢‡πÅ‡∏£‡∏á‡πÉ‡∏ô ETL Pipeline: {str(e)}")
            import traceback
            traceback.print_exc()
            raise


# ============================================================================
# Main Execution
# ============================================================================
if __name__ == "__main__":
    import numpy as np
    try:
        from openpyxl import load_workbook
        from openpyxl.styles import Font, PatternFill, Alignment, Border, Side
        from openpyxl.utils import get_column_letter
    except ImportError:
        print("="*80)
        print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏°‡∏î‡∏π‡∏• openpyxl")
        print("  ‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á: pip install openpyxl")
        print("  ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÑ‡∏ü‡∏•‡πå Excel ‡πÑ‡∏î‡πâ")
        print("="*80)
        # ‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ openpyxl
        exit()
        
    from datetime import datetime
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á ETL instance ‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏ô Pipeline
    etl = RevenueETL()
    df_result, anomaly_results = etl.run()

    historical_anomalies = {}
    if Config.ENABLE_HISTORICAL_HIGHLIGHT:
        historical_anomalies = etl.detect_historical_anomalies(df_result)
    
    # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• YTD ‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ß‡πâ‡πÉ‡∏ô step 4
    df_adj_ytd = etl.df_adj_ytd
    
    print("\n" + "=" * 80)
    print("‡∏™‡∏£‡πâ‡∏≤‡∏á Excel Report...")
    print("=" * 80)
    
    # ============================================================================
    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå Excel ‡∏ó‡∏µ‡πà‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
    # ============================================================================
    excel_output_file = os.path.join(
        etl.paths["final_output"], 
        f"revenue_report_{Config.YEAR}.xlsx"
    )
    print(f"‡πÑ‡∏ü‡∏•‡πå Excel ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á: {excel_output_file}")
    
    # ============================================================================
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á Excel Report
    # ============================================================================
    
    # ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    df = df_result.copy()
    
    # Aggregate ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏° hierarchy ‡πÅ‡∏•‡∏∞‡πÄ‡∏î‡∏∑‡∏≠‡∏ô
    # (‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ agg_data ‡∏à‡∏∞‡∏°‡∏µ NEW_ADJ_BUSINESS_GROUP ‡∏£‡∏ß‡∏°‡∏≠‡∏¢‡∏π‡πà‡∏î‡πâ‡∏ß‡∏¢)
    agg_data = df.groupby([
        'ITEM', 'BUSINESS_GROUP', 
        'SUB_ITEM', 'SERVICE_GROUP', 
        'PRODUCT_KEY', 'PRODUCT_NAME',
        'YEAR', 'MONTH'
    ])['AMOUNT'].sum().reset_index()
    
    # ‡πÅ‡∏õ‡∏•‡∏á MONTH ‡πÄ‡∏õ‡πá‡∏ô int
    agg_data['MONTH'] = agg_data['MONTH'].astype(int)
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö DD/MM/YYYY
    agg_data['DATE_STR'] = agg_data.apply(
        lambda row: f"01/{row['MONTH']:02d}/{row['YEAR']}", 
        axis=1
    )
    

    # [== START MODIFICATION: ‡∏≠‡∏±‡∏õ‡πÄ‡∏Å‡∏£‡∏î create_report ==]
    def create_report(agg_data, anomaly_results, df_adj_ytd, sort_ascending=True):
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏£‡∏≤‡∏¢‡πÄ‡∏î‡∏∑‡∏≠‡∏ô
        ‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏≠‡∏±‡∏õ‡πÄ‡∏Å‡∏£‡∏î:
        - ‡∏£‡∏ß‡∏° ANOMALY_STATUS
        - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÅ‡∏ñ‡∏ß "‡∏£‡∏ß‡∏°‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£"
        - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏•‡∏∏‡πà‡∏° "‡∏ú‡∏•‡∏ï‡∏≠‡∏ö‡πÅ‡∏ó‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏≠‡∏∑‡πà‡∏ô"
        - ‡πÉ‡∏ä‡πâ YTD ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå "‡∏ú‡∏•‡∏£‡∏ß‡∏°" ‡∏Ç‡∏≠‡∏á‡∏Å‡∏•‡∏∏‡πà‡∏° ADJ
        """
        
        print(f"  ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô (sort_ascending={sort_ascending})...")
        
        # --- 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á Anomaly Maps ---
        print("    ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Anomaly Maps...")
        try:
            # Product Map
            df_prod = anomaly_results['product'][['BUSINESS_GROUP', 'SERVICE_GROUP', 'PRODUCT_KEY', 'ANOMALY_STATUS']]
            prod_map = df_prod.set_index(['BUSINESS_GROUP', 'SERVICE_GROUP', 'PRODUCT_KEY'])['ANOMALY_STATUS'].to_dict()
            
            # Service Map
            df_serv = anomaly_results['service'][['BUSINESS_GROUP', 'SERVICE_GROUP', 'ANOMALY_STATUS']]
            serv_map = df_serv.set_index(['BUSINESS_GROUP', 'SERVICE_GROUP'])['ANOMALY_STATUS'].to_dict()
            
            # Business Map
            df_biz = anomaly_results['business'][['BUSINESS_GROUP', 'ANOMALY_STATUS']]
            biz_map = df_biz.set_index(['BUSINESS_GROUP'])['ANOMALY_STATUS'].to_dict()
            
            # Grand Total Status
            grand_total_status = anomaly_results['grand_total']['ANOMALY_STATUS'].values[0]
            
        except KeyError as e:
            print(f"    Warning: ‡πÑ‡∏°‡πà‡∏û‡∏ö anomaly key {e}, ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á")
            prod_map, serv_map, biz_map, grand_total_status = {}, {}, {}, ''
        except Exception as e:
            print(f"    Error creating anomaly maps: {e}")
            prod_map, serv_map, biz_map, grand_total_status = {}, {}, {}, ''
        
        # --- 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á YTD Map ---
        print("    ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á YTD data map...")
        ytd_map = {}
        if df_adj_ytd is not None and not df_adj_ytd.empty:
            
            # [ START FIX ]
            # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'TYPE' ‡πÅ‡∏•‡∏∞ 'REVENUE_TYPE' ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô‡∏Å‡∏±‡∏ô
            if "REVENUE_TYPE" in df_adj_ytd.columns and "TYPE" in df_adj_ytd.columns:
                print("    YTD Map: ‡∏û‡∏ö‡∏ó‡∏±‡πâ‡∏á 'TYPE' ‡πÅ‡∏•‡∏∞ 'REVENUE_TYPE', ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏£‡∏ß‡∏°...")
                df_adj_ytd['TYPE'] = df_adj_ytd['TYPE'].fillna(df_adj_ytd['REVENUE_TYPE'])
                df_adj_ytd = df_adj_ytd.drop(columns=['REVENUE_TYPE'])
            elif "REVENUE_TYPE" in df_adj_ytd.columns:
                print("    YTD Map: ‡∏û‡∏ö 'REVENUE_TYPE', ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡πá‡∏ô 'TYPE'...")
                df_adj_ytd = df_adj_ytd.rename(columns={"REVENUE_TYPE": "TYPE"})
            # [ END FIX ]

            if "TYPE" in df_adj_ytd.columns:
                # Sum by TYPE ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏ñ‡∏ß (‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡πÄ‡∏Å‡∏¥‡∏î Error)
                ytd_summary = df_adj_ytd.groupby("TYPE")["REVENUE_VALUE"].sum()
                ytd_map[Config.FINANCIAL_INCOME_NAME] = ytd_summary.get(Config.FINANCIAL_INCOME_NAME, 0)
                ytd_map[Config.OTHER_REVENUE_ADJ_NAME] = ytd_summary.get(Config.OTHER_REVENUE_ADJ_NAME, 0)
            else:
                print("    Warning: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'TYPE' ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå YTD")
        
        print(f"    YTD Map: {ytd_map}")

        
        # --- 3. ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞ Pivot ---
        if sort_ascending:
            agg_data_sorted = agg_data.sort_values(['YEAR', 'MONTH'], ascending=[True, True])
        else:
            agg_data_sorted = agg_data.sort_values(['YEAR', 'MONTH'], ascending=[True, False])
        
        pivot = agg_data_sorted.pivot_table(
            index=['ITEM', 'BUSINESS_GROUP', 'SUB_ITEM', 'SERVICE_GROUP', 'PRODUCT_KEY', 'PRODUCT_NAME'],
            columns='DATE_STR',
            values='AMOUNT',
            aggfunc='sum',
            fill_value=0
        )
        
        month_cols = agg_data_sorted['DATE_STR'].unique().tolist()
        pivot = pivot[month_cols]
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ú‡∏•‡∏£‡∏ß‡∏° (Sum(‡πÄ‡∏î‡∏∑‡∏≠‡∏ô)) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å‡πÅ‡∏ñ‡∏ß
        pivot.insert(0, '‡∏ú‡∏•‡∏£‡∏ß‡∏°', pivot.sum(axis=1))
        result = pivot.reset_index()

        
        # --- 4. ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏û‡∏£‡πâ‡∏≠‡∏° Status ---
        rows = []
        
        # ‡πÅ‡∏¢‡∏Å‡∏Å‡∏•‡∏∏‡πà‡∏° NT1 ‡∏õ‡∏Å‡∏ï‡∏¥ ‡πÅ‡∏•‡∏∞‡∏Å‡∏•‡∏∏‡πà‡∏° ADJ
        standard_groups_df = result[result['BUSINESS_GROUP'] != Config.NEW_ADJ_BUSINESS_GROUP]
        adj_group_df = result[result['BUSINESS_GROUP'] == Config.NEW_ADJ_BUSINESS_GROUP]
        
        grouped_items = standard_groups_df.groupby(['ITEM', 'BUSINESS_GROUP'])
        
        # ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì "‡∏£‡∏ß‡∏°‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£"
        total_service_revenue_data = pd.DataFrame() 
        
        # --- 4a. ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡∏õ‡∏Å‡∏ï‡∏¥ (NT1) ---
        print("    ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡∏õ‡∏Å‡∏ï‡∏¥ (NT1)")
        for (item, business_group), item_data in grouped_items:
            grouped_sub_items = item_data.groupby(['SUB_ITEM', 'SERVICE_GROUP'])
            
            for (sub_item, service_group), sub_item_data in grouped_sub_items:
                # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÅ‡∏ñ‡∏ß‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ product
                for _, row in sub_item_data.iterrows():
                    
                    prod_key = (row['BUSINESS_GROUP'], row['SERVICE_GROUP'], row['PRODUCT_KEY'])
                    prod_status = prod_map.get(prod_key, '')
                    
                    row_dict = {
                        '‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à': row['BUSINESS_GROUP'],
                        '‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£': row['SERVICE_GROUP'],
                        '‡∏£‡∏´‡∏±‡∏™‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£': row['PRODUCT_KEY'],
                        '‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£': row['PRODUCT_NAME'],
                    }
                    
                    if sort_ascending:
                        row_dict['‡∏ú‡∏•‡∏£‡∏ß‡∏°'] = row['‡∏ú‡∏•‡∏£‡∏ß‡∏°'] # ‡πÉ‡∏ä‡πâ‡∏ú‡∏•‡∏£‡∏ß‡∏°‡∏õ‡∏Å‡∏ï‡∏¥ (sum(‡πÄ‡∏î‡∏∑‡∏≠‡∏ô))
                        for col in month_cols:
                            row_dict[col] = row[col]
                        row_dict['ANOMALY_STATUS'] = prod_status
                    else:
                        row_dict['‡∏ú‡∏•‡∏£‡∏ß‡∏°'] = row['‡∏ú‡∏•‡∏£‡∏ß‡∏°'] # ‡πÉ‡∏ä‡πâ‡∏ú‡∏•‡∏£‡∏ß‡∏°‡∏õ‡∏Å‡∏ï‡∏¥ (sum(‡πÄ‡∏î‡∏∑‡∏≠‡∏ô))
                        row_dict['ANOMALY_STATUS'] = prod_status
                        for col in month_cols:
                            row_dict[col] = row[col]
                    
                    rows.append(row_dict)
                
                # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÅ‡∏ñ‡∏ß‡∏ú‡∏•‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£
                serv_key = (business_group, service_group)
                serv_status = serv_map.get(serv_key, '')
                
                sum_row = {
                    '‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à': '',
                    '‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£': f'‡∏£‡∏ß‡∏° {service_group}',
                    '‡∏£‡∏´‡∏±‡∏™‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£': '',
                    '‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£': '',
                }

                if sort_ascending:
                    sum_row['‡∏ú‡∏•‡∏£‡∏ß‡∏°'] = sub_item_data['‡∏ú‡∏•‡∏£‡∏ß‡∏°'].sum()
                    for col in month_cols:
                        sum_row[col] = sub_item_data[col].sum()
                    sum_row['ANOMALY_STATUS'] = serv_status
                else:
                    sum_row['‡∏ú‡∏•‡∏£‡∏ß‡∏°'] = sub_item_data['‡∏ú‡∏•‡∏£‡∏ß‡∏°'].sum()
                    sum_row['ANOMALY_STATUS'] = serv_status
                    for col in month_cols:
                        sum_row[col] = sub_item_data[col].sum()
                
                rows.append(sum_row)

                # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö "‡∏£‡∏ß‡∏°‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£"
                if service_group != Config.NON_TELECOM_SERVICE_GROUP:
                    total_service_revenue_data = pd.concat([
                        total_service_revenue_data, 
                        sub_item_data
                    ])
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÅ‡∏ñ‡∏ß‡∏ú‡∏•‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à
            biz_key = business_group
            biz_status = biz_map.get(biz_key, '')
            
            sum_row = {
                '‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à': f'‡∏£‡∏ß‡∏° {business_group}',
                '‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£': '',
                '‡∏£‡∏´‡∏±‡∏™‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£': '',
                '‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£': '',
            }

            if sort_ascending:
                sum_row['‡∏ú‡∏•‡∏£‡∏ß‡∏°'] = item_data['‡∏ú‡∏•‡∏£‡∏ß‡∏°'].sum()
                for col in month_cols:
                    sum_row[col] = item_data[col].sum()
                sum_row['ANOMALY_STATUS'] = biz_status
            else:
                sum_row['‡∏ú‡∏•‡∏£‡∏ß‡∏°'] = item_data['‡∏ú‡∏•‡∏£‡∏ß‡∏°'].sum()
                sum_row['ANOMALY_STATUS'] = biz_status
                for col in month_cols:
                    sum_row[col] = item_data[col].sum()
            
            rows.append(sum_row)
        
        # --- 4b. ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÅ‡∏ñ‡∏ß "‡∏£‡∏ß‡∏°‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£" ---
        print("    ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÅ‡∏ñ‡∏ß '‡∏£‡∏ß‡∏°‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£'")
        sum_row_service = {
            '‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à': '‡∏£‡∏ß‡∏°‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£',
            '‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£': '',
            '‡∏£‡∏´‡∏±‡∏™‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£': '',
            '‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£': '',
        }

        if sort_ascending:
            sum_row_service['‡∏ú‡∏•‡∏£‡∏ß‡∏°'] = total_service_revenue_data['‡∏ú‡∏•‡∏£‡∏ß‡∏°'].sum()
            for col in month_cols:
                sum_row_service[col] = total_service_revenue_data[col].sum()
            sum_row_service['ANOMALY_STATUS'] = '' # ‡πÑ‡∏°‡πà‡∏°‡∏µ Anomaly
        else:
            sum_row_service['‡∏ú‡∏•‡∏£‡∏ß‡∏°'] = total_service_revenue_data['‡∏ú‡∏•‡∏£‡∏ß‡∏°'].sum()
            sum_row_service['ANOMALY_STATUS'] = '' # ‡πÑ‡∏°‡πà‡∏°‡∏µ Anomaly
            for col in month_cols:
                sum_row_service[col] = total_service_revenue_data[col].sum()
        
        rows.append(sum_row_service)

        # --- 4c. ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Å‡∏•‡∏∏‡πà‡∏° ADJ ---
        print("    ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Å‡∏•‡∏∏‡πà‡∏° '‡∏ú‡∏•‡∏ï‡∏≠‡∏ö‡πÅ‡∏ó‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏≠‡∏∑‡πà‡∏ô'")
        if not adj_group_df.empty:
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÅ‡∏ñ‡∏ß‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏Å‡∏•‡∏∏‡πà‡∏°
            adj_header_row = {
                '‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à': Config.NEW_ADJ_BUSINESS_GROUP,
                '‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£': '', '‡∏£‡∏´‡∏±‡∏™‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£': '', '‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£': ''
            }
            if sort_ascending:
                adj_header_row['‡∏ú‡∏•‡∏£‡∏ß‡∏°'] = ''
                for col in month_cols: adj_header_row[col] = ''
                adj_header_row['ANOMALY_STATUS'] = ''
            else:
                adj_header_row['‡∏ú‡∏•‡∏£‡∏ß‡∏°'] = ''
                adj_header_row['ANOMALY_STATUS'] = ''
                for col in month_cols: adj_header_row[col] = ''
            rows.append(adj_header_row)

            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö (‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏™‡∏•‡∏±‡∏ö)
            adj_group_df = adj_group_df.sort_values(by='SERVICE_GROUP', ascending=True)

            for _, row in adj_group_df.iterrows():
                service_group = row['SERVICE_GROUP'] # "‡∏ú‡∏•‡∏ï‡∏≠‡∏ö‡πÅ‡∏ó‡∏ô..." or "‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏≠‡∏∑‡πà‡∏ô"
                
                # Get anomaly status
                serv_key = (Config.NEW_ADJ_BUSINESS_GROUP, service_group)
                serv_status = serv_map.get(serv_key, '')
                
                row_dict = {
                    '‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à': '',
                    '‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£': service_group,
                    '‡∏£‡∏´‡∏±‡∏™‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£': '',
                    '‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£': '',
                }
                
                # --- YTD LOGIC ---
                # ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤ YTD ‡∏à‡∏≤‡∏Å map, ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ä‡πâ 0
                ytd_value = ytd_map.get(service_group, 0)
                
                if sort_ascending:
                    row_dict['‡∏ú‡∏•‡∏£‡∏ß‡∏°'] = ytd_value # << ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ YTD
                    for col in month_cols:
                        row_dict[col] = row[col] # ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÄ‡∏î‡∏∑‡∏≠‡∏ô ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ sum ‡∏õ‡∏Å‡∏ï‡∏¥
                    row_dict['ANOMALY_STATUS'] = serv_status
                else:
                    row_dict['‡∏ú‡∏•‡∏£‡∏ß‡∏°'] = ytd_value # << ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ YTD
                    row_dict['ANOMALY_STATUS'] = serv_status
                    for col in month_cols:
                        row_dict[col] = row[col]
                
                rows.append(row_dict)

            # [ START NEW CODE ]
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÅ‡∏ñ‡∏ß "‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏ï‡∏≠‡∏ö‡πÅ‡∏ó‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏≠‡∏∑‡πà‡∏ô"
            sum_adj_total_ytd = sum(ytd_map.values())
            
            sum_row_adj = {
                '‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à': '',
                '‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£': '‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏ï‡∏≠‡∏ö‡πÅ‡∏ó‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏≠‡∏∑‡πà‡∏ô',
                '‡∏£‡∏´‡∏±‡∏™‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£': '',
                '‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£': '',
            }
            
            # Get anomaly status for the whole ADJ group
            biz_key = Config.NEW_ADJ_BUSINESS_GROUP
            biz_status = biz_map.get(biz_key, '') # ‡πÉ‡∏ä‡πâ Anomaly ‡∏Ç‡∏≠‡∏á‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à

            if sort_ascending:
                sum_row_adj['‡∏ú‡∏•‡∏£‡∏ß‡∏°'] = sum_adj_total_ytd # Sum of YTD values
                for col in month_cols:
                    sum_row_adj[col] = adj_group_df[col].sum() # Sum of monthly values
                sum_row_adj['ANOMALY_STATUS'] = biz_status
            else:
                sum_row_adj['‡∏ú‡∏•‡∏£‡∏ß‡∏°'] = sum_adj_total_ytd # Sum of YTD values
                sum_row_adj['ANOMALY_STATUS'] = biz_status
                for col in month_cols:
                    sum_row_adj[col] = adj_group_df[col].sum() # Sum of monthly values
            
            rows.append(sum_row_adj)
            # [ END NEW CODE ]

        
        # --- 4d. ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÅ‡∏ñ‡∏ß‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏¥‡πâ‡∏ô ---
        print("    ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÅ‡∏ñ‡∏ß '‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏¥‡πâ‡∏ô'")
        
        # "‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏¥‡πâ‡∏ô" = ‡∏ú‡∏•‡∏£‡∏ß‡∏° NT1 ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î + ‡∏ú‡∏•‡∏£‡∏ß‡∏° YTD ‡∏Ç‡∏≠‡∏á ADJ
        
        # 1. ‡∏ú‡∏•‡∏£‡∏ß‡∏° NT1 (‡∏à‡∏≤‡∏Å standard_groups_df)
        nt1_total = standard_groups_df['‡∏ú‡∏•‡∏£‡∏ß‡∏°'].sum()
        
        # 2. ‡∏ú‡∏•‡∏£‡∏ß‡∏° YTD ‡∏Ç‡∏≠‡∏á ADJ (‡∏à‡∏≤‡∏Å ytd_map)
        adj_total_ytd = sum(ytd_map.values())
        
        report_grand_total = nt1_total + adj_total_ytd
        
        sum_row = {
            '‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à': '‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏¥‡πâ‡∏ô',
            '‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£': '',
            '‡∏£‡∏´‡∏±‡∏™‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£': '',
            '‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£': '',
        }
        
        if sort_ascending:
            sum_row['‡∏ú‡∏•‡∏£‡∏ß‡∏°'] = report_grand_total # << ‡πÉ‡∏ä‡πâ‡∏¢‡∏≠‡∏î‡∏£‡∏ß‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÉ‡∏´‡∏°‡πà
            for col in month_cols:
                sum_row[col] = result[col].sum() # ‡∏¢‡∏≠‡∏î‡∏£‡∏ß‡∏°‡∏£‡∏≤‡∏¢‡πÄ‡∏î‡∏∑‡∏≠‡∏ô (NT1+ADJ)
            sum_row['ANOMALY_STATUS'] = grand_total_status
        else:
            sum_row['‡∏ú‡∏•‡∏£‡∏ß‡∏°'] = report_grand_total # << ‡πÉ‡∏ä‡πâ‡∏¢‡∏≠‡∏î‡∏£‡∏ß‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÉ‡∏´‡∏°‡πà
            sum_row['ANOMALY_STATUS'] = grand_total_status
            for col in month_cols:
                sum_row[col] = result[col].sum() # ‡∏¢‡∏≠‡∏î‡∏£‡∏ß‡∏°‡∏£‡∏≤‡∏¢‡πÄ‡∏î‡∏∑‡∏≠‡∏ô (NT1+ADJ)
        
        rows.append(sum_row)
        
        return pd.DataFrame(rows)
    
    # [== END MODIFICATION ==]


    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô 2 ‡πÅ‡∏ö‡∏ö
    print("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô...")
    # **‡∏™‡πà‡∏á df_adj_ytd ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô function**
    report_asc = create_report(agg_data, anomaly_results, df_adj_ytd, sort_ascending=True)
    report_desc = create_report(agg_data, anomaly_results, df_adj_ytd, sort_ascending=False)
    
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏õ‡πá‡∏ô Excel (‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ excel_output_file)
    try:
        with pd.ExcelWriter(excel_output_file, engine='openpyxl') as writer:
            report_asc.to_excel(writer, sheet_name='‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏ô‡πâ‡∏≠‡∏¢-‡∏°‡∏≤‡∏Å', index=False)
            report_desc.to_excel(writer, sheet_name='‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏°‡∏≤‡∏Å-‡∏ô‡πâ‡∏≠‡∏¢', index=False)
        print(f"‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß: {excel_output_file}")
    except Exception as e:
        print(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå Excel ‡πÑ‡∏î‡πâ (‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÄ‡∏õ‡∏¥‡∏î‡∏Ñ‡πâ‡∏≤‡∏á‡∏≠‡∏¢‡∏π‡πà): {e}")
        # ‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ñ‡πâ‡∏≤‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ
        exit()
    
    # Format Excel
    def format_excel(filename, anomaly_map=None):
        """‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö Excel ‡πÉ‡∏´‡πâ‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°"""
        print("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö Excel...")
        try:
            wb = load_workbook(filename)
        except Exception as e:
            print(f"  ‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏õ‡∏¥‡∏î‡πÑ‡∏ü‡∏•‡πå Excel ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÑ‡∏î‡πâ: {e}")
            return
            
        for sheet_name in wb.sheetnames:
            # ‡∏Ç‡πâ‡∏≤‡∏° anomaly sheets (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
            if sheet_name.startswith('Anomaly') or sheet_name == 'Anomaly Summary':
                continue
                
            ws = wb[sheet_name]
            
            # ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö header
            header_fill = PatternFill(start_color='366092', end_color='366092', fill_type='solid')
            header_font = Font(bold=True, color='FFFFFF', size=11)
            
            for cell in ws[1]:
                cell.fill = header_fill
                cell.font = header_font
                cell.alignment = Alignment(horizontal='center', vertical='center', wrap_text=True)
            
            
            # ‚úÖ FIX: ‡∏´‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
            text_columns = {'‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à', '‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£', '‡∏£‡∏´‡∏±‡∏™‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£', '‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£', 'ANOMALY_STATUS'}
            
            # ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà text
            for row_idx, row in enumerate(ws.iter_rows(min_row=2, max_row=ws.max_row), 2):
                for col_idx, cell in enumerate(row, 1):
                    # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÑ‡∏´‡∏°
                    header_value = ws.cell(row=1, column=col_idx).value
                    
                    if header_value not in text_columns:
                        if cell.value and isinstance(cell.value, (int, float)):
                            cell.number_format = '#,##0.00'
                            cell.alignment = Alignment(horizontal='right')
            
            # --- [ START MODIFICATION ] ---
            # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏™‡∏µ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏£‡∏∞‡∏î‡∏±‡∏ö
            service_group_fill = PatternFill(start_color='E8F1F8', end_color='E8F1F8', fill_type='solid') # ‡∏ü‡πâ‡∏≤‡∏≠‡πà‡∏≠‡∏ô
            service_group_font = Font(bold=True, size=10)
            
            business_group_fill = PatternFill(start_color='D0E2F0', end_color='D0E2F0', fill_type='solid') # ‡∏ü‡πâ‡∏≤‡∏Å‡∏•‡∏≤‡∏á
            business_group_font = Font(bold=True, size=11)
            
            grand_total_fill = PatternFill(start_color='B8CCE4', end_color='B8CCE4', fill_type='solid') # ‡∏ü‡πâ‡∏≤‡πÄ‡∏Ç‡πâ‡∏°
            grand_total_font = Font(bold=True, size=12)
            
            total_service_fill = PatternFill(start_color='C6E0B4', end_color='C6E0B4', fill_type='solid') # ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß‡∏≠‡πà‡∏≠‡∏ô
            total_service_font = Font(bold=True, size=11, color='006100')

            adj_group_fill = PatternFill(start_color='FCE4D6', end_color='FCE4D6', fill_type='solid') # ‡∏™‡πâ‡∏°‡∏≠‡πà‡∏≠‡∏ô
            adj_group_font = Font(bold=True, size=11)
            
            adj_item_fill = PatternFill(start_color='FDF7F4', end_color='FDF7F4', fill_type='solid') # ‡∏™‡πâ‡∏°‡∏à‡∏≤‡∏á
            adj_item_font = Font(bold=False, size=10)
            
            adj_total_fill = PatternFill(start_color='F8CBAD', end_color='F8CBAD', fill_type='solid') # ‡∏™‡πâ‡∏°‡πÄ‡∏Ç‡πâ‡∏°
            adj_total_font = Font(bold=True, size=11)
            
            # --- [ END MODIFICATION ] ---

            
            # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ highlight ‡πÅ‡∏ñ‡∏ß‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó
            for row in ws.iter_rows(min_row=2, max_row=ws.max_row):
                business_value = row[0].value
                service_value = row[1].value
                
                # --- [ START MODIFICATION ] ---
                if business_value and isinstance(business_value, str):
                    if business_value.startswith('‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏¥‡πâ‡∏ô'):
                        for cell in row:
                            cell.fill = grand_total_fill
                            cell.font = grand_total_font
                    elif business_value.startswith('‡∏£‡∏ß‡∏°‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£'):
                        for cell in row:
                            cell.fill = total_service_fill
                            cell.font = total_service_font
                    elif business_value == Config.NEW_ADJ_BUSINESS_GROUP:
                        for cell in row:
                            cell.fill = adj_group_fill
                            cell.font = adj_group_font
                    elif business_value.startswith('‡∏£‡∏ß‡∏° '):
                        for cell in row:
                            cell.fill = business_group_fill
                            cell.font = business_group_font
                
                elif service_value and isinstance(service_value, str):
                    if service_value.startswith('‡∏£‡∏ß‡∏° '):
                        for cell in row:
                            cell.fill = service_group_fill
                            cell.font = service_group_font
                    elif service_value == '‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏ï‡∏≠‡∏ö‡πÅ‡∏ó‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏≠‡∏∑‡πà‡∏ô':
                        for cell in row:
                            cell.fill = adj_total_fill
                            cell.font = adj_total_font
                    elif service_value in (Config.FINANCIAL_INCOME_NAME, Config.OTHER_REVENUE_ADJ_NAME):
                         for cell in row:
                            cell.fill = adj_item_fill
                            cell.font = adj_item_font
                # --- [ END MODIFICATION ] ---
            
            # ‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
            ws.column_dimensions['A'].width = 30
            ws.column_dimensions['B'].width = 35
            ws.column_dimensions['C'].width = 15
            ws.column_dimensions['D'].width = 40
            
            # ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏≠‡∏∑‡πà‡∏ô‡πÜ
            for col_idx in range(5, ws.max_column + 1):
                ws.column_dimensions[get_column_letter(col_idx)].width = 15
            
            ws.freeze_panes = 'E2'


            # [== ‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°: FORMAT ANOMALY COLUMN ==]
            anomaly_col_idx = None
            for idx, cell in enumerate(ws[1], 1):
                if cell.value == 'ANOMALY_STATUS':
                    anomaly_col_idx = idx
                    break
            
            if anomaly_col_idx:
                try:
                    anomaly_col_letter = get_column_letter(anomaly_col_idx)

                    ws.column_dimensions[anomaly_col_letter].width = 20

                    header_cell = ws.cell(row=1, column=anomaly_col_idx)
                    header_cell.alignment = Alignment(horizontal='center', vertical='center')

                    spike_fill = PatternFill(start_color='FFC7CE', end_color='FFC7CE', fill_type='solid') # Red
                    dip_fill = PatternFill(start_color='FFEB9C', end_color='FFEB9C', fill_type='solid') # Yellow
                    neg_fill = PatternFill(start_color='FF0000', end_color='FF0000', fill_type='solid') # Bright Red

                    for row in ws.iter_rows(min_row=2, max_row=ws.max_row, min_col=anomaly_col_idx, max_col=anomaly_col_idx):
                        cell = row[0]
                        cell.alignment = Alignment(horizontal='center')
                        
                        if cell.value == 'High_Spike':
                            cell.fill = spike_fill
                        elif cell.value == 'Low_Spike':
                            cell.fill = dip_fill
                        elif cell.value == 'Negative_Value':
                            cell.fill = neg_fill
                        elif cell.value == 'Spike_vs_Constant':
                            cell.fill = spike_fill
                        elif cell.value == 'New_Item':
                            cell.fill = PatternFill(start_color='C6E0B4', end_color='C6E0B4', fill_type='solid') # Green
                            
                except Exception as e:
                    print(f"Warning: ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå Anomaly ‡πÑ‡∏î‡πâ: {e}")

            # --- [‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏™‡πà‡∏ß‡∏ô Highlight Cell Anomaly] ---
            if anomaly_map:
                print(f"  Applying anomaly highlights to {sheet_name}...")
                
                # Map column index to date string (Header row)
                col_date_map = {}
                for cell in ws[1]:
                    if cell.value and '/' in str(cell.value): 
                        col_date_map[cell.column] = str(cell.value)

                # Loop rows
                for row in ws.iter_rows(min_row=2, max_row=ws.max_row):
                    b_group = row[0].value
                    s_group = row[1].value
                    p_key = row[2].value
                    
                    row_type = None
                    lookup_key = None
                    
                    # [FIX] ‡∏õ‡∏£‡∏±‡∏ö Logic ‡πÉ‡∏´‡πâ Focus ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Product Level 
                    # ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏≤‡∏¢‡∏™‡∏µ‡∏£‡∏∞‡∏î‡∏±‡∏ö Total ‡∏≠‡∏≠‡∏Å (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ö‡∏≤‡∏¢‡∏ï‡∏≤)
                    
                    if p_key: # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ Product Key ‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ñ‡∏ß Product
                         row_type = 'product'
                         # Key ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô Tuple (Biz, Svc, Key) ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÉ‡∏ô detect_anomalies
                         lookup_key = (b_group, s_group, p_key)
                    
                    # ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏≤‡∏¢‡∏™‡∏µ‡∏£‡∏∞‡∏î‡∏±‡∏ö Service ‡∏´‡∏£‡∏∑‡∏≠ Business ‡∏î‡πâ‡∏ß‡∏¢ ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡∏¥‡∏î comment ‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á
                    # elif s_group and str(s_group).startswith('‡∏£‡∏ß‡∏° '):
                    #      # Logic ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏≤ Business Group ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ñ‡∏ß‡∏£‡∏ß‡∏° Service ‡∏Ñ‡πà‡∏≠‡∏ô‡∏Ç‡πâ‡∏≤‡∏á‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡πÉ‡∏ô Excel 
                    #      # ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ cell ‡∏î‡πâ‡∏≤‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏≠‡∏≤‡∏à‡∏ß‡πà‡∏≤‡∏á ‡∏Ç‡∏≠‡∏Ç‡πâ‡∏≤‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥
                    #      pass 
                    
                    if row_type == 'product' and lookup_key:
                        for col_idx, date_str in col_date_map.items():
                            map_key = (row_type, lookup_key, date_str)
                            
                            if map_key in anomaly_map:
                                status = anomaly_map[map_key]
                                cell_to_color = ws.cell(row=row[0].row, column=col_idx)
                                
                                if status == 'High_Spike':
                                    cell_to_color.fill = spike_fill
                                elif status == 'Low_Spike':
                                    cell_to_color.fill = dip_fill
                                elif status == 'Negative_Value':
                                    cell_to_color.fill = neg_fill
                                    cell_to_color.font = Font(color='FFFFFF')

        try:
            wb.save(filename)
            print(f"‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö Excel ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß")
        except Exception as e:
            print(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå Excel ‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÅ‡∏•‡πâ‡∏ß‡πÑ‡∏î‡πâ (‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÄ‡∏õ‡∏¥‡∏î‡∏Ñ‡πâ‡∏≤‡∏á‡∏≠‡∏¢‡∏π‡πà): {e}")

    
    format_excel(excel_output_file, anomaly_map=historical_anomalies)
    
    # ============================================================================
    # ‡πÄ‡∏û‡∏¥‡πà‡∏° Anomaly Detection Sheets (‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô)
    # ============================================================================
    print("\n" + "=" * 80)
    print("‡πÄ‡∏û‡∏¥‡πà‡∏° Anomaly Detection Report...")
    print("=" * 80)
    
    try:
        etl.create_anomaly_report_sheets(anomaly_results, excel_output_file)
    except Exception as e:
        print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏° Anomaly Sheets: {str(e)}")
        import traceback
        traceback.print_exc()
    
    # ============================================================================
    # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    # ============================================================================
    print("\n" + "=" * 80)
    print("‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:")
    print("=" * 80)
    print(f"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (NT1 + ADJ): {len(df_result):,}")
    print(f"‡∏¢‡∏≠‡∏î‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (NT1 + ADJ): {df_result['REVENUE_VALUE'].sum():,.2f}")
    print(f"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô: {df_result['MONTH'].nunique()}")
    print(f"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Product (‡∏£‡∏ß‡∏° ADJ): {df_result['PRODUCT_KEY'].nunique()}")
    
    print("\n" + "=" * 80)
    print("Anomaly Detection Summary:")
    print("=" * 80)
    if not anomaly_results:
        print("  ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Anomaly (‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏û‡∏≠)")
    else:
        for level_name, df_anomaly in anomaly_results.items():
            status_counts = df_anomaly['ANOMALY_STATUS'].value_counts()
            total = len(df_anomaly)
            normal = status_counts.get('Normal', 0) + status_counts.get('Not_Enough_Data', 0)
            anomalies = total - normal
            print(f"{level_name.upper():15s}: {anomalies:4d} / {total:4d} anomalies detected")
    
    print("\n‚úì ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!")
    print(f"Excel Report: {excel_output_file}")
    print(f"  - Sheet 1: ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏ô‡πâ‡∏≠‡∏¢-‡∏°‡∏≤‡∏Å")
    print(f"  - Sheet 2: ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏°‡∏≤‡∏Å-‡∏ô‡πâ‡∏≠‡∏¢")
    print(f"  - Sheet 3: Anomaly Summary")
    print(f"  - Sheet 4-7: Anomaly Details (Product, Service, Business, Grand Total)")