import pandas as pd
import numpy as np
import glob
import os
from pathlib import Path
from datetime import datetime
import platform
from typing import Dict, Any
from revenue_reconciliation import RevenueReconciliation, ReconciliationError
from logger_utils import ETLLogger


# ============================================================================
# CONFIG ADAPTER - ‡πÅ‡∏õ‡∏•‡∏á dict config ‡πÄ‡∏õ‡πá‡∏ô object-like access
# ============================================================================
class ConfigAdapter:
    """
    Adapter class ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏õ‡∏•‡∏á config dictionary ‡πÄ‡∏õ‡πá‡∏ô attribute-based access
    ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏î‡∏¥‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏°‡∏≤‡∏Å
    """
    def __init__(self, config_dict: Dict[str, Any]):
        """
        Args:
            config_dict: Configuration dictionary ‡∏à‡∏≤‡∏Å ConfigManager
        """
        # ETL Module config
        etl_config = config_dict

        # ‡∏õ‡∏µ‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
        self.YEAR = etl_config['year']

        # Month Settings (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö month filtering)
        self.end_month = etl_config.get('end_month', None)  # ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
        self.fi_month = etl_config.get('fi_month', None)    # ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á FI file

        # Reconciliation Settings
        reconcile = etl_config.get('reconciliation', {})
        self.RECONCILE_FI_MONTH = reconcile.get('fi_month', '10')
        self.RECONCILE_TOLERANCE = reconcile.get('tolerance', 0.00)
        self.ENABLE_RECONCILIATION = reconcile.get('enabled', True)

        # Master Files
        master_files = etl_config.get('master_files', {})
        self.MASTER_PRODUCT_FILE = f"MASTER_PRODUCT_NT_{self.YEAR}.csv"
        self.MASTER_GL_FILE = master_files.get('gl_code', '')
        self.MAPPING_CC_FILE = master_files.get('mapping_cc', '')
        self.MAPPING_PRODUCT_FILE = master_files.get('mapping_product', '')

        # Input File Patterns (‡πÉ‡∏ä‡πâ default ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô config)
        self.INPUT_FILE_PATTERNS = etl_config.get('input_file_patterns', [
            "TRN_REVENUE_NT1_*.csv",
            "TRN_REVENUE_ADJ_GL_NT1_*.csv"
        ])
        self.ADJ_MONTHLY_PATTERN = etl_config.get('adj_monthly_pattern', "TRN_REVENUE_ADJ_*.csv")
        self.ADJ_YTD_PATTERN = etl_config.get('adj_ytd_pattern', "TRN_REVENUE_ADJ_YTD_*.csv")

        # Business Rules
        business_rules = etl_config.get('business_rules', {})
        self.EXCLUDE_BUSINESS_GROUP = business_rules.get('exclude_business_group', '‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏≠‡∏∑‡πà‡∏ô')
        self.NON_TELECOM_SERVICE_GROUP = business_rules.get('non_telecom_service_group', '‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏≠‡∏∑‡πà‡∏ô‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÇ‡∏ó‡∏£‡∏Ñ‡∏°‡∏ô‡∏≤‡∏Ñ‡∏°')
        self.NEW_ADJ_BUSINESS_GROUP = business_rules.get('new_adj_business_group', '‡∏ú‡∏•‡∏ï‡∏≠‡∏ö‡πÅ‡∏ó‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏≠‡∏∑‡πà‡∏ô')
        self.FINANCIAL_INCOME_NAME = business_rules.get('financial_income_name', '‡∏ú‡∏•‡∏ï‡∏≠‡∏ö‡πÅ‡∏ó‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô')
        self.OTHER_REVENUE_ADJ_NAME = business_rules.get('other_revenue_adj_name', '‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏≠‡∏∑‡πà‡∏ô')

        # Output Files
        output_files = etl_config.get('output_files', {})
        self.OUTPUT_CONCAT_FILE = output_files.get('concat', f"trn_revenue_nt_{self.YEAR}.csv")
        self.OUTPUT_MAPPED_CC_FILE = output_files.get('mapped_cc', f"revenue_new_cc_{self.YEAR}.csv")
        self.OUTPUT_MAPPED_PRODUCT_FILE = output_files.get('mapped_product', f"revenue_mapped_product_{self.YEAR}_.csv")
        self.OUTPUT_FINAL_REPORT_FILE = output_files.get('final_report', f"REVENUE_NT_REPORT_{self.YEAR}.csv")
        self.ERROR_GL_FILE = output_files.get('error_gl', f"error_gl_REVENUE_NT_REPORT_{self.YEAR}.csv")
        self.ERROR_PRODUCT_FILE = output_files.get('error_product', f"error_product_REVENUE_NT_REPORT_{self.YEAR}.csv")

        # Required Columns
        self.REQUIRED_COLUMNS = etl_config.get('required_columns', [
            "YEAR", "MONTH", "CUSTOMER_GROUP_KEY", "PRODUCT_KEY",
            "SUB_PRODUCT_KEY", "GL_CODE", "COST_CENTER", "REVENUE_VALUE"
        ])

        # Special Mappings
        self.SPECIAL_MAPPINGS = etl_config.get('special_mappings', [
            {
                "name": "GSaaS to Other Revenue",
                "condition": {
                    "PRODUCT_KEY": "102010407",
                    "GL_CODE": "46400101"
                },
                "mapping": {
                    "PRODUCT_KEY": "292020407",
                    "SUB_PRODUCT_KEY": "1"
                }
            }
        ])

        # Validation Thresholds
        self.GRAND_TOTAL_DIFF_THRESHOLD = etl_config.get('grand_total_diff_threshold', 0.01)

        # Anomaly Detection Settings
        anomaly = etl_config.get('anomaly_detection', {})
        self.ANOMALY_IQR_MULTIPLIER = anomaly.get('iqr_multiplier', 1.5)
        self.ANOMALY_MIN_HISTORY = anomaly.get('min_history', 3)
        self.ANOMALY_ROLLING_WINDOW = anomaly.get('rolling_window', 6)
        self.ENABLE_HISTORICAL_HIGHLIGHT = anomaly.get('enable_historical_highlight', True)

        # Anomaly Levels
        self.ANOMALY_LEVELS = {
            "product": {"group_by": ["BUSINESS_GROUP", "SERVICE_GROUP", "PRODUCT_KEY", "PRODUCT_NAME"]},
            "service": {"group_by": ["BUSINESS_GROUP", "SERVICE_GROUP"]},
            "business": {"group_by": ["BUSINESS_GROUP"]},
            "grand_total": {"group_by": []}
        }

    def get_paths(self):
        """
        Dummy method for backward compatibility
        ‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÄ‡∏û‡∏£‡∏≤‡∏∞ paths ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏™‡πà‡∏á‡∏°‡∏≤‡∏à‡∏≤‡∏Å ConfigManager ‡πÅ‡∏•‡πâ‡∏ß
        """
        raise NotImplementedError("get_paths() should not be called when using ConfigAdapter")


# ============================================================================
# LEGACY CONFIGURATION - ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö backward compatibility ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ‡πÅ‡∏•‡πâ‡∏ß)
# ============================================================================
class Config:
    """
    Configuration ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Revenue ETL Pipeline
    ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ñ‡πà‡∏≤ config ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà
    """
    
    # ============ ‡∏õ‡∏µ ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• ============
    YEAR = "2025"

    # ============ Reconciliation Settings (‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏´‡∏°‡πà) ============
    RECONCILE_FI_MONTH = "10"       # ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå FI
    RECONCILE_TOLERANCE = 0.00      # ‡∏¢‡∏≠‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ
    ENABLE_RECONCILIATION = True    # True = ‡πÄ‡∏õ‡∏¥‡∏î, False = ‡∏õ‡∏¥‡∏î
    
    # ============ ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå Master ============
    MASTER_PRODUCT_FILE = f"MASTER_PRODUCT_NT_{YEAR}.csv"
    MASTER_GL_FILE = "source/MASTER_REVENUE_GL_CODE_NT1_NT_20250723.csv"
    MAPPING_CC_FILE = "MAPPING_CC.csv"
    MAPPING_PRODUCT_FILE = "clean/MAP_PRODUCT_NT_NEW_2024.csv"
    
    # ============ Pattern ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏ï‡πâ‡∏ô‡∏ó‡∏≤‡∏á ============
    # 1. ‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏•‡∏±‡∏Å NT1
    INPUT_FILE_PATTERNS = [
        "TRN_REVENUE_NT1_*.csv",
        "TRN_REVENUE_ADJ_GL_NT1_*.csv"
    ]
    
    # 2. ‡πÑ‡∏ü‡∏•‡πå ADJ (‡∏ú‡∏•‡∏ï‡∏≠‡∏ö‡πÅ‡∏ó‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô, ‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏≠‡∏∑‡πà‡∏ô)
    ADJ_MONTHLY_PATTERN = "TRN_REVENUE_ADJ_*.csv"
    ADJ_YTD_PATTERN = "TRN_REVENUE_ADJ_YTD_*.csv"

    # ============ ‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô ============
    EXCLUDE_BUSINESS_GROUP = "‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏≠‡∏∑‡πà‡∏ô" # ‡∏à‡∏≤‡∏Å Master Product ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏Å‡∏£‡∏≠‡∏á‡∏≠‡∏≠‡∏Å
    NON_TELECOM_SERVICE_GROUP = "‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏≠‡∏∑‡πà‡∏ô‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÇ‡∏ó‡∏£‡∏Ñ‡∏°‡∏ô‡∏≤‡∏Ñ‡∏°"
    
    NEW_ADJ_BUSINESS_GROUP = "‡∏ú‡∏•‡∏ï‡∏≠‡∏ö‡πÅ‡∏ó‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏≠‡∏∑‡πà‡∏ô"
    FINANCIAL_INCOME_NAME = "‡∏ú‡∏•‡∏ï‡∏≠‡∏ö‡πÅ‡∏ó‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô"
    OTHER_REVENUE_ADJ_NAME = "‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏≠‡∏∑‡πà‡∏ô" # ‡∏à‡∏≤‡∏Å ADJ File
    
    
    # ============ ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå Output ============
    OUTPUT_CONCAT_FILE = f"trn_revenue_nt_{YEAR}.csv"
    OUTPUT_MAPPED_CC_FILE = f"revenue_new_cc_{YEAR}.csv"
    OUTPUT_MAPPED_PRODUCT_FILE = f"revenue_mapped_product_{YEAR}_.csv"
    OUTPUT_FINAL_REPORT_FILE = f"REVENUE_NT_REPORT_{YEAR}.csv"
    
    # ============ ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå Error Log ============
    ERROR_GL_FILE = f"error_gl_REVENUE_NT_REPORT_{YEAR}.csv"
    ERROR_PRODUCT_FILE = f"error_product_REVENUE_NT_REPORT_{YEAR}.csv"
    
    # ============ Path Configuration ============
    @staticmethod
    def get_paths():
        """
        ‡∏Å‡∏≥‡∏´‡∏ô‡∏î path ‡∏ï‡∏≤‡∏°‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£
        Returns: dict ‡∏Ç‡∏≠‡∏á paths ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        """
        os_platform = platform.system()
        
        if os_platform == "Darwin":  # macOS
            base_path = "/Users/seal/Library/CloudStorage/OneDrive-Personal/share/Datasource"
            master_path = "/Users/seal/Library/CloudStorage/OneDrive-Personal/share/master"
        elif os_platform == "Linux":
            base_path = "/home/seal/nt/data/2025"
            master_path = "/home/seal/nt/master"
        elif os_platform == "Windows":
            base_path = r"C:\Users\00320845\OneDrive\share\Datasource"
            master_path = r"C:\Users\00320845\OneDrive\share\master"
        else:
            raise ValueError(f"‡πÑ‡∏°‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£: {os_platform}")
        
        return {
            "base": base_path,
            "master": master_path,
            "input": os.path.join(base_path, Config.YEAR, "revenue"),
            "output": os.path.join(base_path, Config.YEAR, "revenue", "output"),
            "final_output": os.path.join(base_path, "all", "revenue", Config.YEAR)
        }
    
    # ============ Column Names ============
    REQUIRED_COLUMNS = [
        "YEAR", "MONTH", "CUSTOMER_GROUP_KEY", "PRODUCT_KEY",
        "SUB_PRODUCT_KEY", "GL_CODE", "COST_CENTER", "REVENUE_VALUE"
    ]
    
    # ============ Special Mapping Rules ============
    SPECIAL_MAPPINGS = [
        {
            "name": "GSaaS to Other Revenue",
            "condition": {
                "PRODUCT_KEY": "102010407",
                "GL_CODE": "46400101"
            },
            "mapping": {
                "PRODUCT_KEY": "292020407",
                "SUB_PRODUCT_KEY": "1"
            }
        }
    ]
    
    # ============ Validation Thresholds ============
    GRAND_TOTAL_DIFF_THRESHOLD = 0.01  # ‡∏ö‡∏≤‡∏ó - ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö rounding error

    # ============ Anomaly Detection Settings ============
    ANOMALY_IQR_MULTIPLIER = 1.5  # ‡∏Ñ‡πà‡∏≤ k ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Tukey's Fences
    ANOMALY_MIN_HISTORY = 3  # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö

    # [‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ] ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Rolling Average
    ANOMALY_ROLLING_WINDOW = 6

    # True = ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏£‡∏∞‡∏ö‡∏≤‡∏¢‡∏™‡∏µ‡∏ó‡∏∏‡∏Å‡πÄ‡∏î‡∏∑‡∏≠‡∏ô, False = ‡∏ó‡∏≥‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏° (‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Report ‡πÅ‡∏¢‡∏Å)
    ENABLE_HISTORICAL_HIGHLIGHT = True
    
    # ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö
    ANOMALY_LEVELS = {
        "product": {"group_by": ["BUSINESS_GROUP", "SERVICE_GROUP", "PRODUCT_KEY", "PRODUCT_NAME"]},
        "service": {"group_by": ["BUSINESS_GROUP", "SERVICE_GROUP"]},
        "business": {"group_by": ["BUSINESS_GROUP"]},
        "grand_total": {"group_by": []}
    }


# ============================================================================
# ETL Pipeline Class
# ============================================================================
class RevenueETL:
    """
    ETL Pipeline ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ
    ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô:
    1. ‡∏£‡∏ß‡∏°‡πÑ‡∏ü‡∏•‡πå CSV ‡∏ï‡πâ‡∏ô‡∏ó‡∏≤‡∏á (TRN_REVENUE_NT1_*.csv ‡πÅ‡∏•‡∏∞ TRN_REVENUE_ADJ_GL_NT1_*.csv)
    2. Mapping Cost Center
    3. Mapping Product & Sub Product
    4. Merge ‡∏Å‡∏±‡∏ö Master Files, ‡∏Å‡∏£‡∏≠‡∏á, ‡∏£‡∏ß‡∏° ADJ Data, ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
    """
    
    def __init__(self, config=None, paths=None):
        """
        Args:
            config: Configuration dictionary (from ConfigManager) ‡∏´‡∏£‡∏∑‡∏≠ Config class (legacy) ‡∏´‡∏£‡∏∑‡∏≠ None
            paths: Dictionary ‡∏Ç‡∏≠‡∏á paths (‡∏à‡∏≤‡∏Å ConfigManager) ‡∏´‡∏£‡∏∑‡∏≠ None
        """
        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö V2: ‡∏£‡∏±‡∏ö config dict ‡πÅ‡∏•‡∏∞ paths dict ‡∏à‡∏≤‡∏Å ConfigManager
        if isinstance(config, dict):
            self.config_dict = config  # ‡πÄ‡∏Å‡πá‡∏ö dict ‡πÑ‡∏ß‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠
            self.config = ConfigAdapter(config)  # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô object-like
            self.paths = paths if paths else config.get('paths', {})
        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö V1 (legacy): ‡∏£‡∏±‡∏ö Config class
        else:
            self.config_dict = None
            self.config = config or Config
            self.paths = self.config.get_paths()

        # Setup logger
        self.logger = ETLLogger.get_logger('etl_module')

        self.setup_directories()

        # ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö YTD data ‡πÑ‡∏ß‡πâ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô report
        self.df_adj_ytd = pd.DataFrame() 
        
    def setup_directories(self):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á directory ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ"""
        Path(self.paths["output"]).mkdir(parents=True, exist_ok=True)
        Path(self.paths["final_output"]).mkdir(parents=True, exist_ok=True)

        self.logger.info("=" * 80)
        self.logger.info("PATH CONFIGURATION")
        self.logger.info("=" * 80)
        for key, path in self.paths.items():
            self.logger.info(f"{key:15s}: {path}")
        self.logger.info("=" * 80)
        
    def log(self, message, grand_total=None):
        """‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° log ‡∏û‡∏£‡πâ‡∏≠‡∏° timestamp"""
        # [FIX] ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡∏≠‡∏á grand_total
        # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô string, ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô level (‡πÄ‡∏ä‡πà‡∏ô "ERROR", "SUCCESS")
        # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç (int/float), ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏¢‡∏≠‡∏î‡∏£‡∏ß‡∏°

        if isinstance(grand_total, str):
            # grand_total ‡πÄ‡∏õ‡πá‡∏ô level
            self.logger.log(message, grand_total)
        elif isinstance(grand_total, (int, float)):
            # grand_total ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
            self.logger.info(message)
            self.logger.info(f"Grand Total = {grand_total:,.2f}")
        else:
            # ‡∏Å‡∏£‡∏ì‡∏µ grand_total=None ‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡∏∑‡πà‡∏ô‡πÜ
            self.logger.info(message)

    def step0_reconcile_revenue(self):
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Reconciliation)"""
        self.log("=" * 80)
        self.log("STEP 0: RECONCILIATION - ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")
        self.log("=" * 80)
        
        if not self.config.ENABLE_RECONCILIATION:
            self.log("‚ö†Ô∏è  Reconciliation ‡∏ñ‡∏π‡∏Å‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")
            return None

        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö V2: ‡∏™‡πà‡∏á config_dict (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ), ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö V1: ‡∏™‡πà‡∏á config object
        config_to_pass = self.config_dict if self.config_dict is not None else self.config
        reconciler = RevenueReconciliation(config_to_pass, self.paths)
        
        fi_file = os.path.join(
            self.paths['base'], 
            self.config.YEAR, 
            'fi', 
            'output', 
            f'pl_revenue_nt_output_{self.config.YEAR}{self.config.RECONCILE_FI_MONTH}.csv'
        )
        
        trn_file = os.path.join(
            self.paths['output'], 
            self.config.OUTPUT_CONCAT_FILE
        )
        
        self.log(f"‡πÑ‡∏ü‡∏•‡πå FI: {os.path.basename(fi_file)}")
        self.log(f"‡πÑ‡∏ü‡∏•‡πå TRN: {os.path.basename(trn_file)}")
        
        try:
            result = reconciler.reconcile_revenue(
                fi_file_path=fi_file,
                trn_file_path=trn_file,
                tolerance=self.config.RECONCILE_TOLERANCE
            )
            
            self.log("=" * 80)
            self.log("‚úì Reconciliation ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!", "SUCCESS")
            self.log("=" * 80)
            return result
            
        except ReconciliationError as e:
            self.log("=" * 80)
            self.log("‚ùå Reconciliation ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß - ‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô", "ERROR")
            self.log("=" * 80)
            # Re-raise ‡πÅ‡∏ï‡πà‡πÉ‡∏ä‡πâ raise from None ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î traceback
            raise ReconciliationError(str(e), e.reconcile_results) from None
        
        except FileNotFoundError as e:
            self.log(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå: {e}", "ERROR")
            self.log("üí° ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö:")
            self.log(f"   1. ‡πÑ‡∏ü‡∏•‡πå FI ‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà: {fi_file}")
            self.log(f"   2. RECONCILE_FI_MONTH = '{self.config.RECONCILE_FI_MONTH}' ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà")
    
    def step1_concat_revenue_files(self):
        """
        ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1: ‡∏£‡∏ß‡∏°‡πÑ‡∏ü‡∏•‡πå CSV ‡∏ï‡πâ‡∏ô‡∏ó‡∏≤‡∏á (NT1)
        - TRN_REVENUE_NT1_*.csv ‡πÅ‡∏•‡∏∞ TRN_REVENUE_ADJ_GL_NT1_*.csv
        - ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ GL_CODE ‡πÅ‡∏•‡∏∞ GL_CODE_NT1 ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô GL_CODE
        """
        self.log("=" * 80)
        self.log("STEP 1: ‡∏£‡∏ß‡∏°‡πÑ‡∏ü‡∏•‡πå CSV ‡∏ï‡πâ‡∏ô‡∏ó‡∏≤‡∏á (NT1)")
        self.log("=" * 80)
        
        # ‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
        all_files = []
        for p in self.config.INPUT_FILE_PATTERNS:
            pattern = os.path.join(self.paths["input"], p)
            self.log(f"‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ pattern: {pattern}")
            all_files.extend(glob.glob(pattern))
        
        # ‡πÉ‡∏ä‡πâ set ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ) ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö
        files = sorted(list(set(all_files)))
        
        if not files:
            patterns = [os.path.join(self.paths["input"], p) for p in self.config.INPUT_FILE_PATTERNS]
            raise FileNotFoundError(f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö patterns: {patterns}")
        
        self.log(f"‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(files)} ‡πÑ‡∏ü‡∏•‡πå")
        
        df_combined = pd.DataFrame()
        
        for file in files:
            self.log(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå: {os.path.basename(file)}")
            
            # ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå
            try:
                df = pd.read_csv(
                    file,
                    converters={
                        "YEAR": str,
                        "MONTH": int,
                        "COST_CENTER": str,
                        "CUSTOMER_GROUP_KEY": str,
                        "PRODUCT_KEY": str,
                        "SUB_PRODUCT_KEY": int
                    }
                )
            except Exception as e:
                self.log(f"  ‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå {file}: {e} - ‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ")
                continue
            
            # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
            df.columns = df.columns.str.strip()
            
            # ‡πÅ‡∏™‡∏î‡∏á GL columns ‡∏ó‡∏µ‡πà‡∏û‡∏ö
            gl_columns = df.filter(like='GL').columns.tolist()
            self.log(f"  ‡∏û‡∏ö GL columns: {gl_columns}")
            
            # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ REVENUE_VALUE
            df = df.dropna(subset=["REVENUE_VALUE"])
            df["REVENUE_VALUE"] = df["REVENUE_VALUE"].astype(str)
            df["REVENUE_VALUE"] = df["REVENUE_VALUE"].str.replace(",", "", regex=False)
            df["REVENUE_VALUE"] = df["REVENUE_VALUE"].str.replace(r"\(", "-", regex=True)
            df["REVENUE_VALUE"] = df["REVENUE_VALUE"].str.replace(r"\)", "", regex=True)
            df["REVENUE_VALUE"] = df["REVENUE_VALUE"].str.replace(" ", "", regex=False)
            df["REVENUE_VALUE"] = pd.to_numeric(df["REVENUE_VALUE"], errors='coerce')
            
            # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ GL_CODE ‡πÅ‡∏•‡∏∞ GL_CODE_NT1
            if "GL_CODE_NT1" in df.columns:
                self.log(f"  ‡πÉ‡∏ä‡πâ GL_CODE_NT1 ‡πÄ‡∏õ‡πá‡∏ô GL_CODE")
                df["GL_CODE"] = df["GL_CODE_NT1"].astype(str)
            elif "GL_CODE" in df.columns:
                self.log(f"  ‡πÉ‡∏ä‡πâ GL_CODE")
                df["GL_CODE"] = df["GL_CODE"].astype(str)
            else:
                if len(gl_columns) > 0:
                    self.log(f"  ‡πÉ‡∏ä‡πâ {gl_columns[0]} ‡πÄ‡∏õ‡πá‡∏ô GL_CODE")
                    df["GL_CODE"] = df[gl_columns[0]].astype(str)
                else:
                    self.log(f"  ‚ö†Ô∏è  ‡πÑ‡∏°‡πà‡∏û‡∏ö column GL_CODE ‡∏´‡∏£‡∏∑‡∏≠ GL_CODE_NT1 ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå {file} - ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÄ‡∏õ‡∏•‡πà‡∏≤")
                    df["GL_CODE"] = np.nan # ‡∏´‡∏£‡∏∑‡∏≠ "NA"
            
            # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
            try:
                df = df[self.config.REQUIRED_COLUMNS]
            except KeyError as e:
                self.log(f"  ‚ùå ‡∏Ç‡∏≤‡∏î‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡πÉ‡∏ô {file}: {e} - ‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ")
                continue

            # ‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà PRODUCT_KEY ‡πÄ‡∏õ‡πá‡∏ô null
            df = df.dropna(subset=["PRODUCT_KEY"])
            
            file_total = df["REVENUE_VALUE"].sum()
            self.log(f"  ‡∏¢‡∏≠‡∏î‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ: {file_total:,.2f}")
            self.log(f"  ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß: {len(df):,}")
            
            df_combined = pd.concat([df_combined, df], ignore_index=True)
        
        # ‡πÅ‡∏õ‡∏•‡∏á MONTH ‡πÄ‡∏õ‡πá‡∏ô string ‡πÅ‡∏ö‡∏ö 2 ‡∏´‡∏•‡∏±‡∏Å
        df_combined["MONTH"] = df_combined["MONTH"].astype(int).astype(str).str.zfill(2)
        df_combined["SUB_PRODUCT_KEY"] = df_combined["SUB_PRODUCT_KEY"].astype(int).astype(str)

        # === [NEW] Month Filtering: ‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏°‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î ===
        # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ end_month ‡πÉ‡∏ô config ‡πÉ‡∏´‡πâ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
        if hasattr(self.config, 'end_month') and self.config.end_month:
            end_month = self.config.end_month
            year = self.config.YEAR

            # ‡πÅ‡∏õ‡∏•‡∏á MONTH ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô int ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö
            df_combined_temp = df_combined.copy()
            df_combined_temp['MONTH_INT'] = df_combined_temp['MONTH'].astype(int)

            # Filter: ‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏õ‡∏µ‡πÅ‡∏•‡∏∞‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
            before_filter_count = len(df_combined_temp)
            before_filter_total = df_combined_temp["REVENUE_VALUE"].sum()

            df_combined = df_combined_temp[
                (df_combined_temp['YEAR'] == year) &
                (df_combined_temp['MONTH_INT'] <= end_month)
            ].copy()

            # ‡∏•‡∏ö MONTH_INT ‡∏≠‡∏≠‡∏Å (‡πÉ‡∏ä‡πâ‡πÅ‡∏Ñ‡πà‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß)
            df_combined = df_combined.drop(columns=['MONTH_INT'])

            after_filter_count = len(df_combined)
            after_filter_total = df_combined["REVENUE_VALUE"].sum()

            self.log("=" * 80)
            self.log(f"üìÖ MONTH FILTERING: ‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏∑‡∏≠‡∏ô 1 - {end_month:02d}/{year}")
            self.log(f"  ‡∏Å‡πà‡∏≠‡∏ô‡∏Å‡∏£‡∏≠‡∏á: {before_filter_count:,} ‡πÅ‡∏ñ‡∏ß, ‡∏¢‡∏≠‡∏î‡∏£‡∏ß‡∏°: {before_filter_total:,.2f}")
            self.log(f"  ‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏£‡∏≠‡∏á: {after_filter_count:,} ‡πÅ‡∏ñ‡∏ß, ‡∏¢‡∏≠‡∏î‡∏£‡∏ß‡∏°: {after_filter_total:,.2f}")
            self.log(f"  ‡∏•‡∏î‡∏•‡∏á: {before_filter_count - after_filter_count:,} ‡πÅ‡∏ñ‡∏ß")
            self.log("=" * 80)
        else:
            self.log("‚ÑπÔ∏è  Month Filtering: ‡πÑ‡∏°‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î - ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ó‡∏∏‡∏Å‡πÄ‡∏î‡∏∑‡∏≠‡∏ô")
        # === [END] Month Filtering ===

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö GL_CODE ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ
        unique_gl = df_combined["GL_CODE"].nunique()
        self.log(f"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô GL_CODE ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô: {unique_gl}")
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå
        output_file = os.path.join(self.paths["output"], self.config.OUTPUT_CONCAT_FILE)
        df_combined.to_csv(output_file, index=False, float_format="%.2f")
        
        self.log(f"‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå: {output_file}")
        self.log(f"‡∏£‡∏ß‡∏°‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô - ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(df_combined):,}")
        self.log(f"‡∏£‡∏ß‡∏°‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô", df_combined["REVENUE_VALUE"].sum())
        
        return df_combined
    
    def step2_mapping_cost_center(self, df):
        """‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2: Mapping Cost Center"""
        self.log("=" * 80)
        self.log("STEP 2: Mapping Cost Center")
        self.log("=" * 80)
        
        # ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå mapping
        mapping_file = os.path.join(self.paths["master"], self.config.MAPPING_CC_FILE)
        
        if not os.path.exists(mapping_file):
            self.log(f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå {mapping_file} - ‡∏Ç‡πâ‡∏≤‡∏°‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ")
            return df
        
        df_mapping = pd.read_csv(mapping_file)
        map_dict = dict(df_mapping.values)
        
        self.log(f"‡∏û‡∏ö‡∏Å‡∏≤‡∏£ mapping {len(map_dict)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
        
        # Mapping cost center
        original_cc = df["COST_CENTER"].copy()
        df["COST_CENTER"] = df["COST_CENTER"].map(map_dict).fillna(df["COST_CENTER"])
        
        # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å map
        mapped_count = (original_cc != df["COST_CENTER"]).sum()
        self.log(f"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Cost Center ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å mapping: {mapped_count:,}")
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå
        output_file = os.path.join(self.paths["output"], self.config.OUTPUT_MAPPED_CC_FILE)
        df.to_csv(output_file, index=False, float_format="%.2f")
        
        self.log(f"‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå: {output_file}")
        self.log(f"Mapping Cost Center ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô", df["REVENUE_VALUE"].sum())
        
        return df
    
    def step3_mapping_product(self, df):
        """‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 3: Mapping Product & Sub Product"""
        self.log("=" * 80)
        self.log("STEP 3: Mapping Product & Sub Product")
        self.log("=" * 80)
        
        # ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå mapping product
        mapping_file = os.path.join(self.paths["master"], self.config.MAPPING_PRODUCT_FILE)
        
        if not os.path.exists(mapping_file):
            self.log(f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå {mapping_file} - ‡∏Ç‡πâ‡∏≤‡∏°‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ")
            return df
        
        df_mapping = pd.read_csv(
            mapping_file,
            converters={
                "PRODUCT_KEY_OLD": str,
                "SUB_PRODUCT_KEY_OLD": str,
                "PRODUCT_KEY": str,
                "SUB_PRODUCT_KEY": str,
                "GL_CODE": str
            }
        )
        
        # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
        df_mapping = df_mapping[["PRODUCT_KEY_OLD", "SUB_PRODUCT_KEY_OLD", 
                                  "PRODUCT_KEY", "SUB_PRODUCT_KEY"]]
        
        # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô int ‡πÅ‡∏•‡πâ‡∏ß‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô str (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏±‡∏î 0 ‡∏Ç‡πâ‡∏≤‡∏á‡∏´‡∏ô‡πâ‡∏≤)
        df_mapping["SUB_PRODUCT_KEY_OLD"] = df_mapping["SUB_PRODUCT_KEY_OLD"].astype(int).astype(str)
        df_mapping["SUB_PRODUCT_KEY"] = df_mapping["SUB_PRODUCT_KEY"].astype(int).astype(str)
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á composite key
        df["product_key_sub_product"] = df["PRODUCT_KEY"] + df["SUB_PRODUCT_KEY"]
        df_mapping["product_key_sub_product"] = (df_mapping["PRODUCT_KEY_OLD"] + 
                                                  df_mapping["SUB_PRODUCT_KEY_OLD"])
        
        self.log(f"‡∏û‡∏ö‡∏Å‡∏≤‡∏£ mapping product {len(df_mapping)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
        
        # Mapping PRODUCT_KEY
        map_product = dict(zip(df_mapping["product_key_sub_product"], 
                              df_mapping["PRODUCT_KEY"]))
        original_product = df["PRODUCT_KEY"].copy()
        df["PRODUCT_KEY"] = df["product_key_sub_product"].map(map_product).fillna(df["PRODUCT_KEY"])
        
        # Mapping SUB_PRODUCT_KEY
        map_sub_product = dict(zip(df_mapping["product_key_sub_product"], 
                                   df_mapping["SUB_PRODUCT_KEY"]))
        df["SUB_PRODUCT_KEY"] = df["product_key_sub_product"].map(map_sub_product).fillna(df["SUB_PRODUCT_KEY"])
        
        # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å map
        mapped_count = (original_product != df["PRODUCT_KEY"]).sum()
        self.log(f"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Product ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å mapping: {mapped_count:,}")
        
        # ‡∏•‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß
        df = df.drop(columns=["product_key_sub_product"])
        
        # Apply special mapping rules
        self.log("‡∏Å‡∏≥‡∏•‡∏±‡∏á apply special mapping rules...")
        for rule in self.config.SPECIAL_MAPPINGS:
            self.log(f"  - {rule['name']}")

            # ‡∏™‡∏£‡πâ‡∏≤‡∏á condition
            condition = pd.Series([True] * len(df))
            for cond_col, cond_val in rule['condition'].items():
                condition &= (df[cond_col] == cond_val)

            special_count = condition.sum()
            if special_count > 0:
                # Apply mapping - ‡πÅ‡∏õ‡∏•‡∏á Series ‡πÄ‡∏õ‡πá‡∏ô numpy array ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ "unhashable type: 'Series'"
                condition_mask = condition.values
                for map_col, map_val in rule['mapping'].items():
                    df.loc[condition_mask, map_col] = map_val
                self.log(f"    Applied to {special_count:,} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå
        output_file = os.path.join(self.paths["output"], self.config.OUTPUT_MAPPED_PRODUCT_FILE)
        df.to_csv(output_file, index=False, float_format="%.2f")
        
        self.log(f"‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå: {output_file}")
        self.log(f"Mapping Product ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô", df["REVENUE_VALUE"].sum())
        
        return df

    def _load_adj_data(self):
        """
        [‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà] ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå ADJ ‡∏£‡∏≤‡∏¢‡πÄ‡∏î‡∏∑‡∏≠‡∏ô ‡πÅ‡∏•‡∏∞ ADJ YTD
        """
        self.log("--- Loading ADJ Data (Financial Income / Other Revenue) ---")
        
        # --- Load Monthly ADJ Files ---
        monthly_pattern = os.path.join(self.paths["input"], self.config.ADJ_MONTHLY_PATTERN)
        
        # [ START FIX ] ---
        # ‡∏Å‡∏£‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå YTD ‡πÅ‡∏•‡∏∞ ADJ_GL_NT1 ‡∏≠‡∏≠‡∏Å
        self.log(f"‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ pattern: {monthly_pattern}")
        monthly_files_all = glob.glob(monthly_pattern)
        monthly_files = []
        for f in monthly_files_all:
            filename_upper = os.path.basename(f).upper()
            if "YTD" not in filename_upper and "ADJ_GL_NT1" not in filename_upper:
                monthly_files.append(f)
            else:
                self.log(f"  ‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏ü‡∏•‡πå: {os.path.basename(f)} (‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç ADJ ‡∏£‡∏≤‡∏¢‡πÄ‡∏î‡∏∑‡∏≠‡∏ô)")
        # [ END FIX ] ---
        
        df_adj_monthly = pd.DataFrame()
        if not monthly_files:
            self.log("‚ö†Ô∏è  ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå ADJ ‡∏£‡∏≤‡∏¢‡πÄ‡∏î‡∏∑‡∏≠‡∏ô (TRN_REVENUE_ADJ_*.csv)")
        else:
            self.log(f"‡∏û‡∏ö {len(monthly_files)} ‡πÑ‡∏ü‡∏•‡πå ADJ ‡∏£‡∏≤‡∏¢‡πÄ‡∏î‡∏∑‡∏≠‡∏ô")
            for file in monthly_files:
                self.log(f"  ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡πà‡∏≤‡∏ô ADJ ‡∏£‡∏≤‡∏¢‡πÄ‡∏î‡∏∑‡∏≠‡∏ô: {os.path.basename(file)}")
                try:
                    df = pd.read_csv(file, encoding='tis-620')
                except (UnicodeDecodeError, LookupError):
                    self.log(f"    tis-620 ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß, ‡∏•‡∏≠‡∏á cp874...")
                    try:
                        df = pd.read_csv(file, encoding='cp874')
                    except Exception as e:
                        self.log(f"    ‚ùå ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå {file} ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}")
                        continue
                except Exception as e:
                    self.log(f"    ‚ùå ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå {file} ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}")
                    continue
                
                df_adj_monthly = pd.concat([df_adj_monthly, df], ignore_index=True)
            
            if not df_adj_monthly.empty:
                # Clean monthly data
                df_adj_monthly = df_adj_monthly.dropna(subset=['REVENUE_VALUE'])
                df_adj_monthly["REVENUE_VALUE"] = pd.to_numeric(
                    df_adj_monthly["REVENUE_VALUE"], errors='coerce'
                ).fillna(0)

                # Filter ADJ data ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏ô config
                if hasattr(self.config, 'end_month') and self.config.end_month:
                    if 'MONTH' in df_adj_monthly.columns:
                        original_count = len(df_adj_monthly)
                        df_adj_monthly['MONTH_INT'] = df_adj_monthly['MONTH'].astype(int)
                        df_adj_monthly = df_adj_monthly[
                            df_adj_monthly['MONTH_INT'] <= self.config.end_month
                        ]
                        df_adj_monthly = df_adj_monthly.drop(columns=['MONTH_INT'])
                        filtered_count = len(df_adj_monthly)
                        self.log(f"‚úì Filter ADJ Monthly: {original_count} ‚Üí {filtered_count} records (‡πÄ‡∏î‡∏∑‡∏≠‡∏ô 1-{self.config.end_month})", "INFO")

                self.log(f"‡∏¢‡∏≠‡∏î‡∏£‡∏ß‡∏° ADJ ‡∏£‡∏≤‡∏¢‡πÄ‡∏î‡∏∑‡∏≠‡∏ô (‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏î‡πâ): {df_adj_monthly['REVENUE_VALUE'].sum():,.2f}")

        # --- Load YTD ADJ File ---
        # ‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏•‡πå YTD ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
        ytd_pattern = os.path.join(self.paths["input"], self.config.ADJ_YTD_PATTERN)
        ytd_files = glob.glob(ytd_pattern)
        
        df_adj_ytd = pd.DataFrame()
        if not ytd_files:
            self.log("‚ö†Ô∏è  ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå ADJ YTD (TRN_REVENUE_ADJ_YTD_*.csv)")
        else:
            latest_ytd_file = sorted(ytd_files)[-1] # ‡πÄ‡∏≠‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏ï‡∏≤‡∏°‡∏ä‡∏∑‡πà‡∏≠
            self.log(f"‡∏û‡∏ö {len(ytd_files)} ‡πÑ‡∏ü‡∏•‡πå YTD. ‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏•‡πå‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î: {os.path.basename(latest_ytd_file)}")
            try:
                df_adj_ytd = pd.read_csv(latest_ytd_file, encoding='tis-620')
            except (UnicodeDecodeError, LookupError):
                self.log(f"    tis-620 ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß, ‡∏•‡∏≠‡∏á cp874...")
                df_adj_ytd = pd.read_csv(latest_ytd_file, encoding='cp874')
            except Exception as e:
                 self.log(f"    ‚ùå ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå YTD {latest_ytd_file} ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}")
            
            if not df_adj_ytd.empty:
                # Clean YTD data
                df_adj_ytd = df_adj_ytd.dropna(subset=['REVENUE_VALUE'])
                df_adj_ytd["REVENUE_VALUE"] = pd.to_numeric(
                    df_adj_ytd["REVENUE_VALUE"], errors='coerce'
                ).fillna(0)
                self.log(f"‡∏¢‡∏≠‡∏î‡∏£‡∏ß‡∏° ADJ YTD (‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏î‡πâ): {df_adj_ytd['REVENUE_VALUE'].sum():,.2f}")
            
        return df_adj_monthly, df_adj_ytd

    def _process_adj_data(self, df_adj):
        """
        [‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà] ‡πÅ‡∏õ‡∏•‡∏á ADJ data ‡πÉ‡∏´‡πâ‡∏°‡∏µ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô df_output
        """
        self.log("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• ADJ data...")
        if df_adj.empty:
            self.log("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ADJ ‡πÉ‡∏´‡πâ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•")
            return pd.DataFrame()
            
        df_adj['YEAR'] = df_adj['YEAR'].astype(str)
        df_adj['MONTH'] = df_adj['MONTH'].astype(int).astype(str).str.zfill(2)
        
        # [ START FIX ]
        # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'TYPE' ‡πÅ‡∏•‡∏∞ 'REVENUE_TYPE' ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô‡∏Å‡∏±‡∏ô
        if "REVENUE_TYPE" in df_adj.columns and "TYPE" in df_adj.columns:
            self.log("  ‡∏û‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'TYPE' ‡πÅ‡∏•‡∏∞ 'REVENUE_TYPE' ... ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏£‡∏ß‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå")
            # ‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡πà‡∏≤ 'TYPE' ‡∏ó‡∏µ‡πà‡∏ß‡πà‡∏≤‡∏á (NaN) ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å 'REVENUE_TYPE'
            df_adj['TYPE'] = df_adj['TYPE'].fillna(df_adj['REVENUE_TYPE'])
            # ‡∏•‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'REVENUE_TYPE' ‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô‡∏≠‡∏≠‡∏Å
            df_adj = df_adj.drop(columns=['REVENUE_TYPE'])
            self.log("  ‡∏£‡∏ß‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå TYPE/REVENUE_TYPE ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")
        elif "REVENUE_TYPE" in df_adj.columns:
            # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡πÅ‡∏Ñ‡πà REVENUE_TYPE, ‡∏Å‡πá‡πÅ‡∏Ñ‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠
            self.log("  ‡∏û‡∏ö 'REVENUE_TYPE', ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡πá‡∏ô 'TYPE'")
            df_adj = df_adj.rename(columns={"REVENUE_TYPE": "TYPE"})
        elif "TYPE" not in df_adj.columns:
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ó‡∏±‡πâ‡∏á‡∏Ñ‡∏π‡πà
            self.log("‚ùå ‡πÑ‡∏ü‡∏•‡πå ADJ ‡∏Ç‡∏≤‡∏î‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'TYPE' ‡∏´‡∏£‡∏∑‡∏≠ 'REVENUE_TYPE'")
            return pd.DataFrame()
        # [ END FIX ]

        # ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ (‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡πÄ‡∏Å‡∏¥‡∏î Error)
        df_adj_filtered = df_adj[df_adj['TYPE'].isin([
            self.config.FINANCIAL_INCOME_NAME, 
            self.config.OTHER_REVENUE_ADJ_NAME
        ])].copy()
        
        if df_adj_filtered.empty:
            self.log("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• '‡∏ú‡∏•‡∏ï‡∏≠‡∏ö‡πÅ‡∏ó‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô' ‡∏´‡∏£‡∏∑‡∏≠ '‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏≠‡∏∑‡πà‡∏ô' ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå ADJ")
            return pd.DataFrame()

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô df_output
        df_processed = pd.DataFrame()
        df_processed['YEAR'] = df_adj_filtered['YEAR']
        df_processed['MONTH'] = df_adj_filtered['MONTH']
        df_processed['REVENUE_VALUE'] = df_adj_filtered['REVENUE_VALUE']
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå Hierarchy ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô
        df_processed['BUSINESS_GROUP'] = self.config.NEW_ADJ_BUSINESS_GROUP
        df_processed['SERVICE_GROUP'] = df_adj_filtered['TYPE'] # e.g., "‡∏ú‡∏•‡∏ï‡∏≠‡∏ö‡πÅ‡∏ó‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô"
        df_processed['PRODUCT_NAME'] = df_adj_filtered['TYPE']
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå Dummy/Placeholder ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ concat ‡∏Å‡∏±‡∏ö df_output ‡πÑ‡∏î‡πâ
        df_processed['ITEM'] = 'ADJ'
        df_processed['SUB_ITEM'] = 'ADJ'
        df_processed['PRODUCT_KEY'] = 'ADJ_' + df_adj_filtered['TYPE'].str.replace(' ', '_') # Unique key
        df_processed['SUB_PRODUCT_KEY'] = '1'
        df_processed['COST_CENTER'] = df_adj_filtered.get('COST_CENTER', 'NA')
        df_processed['CUSTOMER_GROUP_KEY'] = df_adj_filtered.get('CUSTOMER_GROUP_KEY', 'NA')
        df_processed['GL_CODE'] = 'ADJ' # Dummy GL
        df_processed['NT'] = 'NT'
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå Dummy ‡∏à‡∏≤‡∏Å Master GL
        df_processed['REPORT_CODE'] = 'ADJ'
        df_processed['GL_NAME'] = 'ADJ'
        df_processed['GL_GROUP'] = 'ADJ'
        df_processed['‡∏´‡∏°‡∏ß‡∏î‡∏ö‡∏±‡∏ç‡∏ä‡∏µ'] = 'ADJ' # ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ Error ‡πÉ‡∏ô Step 4
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå Dummy ‡∏à‡∏≤‡∏Å Master Product
        df_processed['SUB_PRODUCT_NAME'] = df_adj_filtered['TYPE']
        df_processed['REVENUE_GROUP_TYPE'] = 'ADJ'
        df_processed['BUSINESS'] = df_processed['ITEM'] + " " + df_processed['BUSINESS_GROUP']
        df_processed['SERVICE'] = df_processed['SUB_ITEM'] + " " + df_processed['SERVICE_GROUP']
        df_processed['PRODUCT'] = df_processed['PRODUCT_KEY'] + " " + df_processed['PRODUCT_NAME']
        df_processed['SUB_PRODUCT'] = df_processed['SUB_PRODUCT_KEY'] + " " + df_processed['SUB_PRODUCT_NAME']
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå Date ‡πÅ‡∏•‡∏∞ Amount
        df_processed["dt"] = "01" + df_processed["MONTH"] + df_processed["YEAR"]
        df_processed["DATE"] = pd.to_datetime(df_processed["dt"], errors="coerce", format="%d%m%Y")
        
        df_processed['TYPE'] = '‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ'
        df_processed['AMOUNT'] = df_processed['REVENUE_VALUE']
        
        self.log(f"‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• ADJ data {len(df_processed)} ‡πÅ‡∏ñ‡∏ß‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")
        return df_processed

    def step4_create_final_report(self, df):
        """‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 4: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢"""
        self.log("=" * 80)
        self.log("STEP 4: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢")
        self.log("=" * 80)
        
        grand_total_before = df["REVENUE_VALUE"].sum()
        
        # ‡∏≠‡πà‡∏≤‡∏ô Master Product
        master_product_file = os.path.join(self.paths["master"], self.config.MASTER_PRODUCT_FILE)
        if not os.path.exists(master_product_file):
            raise FileNotFoundError(f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå {master_product_file}")
        
        df_master_product = pd.read_csv(master_product_file, dtype=str)
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏£‡∏ß‡∏°
        df_master_product["BUSINESS"] = df_master_product["ITEM"] + " " + df_master_product["BUSINESS_GROUP"]
        df_master_product["SERVICE"] = df_master_product["SUB_ITEM"] + " " + df_master_product["SERVICE_GROUP"]
        df_master_product["PRODUCT"] = df_master_product["PRODUCT_KEY"] + " " + df_master_product["PRODUCT_NAME"]
        df_master_product["SUB_PRODUCT"] = df_master_product["SUB_PRODUCT_KEY"] + " " + df_master_product["SUB_PRODUCT_NAME"]
        
        df_master_product = df_master_product[[
            "PRODUCT_KEY", "PRODUCT_NAME", "SUB_PRODUCT_KEY", "SUB_PRODUCT_NAME",
            "ITEM", "BUSINESS_GROUP", "SUB_ITEM", "SERVICE_GROUP", 
            "REVENUE_GROUP_TYPE", "BUSINESS", "SERVICE", "PRODUCT", "SUB_PRODUCT"
        ]]
        df_master_product = df_master_product.drop_duplicates()
        
        self.log(f"Master Product: {len(df_master_product):,} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
        
        # ‡∏≠‡πà‡∏≤‡∏ô Master GL
        master_gl_file = os.path.join(self.paths["master"], self.config.MASTER_GL_FILE)
        if not os.path.exists(master_gl_file):
            raise FileNotFoundError(f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå {master_gl_file}")
        
        df_master_gl = pd.read_csv(master_gl_file, converters={"GL_CODE_NT1": str})
        
        # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞ columns ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏≤‡∏Å Master GL
        df_master_gl = df_master_gl[[
            "GL_CODE_NT1", "GL_NAME_NT1", "REPORT_CODE", "GL_GROUP", "‡∏´‡∏°‡∏ß‡∏î‡∏ö‡∏±‡∏ç‡∏ä‡∏µ"
        ]].copy()
        
        df_master_gl.rename(columns={"GL_NAME_NT1": "GL_NAME"}, inplace=True)
        
        self.log(f"Master GL: {len(df_master_gl):,} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÄ‡∏™‡∏£‡∏¥‡∏°
        df["NT"] = "NT"
        df["dt"] = "01" + df["MONTH"] + df["YEAR"]
        df["DATE"] = pd.to_datetime(df["dt"], errors="coerce", format="%d%m%Y")
        
        # Group by
        self.log("Group by ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (NT1)...")
        df_output = df.groupby([
            "YEAR", "MONTH", "DATE", "COST_CENTER", "CUSTOMER_GROUP_KEY",
            "PRODUCT_KEY", "SUB_PRODUCT_KEY", "GL_CODE", "NT"
        ], dropna=False)["REVENUE_VALUE"].sum().reset_index()
        df_output = df_output.round(2)
        
        self.log(f"‡∏´‡∏•‡∏±‡∏á Group by (NT1): {len(df_output):,} ‡πÅ‡∏ñ‡∏ß")
        
        # Merge ‡∏Å‡∏±‡∏ö Master GL
        self.log("Merging (NT1) ‡∏Å‡∏±‡∏ö Master GL...")
        
        gl_in_data = set(df_output["GL_CODE"].dropna().unique())
        gl_in_master = set(df_master_gl["GL_CODE_NT1"].dropna().unique())
        
        self.log(f"GL_CODE ‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (NT1): {len(gl_in_data)} ‡∏£‡∏´‡∏±‡∏™")
        self.log(f"GL_CODE_NT1 ‡πÉ‡∏ô Master: {len(gl_in_master)} ‡∏£‡∏´‡∏±‡∏™")
        
        # ‡∏´‡∏≤ GL ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô Master
        gl_not_in_master = gl_in_data - gl_in_master
        if len(gl_not_in_master) > 0:
            self.log(f"‚ö†Ô∏è  ‡∏û‡∏ö GL_CODE {len(gl_not_in_master)} ‡∏£‡∏´‡∏±‡∏™‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô Master:")
            for gl in sorted(list(gl_not_in_master))[:10]:
                count = (df_output["GL_CODE"] == gl).sum()
                amount = df_output[df_output["GL_CODE"] == gl]["REVENUE_VALUE"].sum()
                self.log(f"    - {gl}: {count:,} ‡πÅ‡∏ñ‡∏ß, ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô {amount:,.2f}")
        
        df_output = pd.merge(
            df_output,
            df_master_gl,
            left_on="GL_CODE",
            right_on="GL_CODE_NT1",
            how="left"
        )
        
        df_output = df_output.drop(columns=["GL_CODE_NT1"])
        
        self.log(f"‡∏´‡∏•‡∏±‡∏á Merge GL (NT1): {len(df_output):,} ‡πÅ‡∏ñ‡∏ß")
        
        # Group by again
        df_output = df_output.groupby([
            "YEAR", "MONTH", "DATE", "COST_CENTER", "CUSTOMER_GROUP_KEY",
            "PRODUCT_KEY", "SUB_PRODUCT_KEY", "REPORT_CODE", "GL_CODE",
            "GL_NAME", "GL_GROUP", "‡∏´‡∏°‡∏ß‡∏î‡∏ö‡∏±‡∏ç‡∏ä‡∏µ", "NT"
        ], dropna=False)["REVENUE_VALUE"].sum().reset_index()
        df_output = df_output.round(2)
        
        df_output["TYPE"] = "‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ"
        df_output["AMOUNT"] = df_output["REVENUE_VALUE"]
        
        # Merge ‡∏Å‡∏±‡∏ö Master Product
        self.log("Merging (NT1) ‡∏Å‡∏±‡∏ö Master Product...")
        df_output = pd.merge(
            df_output,
            df_master_product,
            on=["PRODUCT_KEY", "SUB_PRODUCT_KEY"],
            how="left"
        )
        
        self.log(f"‡∏´‡∏•‡∏±‡∏á Merge Product (NT1): {len(df_output):,} ‡πÅ‡∏ñ‡∏ß")

        # --- [ START MODIFICATION ] ---
        # 1. ‡∏Å‡∏£‡∏≠‡∏á "‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à ‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏≠‡∏∑‡πà‡∏ô" ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• NT1
        self.log(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Å‡∏£‡∏≠‡∏á BUSINESS_GROUP: '{self.config.EXCLUDE_BUSINESS_GROUP}' ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• NT1")
        initial_count = len(df_output)
        
        if "BUSINESS_GROUP" not in df_output.columns:
            self.log("‚ö†Ô∏è  ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå BUSINESS_GROUP, ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Å‡∏£‡∏≠‡∏á‡πÑ‡∏î‡πâ")
        else:
            df_output = df_output[df_output["BUSINESS_GROUP"] != self.config.EXCLUDE_BUSINESS_GROUP].copy()
            filtered_count = initial_count - len(df_output)
            self.log(f"  ‡∏Å‡∏£‡∏≠‡∏á‡∏≠‡∏≠‡∏Å {filtered_count:,} ‡πÅ‡∏ñ‡∏ß")
            
        grand_total_after_filter = df_output["REVENUE_VALUE"].sum()
        self.log(f"‡∏¢‡∏≠‡∏î‡∏£‡∏ß‡∏° (NT1 data) ‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏£‡∏≠‡∏á: {grand_total_after_filter:,.2f}")

        # 2. ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• ADJ data
        df_adj_monthly, df_adj_ytd = self._load_adj_data()
        
        # ‡πÄ‡∏Å‡πá‡∏ö YTD data ‡πÑ‡∏ß‡πâ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô __main__
        self.df_adj_ytd = df_adj_ytd 
        
        # 3. ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• ADJ data
        df_adj_processed = self._process_adj_data(df_adj_monthly)
        
        # 4. ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• NT1 ‡∏ó‡∏µ‡πà‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß ‡∏Å‡∏±‡∏ö ADJ data
        if df_adj_processed is not None and not df_adj_processed.empty:
            self.log("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• NT1 ‡πÅ‡∏•‡∏∞ ADJ...")
            df_output = pd.concat([df_output, df_adj_processed], ignore_index=True)
            self.log(f"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏´‡∏•‡∏±‡∏á‡∏£‡∏ß‡∏°: {len(df_output):,}")
        else:
            self.log("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ADJ ‡∏£‡∏≤‡∏¢‡πÄ‡∏î‡∏∑‡∏≠‡∏ô, ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• NT1 ‡∏ï‡πà‡∏≠‡πÑ‡∏õ")
        
        # --- [ END MODIFICATION ] ---

        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á (‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏° ADJ ‡πÅ‡∏•‡πâ‡∏ß)
        self.log("‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (NT1 + ADJ)...")
        
        # '‡∏´‡∏°‡∏ß‡∏î‡∏ö‡∏±‡∏ç‡∏ä‡∏µ' ‡∏Ç‡∏≠‡∏á ADJ ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô 'ADJ' ‡∏ã‡∏∂‡πà‡∏á‡πÑ‡∏°‡πà .isnull()
        missing_gl = df_output["‡∏´‡∏°‡∏ß‡∏î‡∏ö‡∏±‡∏ç‡∏ä‡∏µ"].isnull() 
        # 'BUSINESS_GROUP' ‡∏Ç‡∏≠‡∏á ADJ ‡∏à‡∏∞‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ (NEW_ADJ_BUSINESS_GROUP) ‡∏ã‡∏∂‡πà‡∏á‡πÑ‡∏°‡πà .isnull()
        missing_bu = df_output["BUSINESS_GROUP"].isnull()
        
        has_error = False
        
        if missing_gl.any():
            missing_count = missing_gl.sum()
            missing_amount = df_output[missing_gl]["REVENUE_VALUE"].sum()
            self.log(f"‚ö†Ô∏è  ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• GL (NT1) ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô Master: {missing_count:,} ‡πÅ‡∏ñ‡∏ß (‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô: {missing_amount:,.2f})")
            
            problem_gl = df_output[missing_gl][["GL_CODE", "REVENUE_VALUE"]].groupby("GL_CODE").agg({
                "REVENUE_VALUE": "sum"
            }).reset_index().sort_values("REVENUE_VALUE", ascending=False)
            
            self.log(f"GL_CODE (NT1) ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ (Top 10):")
            for idx, row in problem_gl.head(10).iterrows():
                self.log(f"  - {row['GL_CODE']}: {row['REVENUE_VALUE']:,.2f}")
            
            error_file = os.path.join(self.paths["final_output"], self.config.ERROR_GL_FILE)
            df_output[missing_gl].to_csv(error_file, index=False)
            self.log(f"‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å error log: {error_file}")
            has_error = True
        
        if missing_bu.any():
            missing_count = missing_bu.sum()
            missing_amount = df_output[missing_bu]["REVENUE_VALUE"].sum()
            self.log(f"‚ö†Ô∏è  ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Product (NT1) ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô Master: {missing_count:,} ‡πÅ‡∏ñ‡∏ß (‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô: {missing_amount:,.2f})")
            
            error_data = df_output.loc[missing_bu, [
                "MONTH", "PRODUCT_KEY", "SUB_PRODUCT_KEY", "REVENUE_VALUE"
            ]].drop_duplicates()
            
            self.log(f"Product (NT1) ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ (Top 10):")
            for idx, row in error_data.head(10).iterrows():
                self.log(f"  - {row['PRODUCT_KEY']}-{row['SUB_PRODUCT_KEY']}: {row['REVENUE_VALUE']:,.2f}")
            
            error_file = os.path.join(self.paths["final_output"], self.config.ERROR_PRODUCT_FILE)
            error_data.to_csv(error_file, index=False)
            self.log(f"‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å error log: {error_file}")
            has_error = True
        
        if not has_error:
            self.log("‚úì ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• GL ‡πÅ‡∏•‡∏∞ Product (NT1) ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô (ADJ ‡∏ñ‡∏π‡∏Å‡∏Ç‡πâ‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ô‡∏µ‡πâ)")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏¢‡∏≠‡∏î‡∏£‡∏ß‡∏°
        grand_total_after = df_output["REVENUE_VALUE"].sum()
        # grand_total_before ‡∏Ñ‡∏∑‡∏≠‡∏¢‡∏≠‡∏î NT1 (‡∏Å‡πà‡∏≠‡∏ô‡∏Å‡∏£‡∏≠‡∏á)
        # grand_total_after ‡∏Ñ‡∏∑‡∏≠‡∏¢‡∏≠‡∏î NT1 (‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏£‡∏≠‡∏á) + ADJ
        # ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ô‡∏µ‡πâ‡∏≠‡∏≤‡∏à‡πÑ‡∏°‡πà‡∏™‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡πÅ‡∏•‡πâ‡∏ß ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡πÑ‡∏ß‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏¢‡∏≠‡∏î‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
        
        self.log(f"Grand Total Before (NT1 Original): {grand_total_before:,.2f}")
        self.log(f"Grand Total After (NT1 Filtered + ADJ):  {grand_total_after:,.2f}")
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏° ADJ ‡πÅ‡∏•‡πâ‡∏ß)
        output_file = os.path.join(self.paths["final_output"], self.config.OUTPUT_FINAL_REPORT_FILE)
        df_output.to_csv(output_file, index=False)
        
        self.log(f"‚úì ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö): {output_file}")
        self.log(f"‚úì ETL Pipeline ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!", grand_total_after)
        
        return df_output
    
    def detect_historical_anomalies(self, df_final):
        """
        ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏ó‡∏∏‡∏Å‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡πÅ‡∏ö‡∏ö Vectorization (‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤ Loop 100 ‡πÄ‡∏ó‡πà‡∏≤)
        Returns: Dictionary {(type, identifier, date_string): status}
        """
        if not self.config.ENABLE_HISTORICAL_HIGHLIGHT:
            return {}

        self.log("=" * 80)
        self.log("HISTORICAL ANOMALY (VECTORIZED): ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö‡∏£‡∏ß‡∏î‡πÄ‡∏£‡πá‡∏ß...")
        
        heatmap_map = {} 
        
        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡πÄ‡∏õ‡πá‡∏ô Int ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
        df_final = df_final.copy()
        df_final['MONTH_INT'] = df_final['MONTH'].astype(int)
        
        # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏ï‡∏≤‡∏°‡∏£‡∏∞‡∏î‡∏±‡∏ö (Product, Service, Business, Grand Total)
        for level_name, level_config in self.config.ANOMALY_LEVELS.items():
            # [OPTION] ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏Ñ‡πà Product Level ‡πÉ‡∏´‡πâ uncomment ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏•‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ
            # if level_name != 'product': continue 
            
            group_by = level_config["group_by"]
            
            # 1. Prepare Wide Format DataFrame
            if group_by:
                df_grouped = df_final.groupby(group_by + ['MONTH_INT'], dropna=False)['REVENUE_VALUE'].sum().reset_index()
                df_pivot = df_grouped.pivot_table(index=group_by, columns='MONTH_INT', values='REVENUE_VALUE', fill_value=0)
            else:
                df_grouped = df_final.groupby(['MONTH_INT'], dropna=False)['REVENUE_VALUE'].sum().reset_index()
                df_pivot = df_grouped.pivot_table(columns='MONTH_INT', values='REVENUE_VALUE', fill_value=0)
                df_pivot.index = ["GRAND_TOTAL"]

            # 2. Vectorized Calculation
            df_calc = df_pivot.replace(0, np.nan)
            min_periods = self.config.ANOMALY_MIN_HISTORY
            
            # q1_matrix = df_calc.expanding(min_periods=min_periods, axis=1).quantile(0.25).shift(1, axis=1)
            # q3_matrix = df_calc.expanding(min_periods=min_periods, axis=1).quantile(0.75).shift(1, axis=1)
            
            # ‡∏™‡∏•‡∏±‡∏ö‡πÅ‡∏Å‡∏ô (T) -> ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì (‡πÑ‡∏°‡πà‡∏°‡∏µ axis=1) -> ‡∏™‡∏•‡∏±‡∏ö‡∏Å‡∏•‡∏±‡∏ö (T)
            q1_matrix = df_calc.T.expanding(min_periods=min_periods).quantile(0.25).shift(1).T
            q3_matrix = df_calc.T.expanding(min_periods=min_periods).quantile(0.75).shift(1).T
            iqr_matrix = q3_matrix - q1_matrix
            
            k = self.config.ANOMALY_IQR_MULTIPLIER
            upper_fence = q3_matrix + (k * iqr_matrix)
            lower_fence = q1_matrix - (k * iqr_matrix)
            lower_fence[lower_fence < 0] = 0 
            
            # 3. Generate Status Masks
            is_neg = df_pivot < 0
            is_high = (df_pivot > upper_fence) & (upper_fence.notna())
            is_low = (df_pivot < lower_fence) & (lower_fence.notna())
            
            # 4. Convert to Dictionary for Excel
            def add_to_map(mask_df, status_code):
                anomalies = mask_df.stack()
                anomalies = anomalies[anomalies] # Filter True only
                
                for idx, _ in anomalies.items():
                    # idx structure depend on group_by length. 
                    # For Product: (Biz, Svc, Key, Name, Month)
                    month_int = idx[-1]
                    row_keys = idx[:-1]
                    
                    # [FIX] ‡∏™‡∏£‡πâ‡∏≤‡∏á Key ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö Excel Loop (‡∏ï‡∏±‡∏î Product Name ‡∏≠‡∏≠‡∏Å)
                    if level_name == 'product':
                        # row_keys = (Biz, Svc, ProdKey, ProdName) -> ‡πÄ‡∏≠‡∏≤‡πÅ‡∏Ñ‡πà 3 ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å
                        lookup_key = (row_keys[0], row_keys[1], row_keys[2]) 
                    elif level_name == 'service':
                        # row_keys = (Biz, Svc)
                        lookup_key = (row_keys[0], row_keys[1])
                    elif level_name == 'business':
                        lookup_key = row_keys[0]
                    else:
                        lookup_key = "GRAND_TOTAL"
                        
                    date_str = f"01/{month_int:02d}/{self.config.YEAR}"
                    heatmap_map[(level_name, lookup_key, date_str)] = status_code

            add_to_map(is_low, 'Low_Spike')
            add_to_map(is_high, 'High_Spike')
            add_to_map(is_neg, 'Negative_Value')
            
        self.log(f"  ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô ‡∏û‡∏ö‡∏à‡∏∏‡∏î‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥ {len(heatmap_map)} ‡∏à‡∏∏‡∏î")
        return heatmap_map
    
    def detect_anomalies(self, df_final):
        """
        ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥‡πÉ‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
        ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÉ‡∏ô 4 ‡∏£‡∏∞‡∏î‡∏±‡∏ö: Product, Service, Business, Grand Total
        (‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏° ADJ ‡πÅ‡∏•‡πâ‡∏ß)
        """
        self.log("=" * 80)
        self.log("ANOMALY DETECTION: ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥‡πÉ‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î")
        self.log("=" * 80)

        # ‡∏´‡∏≤‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î - ‡πÉ‡∏ä‡πâ end_month ‡∏à‡∏≤‡∏Å config ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ max ‡∏à‡∏≤‡∏Å data
        df_final['MONTH_INT'] = df_final['MONTH'].astype(int)
        if hasattr(self.config, 'end_month') and self.config.end_month:
            latest_month = self.config.end_month
            self.log(f"‡πÉ‡∏ä‡πâ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏à‡∏≤‡∏Å config: {latest_month} (end_month)", "INFO")
        else:
            latest_month = df_final['MONTH_INT'].max()
            self.log(f"‡πÉ‡∏ä‡πâ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏à‡∏≤‡∏Å data: {latest_month} (max month)", "WARNING")

        self.log(f"‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö: {latest_month}")
        
        anomaly_results = {}
        
        for level_name, level_config in self.config.ANOMALY_LEVELS.items():
            self.log(f"\n‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏î‡∏±‡∏ö: {level_name.upper()}")
            
            group_by = level_config["group_by"]
            
            # Aggregate ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏°‡∏£‡∏∞‡∏î‡∏±‡∏ö
            if group_by:
                df_grouped = df_final.groupby(
                    group_by + ['MONTH_INT'], 
                    dropna=False
                )['REVENUE_VALUE'].sum().reset_index()
            else:
                # Grand total - ‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
                df_grouped = df_final.groupby(
                    ['MONTH_INT'], 
                    dropna=False
                )['REVENUE_VALUE'].sum().reset_index()
                df_grouped['LEVEL'] = 'GRAND_TOTAL'
                group_by = ['LEVEL']
            
            # Pivot ‡πÄ‡∏õ‡πá‡∏ô wide format (‡πÅ‡∏ñ‡∏ß = entity, ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå = ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô)
            if group_by:
                df_pivot = df_grouped.pivot_table(
                    index=group_by,
                    columns='MONTH_INT',
                    values='REVENUE_VALUE',
                    fill_value=0
                ).reset_index()
            else:
                # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Grand Total
                df_pivot = df_grouped.pivot_table(
                    index=group_by,
                    columns='MONTH_INT',
                    values='REVENUE_VALUE',
                    fill_value=0
                ).reset_index()
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÄ‡∏î‡∏∑‡∏≠‡∏ô
            month_cols = [col for col in df_pivot.columns if isinstance(col, int)]
            month_cols.sort()
            
            if latest_month not in month_cols:
                self.log(f"‚ö†Ô∏è  ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏î‡∏∑‡∏≠‡∏ô {latest_month} ‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• - ‡∏Ç‡πâ‡∏≤‡∏°")
                continue
            
            # ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï
            latest_col = latest_month
            historical_cols = [m for m in month_cols if m < latest_month]
            
            if len(historical_cols) < self.config.ANOMALY_MIN_HISTORY:
                self.log(f"‚ö†Ô∏è  ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠ ({len(historical_cols)} ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô) - ‡∏Ç‡πâ‡∏≤‡∏°")
                continue
            
            self.log(f"‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö {len(df_pivot)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
            self.log(f"‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï: {historical_cols}")
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥
            df_pivot['ANOMALY_STATUS'] = df_pivot.apply(
                lambda row: self._check_anomaly_row(
                    row, 
                    latest_col, 
                    historical_cols
                ), 
                axis=1
            )
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏£‡∏¥‡∏°
            df_pivot['LATEST_MONTH'] = latest_month
            df_pivot['LATEST_VALUE'] = df_pivot[latest_col]
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï
            df_pivot['AVG_HISTORICAL'] = df_pivot[historical_cols].mean(axis=1)

            # [‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏´‡∏°‡πà] 1. ‡∏´‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Rolling Window (‡πÄ‡∏ä‡πà‡∏ô 3 ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î)
            window = self.config.ANOMALY_ROLLING_WINDOW
            # ‡∏ñ‡πâ‡∏≤‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏¢‡∏≤‡∏ß‡∏Å‡∏ß‡πà‡∏≤ window ‡πÉ‡∏´‡πâ‡∏ï‡∏±‡∏î‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡πâ‡∏≤‡∏¢‡πÜ, ‡∏ñ‡πâ‡∏≤‡∏™‡∏±‡πâ‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏Å‡πá‡πÄ‡∏≠‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
            rolling_cols = historical_cols[-window:] if len(historical_cols) >= window else historical_cols
            
            # [‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏´‡∏°‡πà] 2. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Rolling Average
            df_pivot['ROLLING_AVG'] = df_pivot[rolling_cols].mean(axis=1)
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì % change
            df_pivot['PCT_CHANGE'] = (
                (df_pivot['LATEST_VALUE'] - df_pivot['AVG_HISTORICAL']) / 
                df_pivot['AVG_HISTORICAL'].replace(0, np.nan) * 100
            ).fillna(0)

            # [‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏´‡∏°‡πà] 3. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì % change (‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö Rolling Average)
            df_pivot['PCT_ROLLING'] = (
                (df_pivot['LATEST_VALUE'] - df_pivot['ROLLING_AVG']) / 
                df_pivot['ROLLING_AVG'].replace(0, np.nan) * 100
            ).fillna(0)
            
            # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•
            status_counts = df_pivot['ANOMALY_STATUS'].value_counts()
            self.log(f"‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö:")
            for status, count in status_counts.items():
                self.log(f"  - {status}: {count} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")

            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì summary ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö web interface
            summary = {
                'total_anomalies': int(status_counts.drop('Normal', errors='ignore').sum()),
                'high_spikes': int(status_counts.get('High_Spike', 0)),
                'low_dips': int(status_counts.get('Low_Dip', 0)),
                'new_items': int(status_counts.get('New_Item', 0)),
                'negative_values': int(status_counts.get('Negative_Value', 0)),
                'normal': int(status_counts.get('Normal', 0)),
                'total_records': len(df_pivot)
            }

            # ‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏û‡∏£‡πâ‡∏≠‡∏° summary ‡πÅ‡∏•‡∏∞ dataframe
            anomaly_results[level_name] = {
                'summary': summary,
                'dataframe': df_pivot,
                'status_counts': status_counts.to_dict()
            }

        return anomaly_results
    
    def _check_anomaly_row(self, row, latest_col, historical_cols):
        """
        ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥‡∏Ç‡∏≠‡∏á‡πÅ‡∏ñ‡∏ß‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
        ‡πÉ‡∏ä‡πâ IQR method (Tukey's Fences)
        """
        # ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
        latest_val = row[latest_col]
        
        # 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡πà‡∏≤‡∏ï‡∏¥‡∏î‡∏•‡∏ö
        if latest_val < 0:
            return "Negative_Value"
        
        # ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï
        history = row[historical_cols]
        
        # ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡πà‡∏≤‡∏õ‡∏Å‡∏ï‡∏¥ (> 0)
        history_clean = history[history > 0]
        
        # 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏≠‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if len(history_clean) < self.config.ANOMALY_MIN_HISTORY:
            # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥ (‡∏´‡∏£‡∏∑‡∏≠‡∏°‡∏µ‡∏ô‡πâ‡∏≠‡∏¢‡πÑ‡∏õ)
            if latest_val > 0:
                return "New_Item" # ‡πÄ‡∏£‡∏≤‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô "Not_Enough_Data"
            return "Not_Enough_Data"
        
        # 3. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Robust Statistics (IQR)
        Q1 = history_clean.quantile(0.25)
        Q3 = history_clean.quantile(0.75)
        IQR = Q3 - Q1
        
        # 4. ‡∏Å‡∏£‡∏ì‡∏µ‡∏û‡∏¥‡πÄ‡∏®‡∏©: ‡∏¢‡∏≠‡∏î‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï‡∏Ñ‡∏á‡∏ó‡∏µ‡πà (IQR = 0)
        if IQR == 0:
            # ‡∏ñ‡πâ‡∏≤‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï‡πÄ‡∏õ‡πá‡∏ô 0 ‡∏´‡∏°‡∏î ‡πÅ‡∏•‡∏∞‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏Ñ‡πà‡∏≤
            if Q1 == 0 and latest_val > 0:
                return "High_Spike" # ‡∏ñ‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô Spike
            return "Normal" if latest_val == Q1 else "Spike_vs_Constant"
        
        # 5. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏±‡πâ‡∏ß (Tukey's Fences)
        k = self.config.ANOMALY_IQR_MULTIPLIER
        lower_fence = Q1 - (k * IQR)
        upper_fence = Q3 + (k * IQR)
        
        # 6. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á‡∏ú‡∏•
        if latest_val > upper_fence:
            return "High_Spike"
        
        # ‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏Ñ‡πà‡∏≤‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤ 0
        lower_fence_adjusted = max(0, lower_fence) 

        if latest_val < lower_fence_adjusted:
            return "Low_Spike"
        
        return "Normal"
    
    def create_anomaly_report_sheets(self, anomaly_results, output_file):
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á sheet ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÉ‡∏ô Excel ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Anomaly Detection Report
        """
        self.log("=" * 80)
        self.log("‡∏™‡∏£‡πâ‡∏≤‡∏á Anomaly Detection Report Sheets")
        self.log("=" * 80)
        
        try:
            from openpyxl import load_workbook
            from openpyxl.styles import Font, PatternFill, Alignment
            from openpyxl.utils import get_column_letter
        except ImportError:
            self.log("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö openpyxl, ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á Anomaly Sheets ‡πÑ‡∏î‡πâ")
            self.log("  ‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏î‡πâ‡∏ß‡∏¢: pip install openpyxl")
            return

        
        # ‡πÄ‡∏õ‡∏¥‡∏î‡πÑ‡∏ü‡∏•‡πå Excel ‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ß‡πâ‡πÅ‡∏•‡πâ‡∏ß
        try:
            wb = load_workbook(output_file)
        except Exception as e:
            self.log(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏õ‡∏¥‡∏î‡πÑ‡∏ü‡∏•‡πå Excel ‡πÑ‡∏î‡πâ: {e}")
            return
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Summary Sheet
        self._create_anomaly_summary_sheet(wb, anomaly_results)

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Detail Sheets ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏£‡∏∞‡∏î‡∏±‡∏ö
        for level_name, level_data in anomaly_results.items():
            self._create_anomaly_detail_sheet(wb, level_name, level_data)
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå
        try:
            wb.save(output_file)
            self.log(f"‚úì ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Anomaly Report ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢: {output_file}")
        except Exception as e:
            self.log(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå Excel ‡πÑ‡∏î‡πâ (‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÄ‡∏õ‡∏¥‡∏î‡∏Ñ‡πâ‡∏≤‡∏á‡∏≠‡∏¢‡∏π‡πà): {e}")
    
    def _create_anomaly_summary_sheet(self, wb, anomaly_results):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á Summary Sheet ‡πÅ‡∏™‡∏î‡∏á‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö"""
        
        from openpyxl.styles import Font, PatternFill, Alignment

        if 'Anomaly Summary' in wb.sheetnames:
            try:
                del wb['Anomaly Summary']
            except Exception as e:
                self.log(f"  Warning: ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏•‡∏ö sheet 'Anomaly Summary' ‡πÄ‡∏Å‡πà‡∏≤‡πÑ‡∏î‡πâ: {e}")

        ws = wb.create_sheet('Anomaly Summary', 0) # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ß‡πâ‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏£‡∏Å
        
        # Header
        ws['A1'] = 'ANOMALY DETECTION SUMMARY'
        ws['A1'].font = Font(bold=True, size=14, color='FFFFFF')
        ws['A1'].fill = PatternFill(start_color='1F4E78', end_color='1F4E78', fill_type='solid')
        ws['A1'].alignment = Alignment(horizontal='center', vertical='center')
        ws.merge_cells('A1:F1')
        
        # Column headers
        headers = ['Level', 'Total Items', 'Normal', 'High Spike', 'Low Spike', 'Other Issues']
        for col_idx, header in enumerate(headers, 1):
            cell = ws.cell(row=3, column=col_idx, value=header)
            cell.font = Font(bold=True)
            cell.fill = PatternFill(start_color='D9E1F2', end_color='D9E1F2', fill_type='solid')
            cell.alignment = Alignment(horizontal='center')
        
        # Data
        row = 4
        if not anomaly_results:
            ws.cell(row=row, column=1, value="‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö")
            return

        for level_name, level_data in anomaly_results.items():
            # ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á dataframe ‡πÅ‡∏•‡∏∞ status_counts ‡∏à‡∏≤‡∏Å dict structure ‡πÉ‡∏´‡∏°‡πà
            df_result = level_data.get('dataframe')
            if df_result is None:
                continue

            # ‡πÉ‡∏ä‡πâ status_counts ‡∏ó‡∏µ‡πà‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÑ‡∏ß‡πâ‡πÅ‡∏•‡πâ‡∏ß ‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÉ‡∏´‡∏°‡πà
            status_counts = level_data.get('status_counts', {})
            if not status_counts and 'ANOMALY_STATUS' in df_result.columns:
                status_counts = df_result['ANOMALY_STATUS'].value_counts().to_dict()

            ws.cell(row=row, column=1, value=level_name.upper())
            ws.cell(row=row, column=2, value=len(df_result))
            ws.cell(row=row, column=3, value=status_counts.get('Normal', 0))
            ws.cell(row=row, column=4, value=status_counts.get('High_Spike', 0))
            ws.cell(row=row, column=5, value=status_counts.get('Low_Spike', 0))

            other = (
                status_counts.get('Negative_Value', 0) +
                status_counts.get('Not_Enough_Data', 0) +
                status_counts.get('Spike_vs_Constant', 0) +
                status_counts.get('New_Item', 0)
            )
            ws.cell(row=row, column=6, value=other)
            
            # Highlight ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤
            if (status_counts.get('High_Spike', 0) > 0 or 
                status_counts.get('Low_Spike', 0) > 0 or
                status_counts.get('Negative_Value', 0) > 0):
                for col in range(1, 7):
                    ws.cell(row=row, column=col).fill = PatternFill(
                        start_color='FFF2CC', end_color='FFF2CC', fill_type='solid'
                    )
            
            row += 1
        
        # ‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
        ws.column_dimensions['A'].width = 20
        for col in ['B', 'C', 'D', 'E', 'F']:
            ws.column_dimensions[col].width = 15
    
    def _create_anomaly_detail_sheet(self, wb, level_name, level_data):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á Detail Sheet ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏£‡∏∞‡∏î‡∏±‡∏ö"""

        from openpyxl.styles import Font, PatternFill, Alignment
        from openpyxl.utils import get_column_letter

        # ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á dataframe ‡∏à‡∏≤‡∏Å dict structure ‡πÉ‡∏´‡∏°‡πà
        df_result = level_data.get('dataframe')
        if df_result is None:
            return

        sheet_name = f'Anomaly_{level_name.title()}'

        if sheet_name in wb.sheetnames:
            try:
                del wb[sheet_name]
            except Exception as e:
                self.log(f"  Warning: ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏•‡∏ö sheet '{sheet_name}' ‡πÄ‡∏Å‡πà‡∏≤‡πÑ‡∏î‡πâ: {e}")

        ws = wb.create_sheet(sheet_name)

        # ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥
        if 'ANOMALY_STATUS' in df_result.columns:
            df_anomaly = df_result[~df_result['ANOMALY_STATUS'].isin(['Normal', 'Not_Enough_Data'])].copy()
        else:
            df_anomaly = pd.DataFrame()  # Empty dataframe if no ANOMALY_STATUS column

        if len(df_anomaly) == 0:
            ws['A1'] = f'‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö {level_name.upper()}'
            ws['A1'].font = Font(bold=True, size=12, color='008000')
            return
        
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏° absolute % change
        df_anomaly['ABS_PCT_CHANGE'] = df_anomaly['PCT_CHANGE'].abs()
        df_anomaly = df_anomaly.sort_values('ABS_PCT_CHANGE', ascending=False)
        
        # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÅ‡∏™‡∏î‡∏á
        display_cols = []
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏° group columns
        group_cols = self.config.ANOMALY_LEVELS[level_name]["group_by"]
        if group_cols:
            display_cols.extend(group_cols)
        else:
            display_cols.append('LEVEL')
        
        display_cols.extend([
            'ANOMALY_STATUS', 'LATEST_VALUE', 'AVG_HISTORICAL', 'PCT_CHANGE', 'ROLLING_AVG', 'PCT_ROLLING'
        ])
        
        df_display = df_anomaly[display_cols].copy()
        
        # ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á Excel
        # Headers
        for col_idx, col_name in enumerate(df_display.columns, 1):
            cell = ws.cell(row=1, column=col_idx, value=col_name)
            cell.font = Font(bold=True, color='FFFFFF')
            cell.fill = PatternFill(start_color='1F4E78', end_color='1F4E78', fill_type='solid')
            cell.alignment = Alignment(horizontal='center', vertical='center')
        
        # Data
        for row_idx, (_, row) in enumerate(df_display.iterrows(), 2):
            for col_idx, value in enumerate(row, 1):
                cell = ws.cell(row=row_idx, column=col_idx, value=value)
                
                # ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
                if col_idx > len(group_cols):
                    # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ PCT ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏™‡πà %
                    if 'PCT' in df_display.columns[col_idx-1]:
                        cell.number_format = '0.00"%"'
                    else:
                        cell.number_format = '#,##0.00'
                    cell.alignment = Alignment(horizontal='right')
                
                # Highlight ‡∏ï‡∏≤‡∏° status
                status = row['ANOMALY_STATUS']
                if status == 'High_Spike':
                    cell.fill = PatternFill(start_color='FFC7CE', end_color='FFC7CE', fill_type='solid') # Red
                elif status == 'Low_Spike':
                    cell.fill = PatternFill(start_color='FFEB9C', end_color='FFEB9C', fill_type='solid') # Yellow
                elif status == 'Negative_Value':
                    cell.fill = PatternFill(start_color='FF0000', end_color='FF0000', fill_type='solid') # Bright Red
                    cell.font = Font(color='FFFFFF')
                elif status == 'Spike_vs_Constant':
                    cell.fill = PatternFill(start_color='FFC7CE', end_color='FFC7CE', fill_type='solid') # Red
                elif status == 'New_Item':
                    cell.fill = PatternFill(start_color='C6E0B4', end_color='C6E0B4', fill_type='solid') # Green
        
        # ‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
        for col_idx, col_name in enumerate(df_display.columns, 1):
            width = 20
            if col_name in ('PRODUCT_NAME', 'SERVICE_GROUP', 'BUSINESS_GROUP'):
                width = 35
            ws.column_dimensions[get_column_letter(col_idx)].width = width
    
    def run(self):
        """‡∏£‡∏±‡∏ô ETL Pipeline ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ‡∏û‡∏£‡πâ‡∏≠‡∏° Anomaly Detection"""
        try:
            start_time = datetime.now()
            self.log(f"‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô ETL Pipeline ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏µ {self.config.YEAR}")
            self.log(f"‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô: {start_time}")
            
            # Step 1-4: ETL Process (Step 4 ‡∏ñ‡∏π‡∏Å‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÉ‡∏´‡πâ‡∏£‡∏ß‡∏° ADJ data)
            df = self.step1_concat_revenue_files()

            # ============================================================
            # Step 0: Reconciliation (‡∏´‡∏•‡∏±‡∏á step1, ‡∏Å‡πà‡∏≠‡∏ô step2) <<< ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ
            # ============================================================
            try:
                self.step0_reconcile_revenue()
            except ReconciliationError as e:
                # ‡πÅ‡∏™‡∏î‡∏á error message ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà print traceback
                self.log("\n‚ùå ETL Pipeline ‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô: Reconciliation Failed", "ERROR")
                self.log("üí° ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö log file ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î", "INFO")
                # Return None ‡πÅ‡∏ó‡∏ô raise ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î traceback ‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô
                return None, {}

            df = self.step2_mapping_cost_center(df)
            df = self.step3_mapping_product(df)
            df_final = self.step4_create_final_report(df)
            
            # Step 5: Anomaly Detection (‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏£‡∏±‡∏ô‡∏ö‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• NT1 + ADJ)
            self.log("\n" + "=" * 80)
            self.log("STEP 5: Anomaly Detection")
            self.log("=" * 80)
            
            anomaly_results = self.detect_anomalies(df_final)
            
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å anomaly results ‡πÄ‡∏õ‡πá‡∏ô CSV
            for level_name, level_data in anomaly_results.items():
                # ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á dataframe ‡∏à‡∏≤‡∏Å dict structure ‡πÉ‡∏´‡∏°‡πà
                df_result = level_data.get('dataframe')
                if df_result is None or df_result.empty:
                    self.log(f"‡∏Ç‡πâ‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å anomaly level '{level_name}' (‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•)")
                    continue
                output_file = os.path.join(
                    self.paths["final_output"],
                    f"anomaly_{level_name}_{self.config.YEAR}.csv"
                )
                df_result.to_csv(output_file, index=False)
                self.log(f"‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏• anomaly detection: {output_file}")
            
            end_time = datetime.now()
            duration = end_time - start_time
            
            self.log("=" * 80)
            self.log(f"‚úì ETL Pipeline ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!")
            self.log(f"‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ: {duration}")
            self.log("=" * 80)
            
            return df_final, anomaly_results

        except Exception as e:
            self.log(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏£‡πâ‡∏≤‡∏¢‡πÅ‡∏£‡∏á‡πÉ‡∏ô ETL Pipeline: {str(e)}")
            import traceback
            traceback.print_exc()
            raise

    def create_excel_report(self, df_result, anomaly_results, excel_output_file=None):
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á Excel Report ‡∏û‡∏£‡πâ‡∏≠‡∏° Pivot Table ‡πÅ‡∏•‡∏∞ Formatting

        Args:
            df_result: DataFrame ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å ETL Pipeline
            anomaly_results: Dictionary ‡∏Ç‡∏≠‡∏á anomaly detection results
            excel_output_file: Optional path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå Excel output (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏à‡∏∞‡πÉ‡∏ä‡πâ default)

        Returns:
            str: Path ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå Excel ‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏™‡∏£‡πá‡∏à
        """
        # Import dependencies
        import numpy as np
        try:
            from openpyxl import load_workbook
            from openpyxl.styles import Font, PatternFill, Alignment, Border, Side
            from openpyxl.utils import get_column_letter
        except ImportError:
            self.logger.error("="*80)
            self.logger.error("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏°‡∏î‡∏π‡∏• openpyxl")
            self.logger.error("  ‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á: pip install openpyxl")
            self.logger.error("  ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÑ‡∏ü‡∏•‡πå Excel ‡πÑ‡∏î‡πâ")
            self.logger.error("="*80)
            raise ImportError("openpyxl is required for creating Excel reports")

        # Detect historical anomalies
        historical_anomalies = {}
        if self.config.ENABLE_HISTORICAL_HIGHLIGHT:
            self.log("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Historical Anomalies...")
            historical_anomalies = self.detect_historical_anomalies(df_result)

        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• YTD ‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ß‡πâ‡πÉ‡∏ô step 4
        df_adj_ytd = self.df_adj_ytd

        self.logger.info("\n" + "=" * 80)
        self.logger.info("‡∏™‡∏£‡πâ‡∏≤‡∏á Excel Report...")
        self.logger.info("=" * 80)

        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå Excel
        if excel_output_file is None:
            # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå
            # 1. ‡πÉ‡∏ä‡πâ end_month ‡∏à‡∏≤‡∏Å config (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
            # 2. ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ ‡πÉ‡∏ä‡πâ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
            if hasattr(self.config, 'end_month') and self.config.end_month:
                report_month = self.config.end_month
                self.logger.info(f"‡πÉ‡∏ä‡πâ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏à‡∏≤‡∏Å config: {report_month} (end_month)")
            else:
                # ‡∏´‡∏≤‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• df_result
                if 'MONTH' in df_result.columns:
                    report_month = int(df_result['MONTH'].max())
                    self.logger.info(f"‡πÉ‡∏ä‡πâ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {report_month} (max month)")
                else:
                    report_month = 12  # default ‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠
                    self.logger.warning(f"‡πÑ‡∏°‡πà‡∏û‡∏ö MONTH column, ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ default: {report_month}")

            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏ö‡∏ö revenue_report_YYYYMM.xlsx
            excel_output_file = os.path.join(
                self.paths["final_output"],
                f"revenue_report_{self.config.YEAR}{report_month:02d}.xlsx"
            )
        self.logger.info(f"‡πÑ‡∏ü‡∏•‡πå Excel ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á: {excel_output_file}")

        # Aggregate ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏° hierarchy ‡πÅ‡∏•‡∏∞‡πÄ‡∏î‡∏∑‡∏≠‡∏ô
        df = df_result.copy()
        agg_data = df.groupby([
            'ITEM', 'BUSINESS_GROUP',
            'SUB_ITEM', 'SERVICE_GROUP',
            'PRODUCT_KEY', 'PRODUCT_NAME',
            'YEAR', 'MONTH'
        ])['AMOUNT'].sum().reset_index()

        # ‡πÅ‡∏õ‡∏•‡∏á MONTH ‡πÄ‡∏õ‡πá‡∏ô int
        agg_data['MONTH'] = agg_data['MONTH'].astype(int)

        # Filter ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Excel ‡∏°‡∏µ column ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£)
        if hasattr(self.config, 'end_month') and self.config.end_month:
            original_count = len(agg_data)
            agg_data = agg_data[agg_data['MONTH'] <= self.config.end_month]
            filtered_count = len(agg_data)
            self.logger.info(f"  ‚úì Filter Excel Report Data: {original_count} ‚Üí {filtered_count} records (‡πÄ‡∏î‡∏∑‡∏≠‡∏ô 1-{self.config.end_month})")

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö DD/MM/YYYY
        agg_data['DATE_STR'] = agg_data.apply(
            lambda row: f"01/{row['MONTH']:02d}/{row['YEAR']}",
            axis=1
        )

        # ========================================================================
        # Nested function: create_report
        # ========================================================================
        def create_report(agg_data, anomaly_results, df_adj_ytd, sort_ascending=True):
            """
            ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏£‡∏≤‡∏¢‡πÄ‡∏î‡∏∑‡∏≠‡∏ô
            ‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏≠‡∏±‡∏õ‡πÄ‡∏Å‡∏£‡∏î:
            - ‡∏£‡∏ß‡∏° ANOMALY_STATUS
            - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÅ‡∏ñ‡∏ß "‡∏£‡∏ß‡∏°‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£"
            - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏•‡∏∏‡πà‡∏° "‡∏ú‡∏•‡∏ï‡∏≠‡∏ö‡πÅ‡∏ó‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏≠‡∏∑‡πà‡∏ô"
            - ‡πÉ‡∏ä‡πâ YTD ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå "‡∏ú‡∏•‡∏£‡∏ß‡∏°" ‡∏Ç‡∏≠‡∏á‡∏Å‡∏•‡∏∏‡πà‡∏° ADJ
            """

            self.logger.info(f"  ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô (sort_ascending={sort_ascending})...")

            # --- 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á Anomaly Maps ---
            self.logger.info("    ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Anomaly Maps...")
            prod_map, serv_map, biz_map, grand_total_status = {}, {}, {}, ''

            try:
                # Product Map
                if 'product' in anomaly_results and 'dataframe' in anomaly_results['product']:
                    df_prod = anomaly_results['product']['dataframe']
                    if 'ANOMALY_STATUS' in df_prod.columns:
                        prod_cols = ['BUSINESS_GROUP', 'SERVICE_GROUP', 'PRODUCT_KEY', 'ANOMALY_STATUS']
                        df_prod_subset = df_prod[prod_cols]
                        prod_map = df_prod_subset.set_index(['BUSINESS_GROUP', 'SERVICE_GROUP', 'PRODUCT_KEY'])['ANOMALY_STATUS'].to_dict()

                # Service Map
                if 'service' in anomaly_results and 'dataframe' in anomaly_results['service']:
                    df_serv = anomaly_results['service']['dataframe']
                    if 'ANOMALY_STATUS' in df_serv.columns:
                        serv_cols = ['BUSINESS_GROUP', 'SERVICE_GROUP', 'ANOMALY_STATUS']
                        df_serv_subset = df_serv[serv_cols]
                        serv_map = df_serv_subset.set_index(['BUSINESS_GROUP', 'SERVICE_GROUP'])['ANOMALY_STATUS'].to_dict()

                # Business Map
                if 'business' in anomaly_results and 'dataframe' in anomaly_results['business']:
                    df_biz = anomaly_results['business']['dataframe']
                    if 'ANOMALY_STATUS' in df_biz.columns:
                        biz_cols = ['BUSINESS_GROUP', 'ANOMALY_STATUS']
                        df_biz_subset = df_biz[biz_cols]
                        biz_map = df_biz_subset.set_index(['BUSINESS_GROUP'])['ANOMALY_STATUS'].to_dict()

                # Grand Total Status
                if 'grand_total' in anomaly_results and 'dataframe' in anomaly_results['grand_total']:
                    df_grand = anomaly_results['grand_total']['dataframe']
                    if 'ANOMALY_STATUS' in df_grand.columns and len(df_grand) > 0:
                        grand_total_status = df_grand['ANOMALY_STATUS'].values[0]

            except KeyError as e:
                self.logger.warning(f"    Warning: ‡πÑ‡∏°‡πà‡∏û‡∏ö anomaly key {e}, ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á")
                prod_map, serv_map, biz_map, grand_total_status = {}, {}, {}, ''
            except Exception as e:
                self.logger.error(f"    Error creating anomaly maps: {e}")
                prod_map, serv_map, biz_map, grand_total_status = {}, {}, {}, ''

            # --- 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á YTD Map ---
            self.logger.info("    ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á YTD data map...")
            ytd_map = {}
            if df_adj_ytd is not None and not df_adj_ytd.empty:

                # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'TYPE' ‡πÅ‡∏•‡∏∞ 'REVENUE_TYPE' ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô‡∏Å‡∏±‡∏ô
                if "REVENUE_TYPE" in df_adj_ytd.columns and "TYPE" in df_adj_ytd.columns:
                    print("    YTD Map: ‡∏û‡∏ö‡∏ó‡∏±‡πâ‡∏á 'TYPE' ‡πÅ‡∏•‡∏∞ 'REVENUE_TYPE', ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏£‡∏ß‡∏°...")
                    df_adj_ytd['TYPE'] = df_adj_ytd['TYPE'].fillna(df_adj_ytd['REVENUE_TYPE'])
                    df_adj_ytd = df_adj_ytd.drop(columns=['REVENUE_TYPE'])
                elif "REVENUE_TYPE" in df_adj_ytd.columns:
                    print("    YTD Map: ‡∏û‡∏ö 'REVENUE_TYPE', ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡πá‡∏ô 'TYPE'...")
                    df_adj_ytd = df_adj_ytd.rename(columns={"REVENUE_TYPE": "TYPE"})

                if "TYPE" in df_adj_ytd.columns:
                    # Sum by TYPE ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏ñ‡∏ß
                    ytd_summary = df_adj_ytd.groupby("TYPE")["REVENUE_VALUE"].sum()
                    ytd_map[Config.FINANCIAL_INCOME_NAME] = ytd_summary.get(Config.FINANCIAL_INCOME_NAME, 0)
                    ytd_map[Config.OTHER_REVENUE_ADJ_NAME] = ytd_summary.get(Config.OTHER_REVENUE_ADJ_NAME, 0)
                else:
                    self.logger.warning("    Warning: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'TYPE' ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå YTD")

            print(f"    YTD Map: {ytd_map}")

            # --- 3. ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞ Pivot ---
            if sort_ascending:
                agg_data_sorted = agg_data.sort_values(['YEAR', 'MONTH'], ascending=[True, True])
            else:
                agg_data_sorted = agg_data.sort_values(['YEAR', 'MONTH'], ascending=[True, False])

            pivot = agg_data_sorted.pivot_table(
                index=['ITEM', 'BUSINESS_GROUP', 'SUB_ITEM', 'SERVICE_GROUP', 'PRODUCT_KEY', 'PRODUCT_NAME'],
                columns='DATE_STR',
                values='AMOUNT',
                aggfunc='sum',
                fill_value=0
            )

            month_cols = agg_data_sorted['DATE_STR'].unique().tolist()
            pivot = pivot[month_cols]

            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ú‡∏•‡∏£‡∏ß‡∏° (Sum(‡πÄ‡∏î‡∏∑‡∏≠‡∏ô)) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å‡πÅ‡∏ñ‡∏ß
            pivot.insert(0, '‡∏ú‡∏•‡∏£‡∏ß‡∏°', pivot.sum(axis=1))
            result = pivot.reset_index()

            # --- 4. ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏û‡∏£‡πâ‡∏≠‡∏° Status ---
            rows = []

            # ‡πÅ‡∏¢‡∏Å‡∏Å‡∏•‡∏∏‡πà‡∏° NT1 ‡∏õ‡∏Å‡∏ï‡∏¥ ‡πÅ‡∏•‡∏∞‡∏Å‡∏•‡∏∏‡πà‡∏° ADJ
            standard_groups_df = result[result['BUSINESS_GROUP'] != Config.NEW_ADJ_BUSINESS_GROUP]
            adj_group_df = result[result['BUSINESS_GROUP'] == Config.NEW_ADJ_BUSINESS_GROUP]

            grouped_items = standard_groups_df.groupby(['ITEM', 'BUSINESS_GROUP'])

            # ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì "‡∏£‡∏ß‡∏°‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£"
            total_service_revenue_data = pd.DataFrame()

            # --- 4a. ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡∏õ‡∏Å‡∏ï‡∏¥ (NT1) ---
            print("    ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡∏õ‡∏Å‡∏ï‡∏¥ (NT1)")
            for (item, business_group), item_data in grouped_items:
                grouped_sub_items = item_data.groupby(['SUB_ITEM', 'SERVICE_GROUP'])

                for (sub_item, service_group), sub_item_data in grouped_sub_items:
                    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÅ‡∏ñ‡∏ß‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ product
                    for _, row in sub_item_data.iterrows():

                        prod_key = (row['BUSINESS_GROUP'], row['SERVICE_GROUP'], row['PRODUCT_KEY'])
                        prod_status = prod_map.get(prod_key, '')

                        row_dict = {
                            '‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à': row['BUSINESS_GROUP'],
                            '‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£': row['SERVICE_GROUP'],
                            '‡∏£‡∏´‡∏±‡∏™‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£': row['PRODUCT_KEY'],
                            '‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£': row['PRODUCT_NAME'],
                        }

                        if sort_ascending:
                            row_dict['‡∏ú‡∏•‡∏£‡∏ß‡∏°'] = row['‡∏ú‡∏•‡∏£‡∏ß‡∏°']
                            for col in month_cols:
                                row_dict[col] = row[col]
                            row_dict['ANOMALY_STATUS'] = prod_status
                        else:
                            row_dict['‡∏ú‡∏•‡∏£‡∏ß‡∏°'] = row['‡∏ú‡∏•‡∏£‡∏ß‡∏°']
                            row_dict['ANOMALY_STATUS'] = prod_status
                            for col in month_cols:
                                row_dict[col] = row[col]

                        rows.append(row_dict)

                    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÅ‡∏ñ‡∏ß‡∏ú‡∏•‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£
                    serv_key = (business_group, service_group)
                    serv_status = serv_map.get(serv_key, '')

                    sum_row = {
                        '‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à': '',
                        '‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£': f'‡∏£‡∏ß‡∏° {service_group}',
                        '‡∏£‡∏´‡∏±‡∏™‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£': '',
                        '‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£': '',
                    }

                    if sort_ascending:
                        sum_row['‡∏ú‡∏•‡∏£‡∏ß‡∏°'] = sub_item_data['‡∏ú‡∏•‡∏£‡∏ß‡∏°'].sum()
                        for col in month_cols:
                            sum_row[col] = sub_item_data[col].sum()
                        sum_row['ANOMALY_STATUS'] = serv_status
                    else:
                        sum_row['‡∏ú‡∏•‡∏£‡∏ß‡∏°'] = sub_item_data['‡∏ú‡∏•‡∏£‡∏ß‡∏°'].sum()
                        sum_row['ANOMALY_STATUS'] = serv_status
                        for col in month_cols:
                            sum_row[col] = sub_item_data[col].sum()

                    rows.append(sum_row)

                    # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö "‡∏£‡∏ß‡∏°‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£"
                    if service_group != Config.NON_TELECOM_SERVICE_GROUP:
                        total_service_revenue_data = pd.concat([
                            total_service_revenue_data,
                            sub_item_data
                        ])

                # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÅ‡∏ñ‡∏ß‡∏ú‡∏•‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à
                biz_key = business_group
                biz_status = biz_map.get(biz_key, '')

                sum_row = {
                    '‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à': f'‡∏£‡∏ß‡∏° {business_group}',
                    '‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£': '',
                    '‡∏£‡∏´‡∏±‡∏™‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£': '',
                    '‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£': '',
                }

                if sort_ascending:
                    sum_row['‡∏ú‡∏•‡∏£‡∏ß‡∏°'] = item_data['‡∏ú‡∏•‡∏£‡∏ß‡∏°'].sum()
                    for col in month_cols:
                        sum_row[col] = item_data[col].sum()
                    sum_row['ANOMALY_STATUS'] = biz_status
                else:
                    sum_row['‡∏ú‡∏•‡∏£‡∏ß‡∏°'] = item_data['‡∏ú‡∏•‡∏£‡∏ß‡∏°'].sum()
                    sum_row['ANOMALY_STATUS'] = biz_status
                    for col in month_cols:
                        sum_row[col] = item_data[col].sum()

                rows.append(sum_row)

            # --- 4b. ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÅ‡∏ñ‡∏ß "‡∏£‡∏ß‡∏°‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£" ---
            print("    ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÅ‡∏ñ‡∏ß '‡∏£‡∏ß‡∏°‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£'")
            sum_row_service = {
                '‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à': '‡∏£‡∏ß‡∏°‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£',
                '‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£': '',
                '‡∏£‡∏´‡∏±‡∏™‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£': '',
                '‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£': '',
            }

            if sort_ascending:
                sum_row_service['‡∏ú‡∏•‡∏£‡∏ß‡∏°'] = total_service_revenue_data['‡∏ú‡∏•‡∏£‡∏ß‡∏°'].sum()
                for col in month_cols:
                    sum_row_service[col] = total_service_revenue_data[col].sum()
                sum_row_service['ANOMALY_STATUS'] = ''
            else:
                sum_row_service['‡∏ú‡∏•‡∏£‡∏ß‡∏°'] = total_service_revenue_data['‡∏ú‡∏•‡∏£‡∏ß‡∏°'].sum()
                sum_row_service['ANOMALY_STATUS'] = ''
                for col in month_cols:
                    sum_row_service[col] = total_service_revenue_data[col].sum()

            rows.append(sum_row_service)

            # --- 4c. ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Å‡∏•‡∏∏‡πà‡∏° ADJ ---
            print("    ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Å‡∏•‡∏∏‡πà‡∏° '‡∏ú‡∏•‡∏ï‡∏≠‡∏ö‡πÅ‡∏ó‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏≠‡∏∑‡πà‡∏ô'")
            if not adj_group_df.empty:
                # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÅ‡∏ñ‡∏ß‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏Å‡∏•‡∏∏‡πà‡∏°
                adj_header_row = {
                    '‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à': Config.NEW_ADJ_BUSINESS_GROUP,
                    '‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£': '', '‡∏£‡∏´‡∏±‡∏™‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£': '', '‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£': ''
                }
                if sort_ascending:
                    adj_header_row['‡∏ú‡∏•‡∏£‡∏ß‡∏°'] = ''
                    for col in month_cols: adj_header_row[col] = ''
                    adj_header_row['ANOMALY_STATUS'] = ''
                else:
                    adj_header_row['‡∏ú‡∏•‡∏£‡∏ß‡∏°'] = ''
                    adj_header_row['ANOMALY_STATUS'] = ''
                    for col in month_cols: adj_header_row[col] = ''
                rows.append(adj_header_row)

                # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö
                adj_group_df = adj_group_df.sort_values(by='SERVICE_GROUP', ascending=True)

                for _, row in adj_group_df.iterrows():
                    service_group = row['SERVICE_GROUP']

                    serv_key = (Config.NEW_ADJ_BUSINESS_GROUP, service_group)
                    serv_status = serv_map.get(serv_key, '')

                    row_dict = {
                        '‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à': '',
                        '‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£': service_group,
                        '‡∏£‡∏´‡∏±‡∏™‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£': '',
                        '‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£': '',
                    }

                    # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ YTD ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå "‡∏ú‡∏•‡∏£‡∏ß‡∏°"
                    ytd_value = ytd_map.get(service_group, 0)

                    if sort_ascending:
                        row_dict['‡∏ú‡∏•‡∏£‡∏ß‡∏°'] = ytd_value
                        for col in month_cols:
                            row_dict[col] = row[col]
                        row_dict['ANOMALY_STATUS'] = serv_status
                    else:
                        row_dict['‡∏ú‡∏•‡∏£‡∏ß‡∏°'] = ytd_value
                        row_dict['ANOMALY_STATUS'] = serv_status
                        for col in month_cols:
                            row_dict[col] = row[col]

                    rows.append(row_dict)

                # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÅ‡∏ñ‡∏ß "‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏ï‡∏≠‡∏ö‡πÅ‡∏ó‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏≠‡∏∑‡πà‡∏ô"
                sum_adj_total_ytd = sum(ytd_map.values())

                sum_row_adj = {
                    '‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à': '',
                    '‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£': '‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏ï‡∏≠‡∏ö‡πÅ‡∏ó‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏≠‡∏∑‡πà‡∏ô',
                    '‡∏£‡∏´‡∏±‡∏™‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£': '',
                    '‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£': '',
                }

                biz_key = Config.NEW_ADJ_BUSINESS_GROUP
                biz_status = biz_map.get(biz_key, '')

                if sort_ascending:
                    sum_row_adj['‡∏ú‡∏•‡∏£‡∏ß‡∏°'] = sum_adj_total_ytd
                    for col in month_cols:
                        sum_row_adj[col] = adj_group_df[col].sum()
                    sum_row_adj['ANOMALY_STATUS'] = biz_status
                else:
                    sum_row_adj['‡∏ú‡∏•‡∏£‡∏ß‡∏°'] = sum_adj_total_ytd
                    sum_row_adj['ANOMALY_STATUS'] = biz_status
                    for col in month_cols:
                        sum_row_adj[col] = adj_group_df[col].sum()

                rows.append(sum_row_adj)

            # --- 4d. ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÅ‡∏ñ‡∏ß‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏¥‡πâ‡∏ô ---
            print("    ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÅ‡∏ñ‡∏ß '‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏¥‡πâ‡∏ô'")

            nt1_total = standard_groups_df['‡∏ú‡∏•‡∏£‡∏ß‡∏°'].sum()
            adj_total_ytd = sum(ytd_map.values())
            report_grand_total = nt1_total + adj_total_ytd

            sum_row = {
                '‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à': '‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏¥‡πâ‡∏ô',
                '‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£': '',
                '‡∏£‡∏´‡∏±‡∏™‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£': '',
                '‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£': '',
            }

            if sort_ascending:
                sum_row['‡∏ú‡∏•‡∏£‡∏ß‡∏°'] = report_grand_total
                for col in month_cols:
                    sum_row[col] = result[col].sum()
                sum_row['ANOMALY_STATUS'] = grand_total_status
            else:
                sum_row['‡∏ú‡∏•‡∏£‡∏ß‡∏°'] = report_grand_total
                sum_row['ANOMALY_STATUS'] = grand_total_status
                for col in month_cols:
                    sum_row[col] = result[col].sum()

            rows.append(sum_row)

            return pd.DataFrame(rows)

        # ========================================================================
        # Nested function: format_excel
        # ========================================================================
        def format_excel(filename, anomaly_map=None):
            """‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö Excel ‡πÉ‡∏´‡πâ‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°"""
            self.logger.info("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö Excel...")
            try:
                wb = load_workbook(filename)
            except Exception as e:
                self.logger.error(f"  ‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏õ‡∏¥‡∏î‡πÑ‡∏ü‡∏•‡πå Excel ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÑ‡∏î‡πâ: {e}")
                return

            for sheet_name in wb.sheetnames:
                # ‡∏Ç‡πâ‡∏≤‡∏° anomaly sheets
                if sheet_name.startswith('Anomaly') or sheet_name == 'Anomaly Summary':
                    continue

                ws = wb[sheet_name]

                # ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö header
                header_fill = PatternFill(start_color='366092', end_color='366092', fill_type='solid')
                header_font = Font(bold=True, color='FFFFFF', size=11)

                for cell in ws[1]:
                    cell.fill = header_fill
                    cell.font = header_font
                    cell.alignment = Alignment(horizontal='center', vertical='center', wrap_text=True)

                # ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
                text_columns = {'‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à', '‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£', '‡∏£‡∏´‡∏±‡∏™‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£', '‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£', 'ANOMALY_STATUS'}

                for row_idx, row in enumerate(ws.iter_rows(min_row=2, max_row=ws.max_row), 2):
                    for col_idx, cell in enumerate(row, 1):
                        header_value = ws.cell(row=1, column=col_idx).value

                        if header_value not in text_columns:
                            if cell.value and isinstance(cell.value, (int, float)):
                                cell.number_format = '#,##0.00'
                                cell.alignment = Alignment(horizontal='right')

                # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏™‡∏µ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏£‡∏∞‡∏î‡∏±‡∏ö
                service_group_fill = PatternFill(start_color='E8F1F8', end_color='E8F1F8', fill_type='solid')
                service_group_font = Font(bold=True, size=10)

                business_group_fill = PatternFill(start_color='D0E2F0', end_color='D0E2F0', fill_type='solid')
                business_group_font = Font(bold=True, size=11)

                grand_total_fill = PatternFill(start_color='B8CCE4', end_color='B8CCE4', fill_type='solid')
                grand_total_font = Font(bold=True, size=12)

                total_service_fill = PatternFill(start_color='C6E0B4', end_color='C6E0B4', fill_type='solid')
                total_service_font = Font(bold=True, size=11, color='006100')

                adj_group_fill = PatternFill(start_color='FCE4D6', end_color='FCE4D6', fill_type='solid')
                adj_group_font = Font(bold=True, size=11)

                adj_item_fill = PatternFill(start_color='FDF7F4', end_color='FDF7F4', fill_type='solid')
                adj_item_font = Font(bold=False, size=10)

                adj_total_fill = PatternFill(start_color='F8CBAD', end_color='F8CBAD', fill_type='solid')
                adj_total_font = Font(bold=True, size=11)

                # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ highlight ‡πÅ‡∏ñ‡∏ß‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó
                for row in ws.iter_rows(min_row=2, max_row=ws.max_row):
                    business_value = row[0].value
                    service_value = row[1].value

                    if business_value and isinstance(business_value, str):
                        if business_value.startswith('‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏¥‡πâ‡∏ô'):
                            for cell in row:
                                cell.fill = grand_total_fill
                                cell.font = grand_total_font
                        elif business_value.startswith('‡∏£‡∏ß‡∏°‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£'):
                            for cell in row:
                                cell.fill = total_service_fill
                                cell.font = total_service_font
                        elif business_value == Config.NEW_ADJ_BUSINESS_GROUP:
                            for cell in row:
                                cell.fill = adj_group_fill
                                cell.font = adj_group_font
                        elif business_value.startswith('‡∏£‡∏ß‡∏° '):
                            for cell in row:
                                cell.fill = business_group_fill
                                cell.font = business_group_font

                    elif service_value and isinstance(service_value, str):
                        if service_value.startswith('‡∏£‡∏ß‡∏° '):
                            for cell in row:
                                cell.fill = service_group_fill
                                cell.font = service_group_font
                        elif service_value == '‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏ï‡∏≠‡∏ö‡πÅ‡∏ó‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏≠‡∏∑‡πà‡∏ô':
                            for cell in row:
                                cell.fill = adj_total_fill
                                cell.font = adj_total_font
                        elif service_value in (Config.FINANCIAL_INCOME_NAME, Config.OTHER_REVENUE_ADJ_NAME):
                             for cell in row:
                                cell.fill = adj_item_fill
                                cell.font = adj_item_font

                # ‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
                ws.column_dimensions['A'].width = 30
                ws.column_dimensions['B'].width = 35
                ws.column_dimensions['C'].width = 15
                ws.column_dimensions['D'].width = 40

                for col_idx in range(5, ws.max_column + 1):
                    ws.column_dimensions[get_column_letter(col_idx)].width = 15

                ws.freeze_panes = 'E2'

                # Format ANOMALY_STATUS column
                anomaly_col_idx = None
                for idx, cell in enumerate(ws[1], 1):
                    if cell.value == 'ANOMALY_STATUS':
                        anomaly_col_idx = idx
                        break

                if anomaly_col_idx:
                    try:
                        anomaly_col_letter = get_column_letter(anomaly_col_idx)
                        ws.column_dimensions[anomaly_col_letter].width = 20

                        header_cell = ws.cell(row=1, column=anomaly_col_idx)
                        header_cell.alignment = Alignment(horizontal='center', vertical='center')

                        spike_fill = PatternFill(start_color='FFC7CE', end_color='FFC7CE', fill_type='solid')
                        dip_fill = PatternFill(start_color='FFEB9C', end_color='FFEB9C', fill_type='solid')
                        neg_fill = PatternFill(start_color='FF0000', end_color='FF0000', fill_type='solid')

                        for row in ws.iter_rows(min_row=2, max_row=ws.max_row, min_col=anomaly_col_idx, max_col=anomaly_col_idx):
                            cell = row[0]
                            cell.alignment = Alignment(horizontal='center')

                            if cell.value == 'High_Spike':
                                cell.fill = spike_fill
                            elif cell.value == 'Low_Spike':
                                cell.fill = dip_fill
                            elif cell.value == 'Negative_Value':
                                cell.fill = neg_fill
                            elif cell.value == 'Spike_vs_Constant':
                                cell.fill = spike_fill
                            elif cell.value == 'New_Item':
                                cell.fill = PatternFill(start_color='C6E0B4', end_color='C6E0B4', fill_type='solid')

                    except Exception as e:
                        self.logger.warning(f"Warning: ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå Anomaly ‡πÑ‡∏î‡πâ: {e}")

                # Highlight Cell Anomaly
                if anomaly_map:
                    print(f"  Applying anomaly highlights to {sheet_name}...")

                    col_date_map = {}
                    for cell in ws[1]:
                        if cell.value and '/' in str(cell.value):
                            col_date_map[cell.column] = str(cell.value)

                    for row in ws.iter_rows(min_row=2, max_row=ws.max_row):
                        b_group = row[0].value
                        s_group = row[1].value
                        p_key = row[2].value

                        row_type = None
                        lookup_key = None

                        if p_key:
                             row_type = 'product'
                             lookup_key = (b_group, s_group, p_key)

                        if row_type == 'product' and lookup_key:
                            for col_idx, date_str in col_date_map.items():
                                map_key = (row_type, lookup_key, date_str)

                                if map_key in anomaly_map:
                                    status = anomaly_map[map_key]
                                    cell_to_color = ws.cell(row=row[0].row, column=col_idx)

                                    if status == 'High_Spike':
                                        cell_to_color.fill = spike_fill
                                    elif status == 'Low_Spike':
                                        cell_to_color.fill = dip_fill
                                    elif status == 'Negative_Value':
                                        cell_to_color.fill = neg_fill
                                        cell_to_color.font = Font(color='FFFFFF')

            try:
                wb.save(filename)
                self.logger.success(f"‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö Excel ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß")
            except Exception as e:
                self.logger.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå Excel ‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÅ‡∏•‡πâ‡∏ß‡πÑ‡∏î‡πâ (‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÄ‡∏õ‡∏¥‡∏î‡∏Ñ‡πâ‡∏≤‡∏á‡∏≠‡∏¢‡∏π‡πà): {e}")

        # ========================================================================
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô 2 ‡πÅ‡∏ö‡∏ö
        # ========================================================================
        self.logger.info("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô...")
        report_asc = create_report(agg_data, anomaly_results, df_adj_ytd, sort_ascending=True)
        report_desc = create_report(agg_data, anomaly_results, df_adj_ytd, sort_ascending=False)

        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏õ‡πá‡∏ô Excel
        try:
            with pd.ExcelWriter(excel_output_file, engine='openpyxl') as writer:
                report_asc.to_excel(writer, sheet_name='‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏ô‡πâ‡∏≠‡∏¢-‡∏°‡∏≤‡∏Å', index=False)
                report_desc.to_excel(writer, sheet_name='‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏°‡∏≤‡∏Å-‡∏ô‡πâ‡∏≠‡∏¢', index=False)
            self.logger.success(f"‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß: {excel_output_file}")
        except Exception as e:
            self.logger.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå Excel ‡πÑ‡∏î‡πâ (‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÄ‡∏õ‡∏¥‡∏î‡∏Ñ‡πâ‡∏≤‡∏á‡∏≠‡∏¢‡∏π‡πà): {e}")
            raise

        # Format Excel
        format_excel(excel_output_file, anomaly_map=historical_anomalies)

        # ‡πÄ‡∏û‡∏¥‡πà‡∏° Anomaly Detection Sheets
        print("\n" + "=" * 80)
        self.logger.info("‡πÄ‡∏û‡∏¥‡πà‡∏° Anomaly Detection Report...")
        print("=" * 80)

        try:
            self.create_anomaly_report_sheets(anomaly_results, excel_output_file)
        except Exception as e:
            self.logger.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏° Anomaly Sheets: {str(e)}")
            import traceback
            traceback.print_exc()

        # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        print("\n" + "=" * 80)
        self.logger.info("‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:")
        self.logger.info("=" * 80)
        self.logger.info(f"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (NT1 + ADJ): {len(df_result):,}")
        self.logger.info(f"‡∏¢‡∏≠‡∏î‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (NT1 + ADJ): {df_result['REVENUE_VALUE'].sum():,.2f}")
        self.logger.info(f"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô: {df_result['MONTH'].nunique()}")
        print(f"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Product (‡∏£‡∏ß‡∏° ADJ): {df_result['PRODUCT_KEY'].nunique()}")

        print("\n" + "=" * 80)
        print("Anomaly Detection Summary:")
        self.logger.info("=" * 80)
        if not anomaly_results:
            print("  ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Anomaly (‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏û‡∏≠)")
        else:
            for level_name, level_data in anomaly_results.items():
                # ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á dataframe ‡πÅ‡∏•‡∏∞ status_counts ‡∏à‡∏≤‡∏Å dict structure
                df_anomaly = level_data.get('dataframe')
                if df_anomaly is None:
                    continue

                # ‡πÉ‡∏ä‡πâ status_counts ‡∏ó‡∏µ‡πà‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÑ‡∏ß‡πâ‡πÅ‡∏•‡πâ‡∏ß
                status_counts = level_data.get('status_counts', {})
                if not status_counts and 'ANOMALY_STATUS' in df_anomaly.columns:
                    status_counts = df_anomaly['ANOMALY_STATUS'].value_counts().to_dict()

                total = len(df_anomaly)
                normal = status_counts.get('Normal', 0) + status_counts.get('Not_Enough_Data', 0)
                anomalies = total - normal
                print(f"{level_name.upper():15s}: {anomalies:4d} / {total:4d} anomalies detected")

        print("\n‚úì ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!")
        print(f"Excel Report: {excel_output_file}")
        print(f"  - Sheet 1: ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏ô‡πâ‡∏≠‡∏¢-‡∏°‡∏≤‡∏Å")
        print(f"  - Sheet 2: ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏°‡∏≤‡∏Å-‡∏ô‡πâ‡∏≠‡∏¢")

        return excel_output_file


# ============================================================================
# Main Execution
# ============================================================================
if __name__ == "__main__":
    from datetime import datetime

    print("="*80)
    print("Revenue ETL System - Standalone Execution")
    print("="*80)

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á ETL instance ‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏ô Pipeline
    etl = RevenueETL()
    df_result, anomaly_results = etl.run()

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á Excel Report
    excel_file = etl.create_excel_report(df_result, anomaly_results)
